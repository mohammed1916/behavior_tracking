{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7e9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import signal\n",
    "# import torch\n",
    "# import transformers\n",
    "# from open_flamingo import create_model_and_transforms\n",
    "\n",
    "# # Fix Windows compatibility\n",
    "# if not hasattr(signal, 'SIGALRM'):\n",
    "#     signal.SIGALRM = 14\n",
    "\n",
    "# # Set environment variables for compatibility\n",
    "# os.environ['TRUST_REMOTE_CODE'] = 'True'\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# print(\"üîß Setting up OpenFlamingo with MosaicGPT patches...\")\n",
    "\n",
    "# # Patch MosaicGPT class for embedding methods\n",
    "# def patch_mosaicgpt():\n",
    "#     import sys\n",
    "#     for module_name, module in list(sys.modules.items()):\n",
    "#         if module is None:\n",
    "#             continue\n",
    "#         try:\n",
    "#             for attr_name in dir(module):\n",
    "#                 try:\n",
    "#                     attr = getattr(module, attr_name)\n",
    "#                     if (isinstance(attr, type) and \n",
    "#                         'MosaicGPT' in attr.__name__ and \n",
    "#                         not hasattr(attr, '_embedding_patched')):\n",
    "                        \n",
    "#                         def get_input_embeddings(self):\n",
    "#                             return self.transformer.wte\n",
    "                        \n",
    "#                         def set_input_embeddings(self, value):\n",
    "#                             self.transformer.wte = value\n",
    "                        \n",
    "#                         attr.get_input_embeddings = get_input_embeddings\n",
    "#                         attr.set_input_embeddings = set_input_embeddings\n",
    "#                         attr._embedding_patched = True\n",
    "#                         print(f\"‚úÖ Patched {attr.__name__}\")\n",
    "#                         return True\n",
    "#                 except:\n",
    "#                     continue\n",
    "#         except:\n",
    "#             continue\n",
    "#     return False\n",
    "\n",
    "# # Monkey patch transformers\n",
    "# original_from_pretrained = transformers.AutoModelForCausalLM.from_pretrained\n",
    "\n",
    "# def patched_from_pretrained(*args, **kwargs):\n",
    "#     kwargs['trust_remote_code'] = True\n",
    "#     patch_mosaicgpt()  # Patch before model creation\n",
    "#     return original_from_pretrained(*args, **kwargs)\n",
    "\n",
    "# transformers.AutoModelForCausalLM.from_pretrained = patched_from_pretrained\n",
    "\n",
    "# try:\n",
    "#     print(\"üì• Creating OpenFlamingo model...\")\n",
    "#     model, image_processor, tokenizer = create_model_and_transforms(\n",
    "#         clip_vision_encoder_path=\"ViT-L-14\",\n",
    "#         clip_vision_encoder_pretrained=\"openai\",\n",
    "#         lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "#         tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "#         cross_attn_every_n_layers=1,\n",
    "#         decoder_layers_attr_name=\"transformer.blocks\"\n",
    "#     )\n",
    "    \n",
    "#     print(\"‚úÖ OpenFlamingo model created successfully!\")\n",
    "#     print(f\"Model: {type(model)}\")\n",
    "#     print(f\"Image processor: {type(image_processor)}\")\n",
    "#     print(f\"Tokenizer: {type(tokenizer)}\")\n",
    "\n",
    "# finally:\n",
    "#     # Restore original method\n",
    "#     transformers.AutoModelForCausalLM.from_pretrained = original_from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47049299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import requests\n",
    "# import torch\n",
    "\n",
    "# print(\"üñºÔ∏è Testing OpenFlamingo with images...\")\n",
    "\n",
    "# # Load a simple test image instead of downloading\n",
    "# import numpy as np\n",
    "# demo_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "# print(\"üìä Processing image...\")\n",
    "# # Process image for OpenFlamingo\n",
    "# vision_x = image_processor(demo_image).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "# print(f\"Vision input shape: {vision_x.shape}\")\n",
    "\n",
    "# # Prepare text prompt  \n",
    "# tokenizer.padding_side = \"left\"\n",
    "# text_prompt = \"An image of\"\n",
    "# lang_x = tokenizer([f\"<image>{text_prompt}\"], return_tensors=\"pt\")\n",
    "# print(f\"Text input shape: {lang_x['input_ids'].shape}\")\n",
    "\n",
    "# print(\"üéØ Generating caption...\")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "# vision_x = vision_x.to(device)\n",
    "# lang_x = {k: v.to(device) for k, v in lang_x.items()}\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     generated_tokens = model.generate(\n",
    "#         vision_x=vision_x,\n",
    "#         lang_x=lang_x[\"input_ids\"],\n",
    "#         attention_mask=lang_x[\"attention_mask\"],\n",
    "#         max_new_tokens=10,\n",
    "#         do_sample=False,\n",
    "#     )\n",
    "\n",
    "# generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "# caption = generated_text.replace(f\"<image>{text_prompt}\", \"\").strip()\n",
    "\n",
    "# print(f\"üéâ Generated caption: '{caption}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2c6248",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m hf_hub_download(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenflamingo/OpenFlamingo-3B-vitl-mpt1b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
