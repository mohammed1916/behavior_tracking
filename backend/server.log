2025-12-03 17:07:16,510 INFO Probing 2 local VLM models...
2025-12-03 17:07:16,510 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:07:23,904 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:07:36,355 INFO Probing 2 local VLM models...
2025-12-03 17:07:36,355 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:07:40,771 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:07:50,452 INFO Probing 2 local VLM models...
2025-12-03 17:07:50,452 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:07:58,120 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:08:13,603 INFO Probing 2 local VLM models...
2025-12-03 17:08:13,603 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:08:17,541 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:08:26,872 INFO Probing 2 local VLM models...
2025-12-03 17:08:26,872 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:08:30,866 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:13:32,144 INFO Probing 2 local VLM models...
2025-12-03 17:13:32,144 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:13:37,548 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:22:30,892 INFO Probing 2 local VLM models...
2025-12-03 17:22:30,892 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:22:35,395 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:24:07,414 INFO Probing 2 local VLM models...
2025-12-03 17:24:07,414 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:24:09,924 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:24:24,150 INFO Probing 2 local VLM models...
2025-12-03 17:24:24,150 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:24:28,145 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:35:03,071 INFO Probing 2 local VLM models...
2025-12-03 17:35:03,071 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:35:07,612 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:36:39,645 INFO Saved VLM upload to vlm_uploads\b0df66e5-7989-4586-b8e3-dbe12b03b919_assembly_idle.mp4
2025-12-03 17:45:56,982 INFO Saved VLM upload to vlm_uploads\2c362c0d-a6f3-4111-b506-a5a0b29eaa3e_assembly_idle.mp4
2025-12-03 17:55:04,161 INFO Saved VLM upload to vlm_uploads\5ccbd58f-9cac-49da-ac91-59177f73a8b6_assembly_idle.mp4
2025-12-04 09:11:45,231 INFO Probing 2 local VLM models...
2025-12-04 09:11:45,231 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:11:52,660 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:12:18,122 INFO Probing 2 local VLM models...
2025-12-04 09:12:18,122 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:12:24,369 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:15:54,711 INFO Saved VLM upload to vlm_uploads\364d2121-5985-4a75-bc84-6f64d1d88962_assembly_idle.mp4
2025-12-04 09:16:21,821 INFO Failed to load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 09:16:37,965 INFO Saved VLM upload to vlm_uploads\f9b86c3c-11c1-4198-91fb-f0ed2ffdabe8_assembly_idle.mp4
2025-12-04 09:16:45,670 INFO Saved VLM upload to vlm_uploads\f57ea612-804a-4974-8f79-6e4f394a5eb0_assembly_idle.mp4
2025-12-04 09:16:54,080 INFO Failed to load captioner for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 09:17:15,612 INFO Saved VLM upload to vlm_uploads\2cb5a2a8-4c65-4915-9292-4ad081efafae_assembly_idle.mp4
2025-12-04 09:39:30,473 INFO Probing 2 local VLM models...
2025-12-04 09:39:30,473 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:39:36,668 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:39:46,525 INFO Probing 2 local VLM models...
2025-12-04 09:39:46,525 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:39:50,279 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:39:59,918 INFO Saved VLM upload to vlm_uploads\32ed470f-28cd-48fd-a339-19b70d17ce10_assembly_idle.mp4
2025-12-04 09:40:00,307 INFO Failed to load captioner for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 09:41:48,479 INFO Saved VLM upload to vlm_uploads\c0a7c772-0ee7-41a8-ae73-91dd0e1e940a_assembly_idle.mp4
2025-12-04 10:00:12,452 INFO Saved VLM upload to vlm_uploads\12efb212-d2d1-43f6-9844-0832e4789f07_assembly_idle.mp4
2025-12-04 10:25:05,193 INFO Saved VLM upload to vlm_uploads\5e9b57e2-7bc3-4c9d-8a27-32200892a7ee_assembly_idle.mp4
2025-12-04 10:25:27,486 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 10:25:27,489 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 10:25:53,304 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 10:42:28,444 INFO Probing 2 local VLM models...
2025-12-04 10:42:28,451 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 10:42:31,351 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 10:42:33,973 INFO Probing 2 local VLM models...
2025-12-04 10:42:33,973 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 10:42:38,457 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 10:44:46,606 INFO Saved VLM upload to vlm_uploads\b46ecd4c-02e6-4124-a9cf-4274e1a70eab_assembly_idle.mp4
2025-12-04 11:11:11,683 INFO Probing 2 local VLM models...
2025-12-04 11:11:11,683 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 11:11:17,640 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 11:16:44,438 WARNING Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-12-04 11:29:45,740 INFO Saved VLM upload to vlm_uploads\44f737ea-c3f7-4326-80b2-d7c1c7a79c60_assembly_idle.mp4
2025-12-04 11:29:50,027 INFO Local-only load failed for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 11:29:50,027 INFO Attempting to download model gpt-4o-vlm (task=image-to-text)...
2025-12-04 11:29:50,252 INFO Failed to download/load captioner for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 11:43:12,529 INFO Probing 2 local VLM models...
2025-12-04 11:43:12,545 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 11:43:17,430 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 11:43:28,254 INFO Probing 2 local VLM models...
2025-12-04 11:43:28,255 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 11:43:31,990 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 11:45:17,917 INFO Saved VLM upload to vlm_uploads\8f521876-8c05-434e-a7d5-4c7196c023a2_assembly_idle.mp4
2025-12-04 11:45:40,357 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:45:40,389 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 11:46:09,241 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:49:30,489 INFO Saved VLM upload to vlm_uploads\6f72def3-5a3f-41d1-b40a-d4db07a44c1e_assembly_idle.mp4
2025-12-04 11:49:51,352 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:49:51,352 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 11:50:15,806 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:50:15,838 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 11:50:18,079 INFO Downloaded and loaded captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU
2025-12-04 11:51:31,937 INFO Saved VLM upload to vlm_uploads\02f9c9ea-7cb1-418e-8c09-1fbb7385bd26_assembly_idle.mp4
2025-12-04 11:51:35,094 INFO Local-only load failed for Salesforce/blip-image-captioning-large: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:51:35,094 INFO Detected CUDA-related error while loading Salesforce/blip-image-captioning-large, attempting CPU fallback...
2025-12-04 11:51:39,008 INFO CPU fallback failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:51:39,012 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 11:51:40,648 INFO Failed to download/load captioner for Salesforce/blip-image-captioning-large on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:57:55,391 INFO Saved VLM upload to vlm_uploads\cc4d8d18-d34f-426b-afb6-2db0b1cd454c_assembly_idle.mp4
2025-12-04 11:57:59,248 INFO Saved VLM upload to vlm_uploads\30bfd897-e8a8-407b-943a-7b05ee2d4922_assembly_idle.mp4
2025-12-04 11:58:27,608 INFO Saved VLM upload to vlm_uploads\5592977a-6b4c-4f26-8c7d-8b021da9cf5b_assembly_idle.mp4
2025-12-04 11:58:51,823 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:58:51,823 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 11:58:55,955 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 12:22:31,369 INFO Probing 2 local VLM models...
2025-12-04 12:22:31,369 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:22:34,518 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:22:59,135 INFO Probing 2 local VLM models...
2025-12-04 12:22:59,154 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:23:07,519 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:23:26,392 INFO Saved VLM upload to vlm_uploads\aefd6448-d69f-477f-bc61-4a4016d93ac8_assembly_idle.mp4
2025-12-04 12:23:27,324 INFO Using Ollama CLI model strat for text LLM
2025-12-04 12:25:23,508 INFO Probing 2 local VLM models...
2025-12-04 12:25:23,508 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:25:30,322 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:25:47,124 INFO Probing 2 local VLM models...
2025-12-04 12:25:47,124 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:25:53,428 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:26:15,369 INFO Saved VLM upload to vlm_uploads\fe3250be-de97-4d32-81e5-856980a3065d_assembly_idle.mp4
2025-12-04 12:28:30,666 INFO Saved VLM upload to vlm_uploads\c6862a08-9585-4e46-8b15-9f8b3cc6c0fe_assembly_idle.mp4
2025-12-04 12:29:08,545 INFO Saved VLM upload to vlm_uploads\6b464b72-c690-4c49-987d-ac143f638aa0_assembly_idle.mp4
2025-12-04 12:29:25,119 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 12:29:25,119 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 12:29:27,699 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 12:32:18,920 INFO Probing 2 local VLM models...
2025-12-04 12:32:18,920 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:32:21,436 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:32:35,574 INFO Probing 2 local VLM models...
2025-12-04 12:32:35,575 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:32:39,214 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:32:49,167 INFO Saved VLM upload to vlm_uploads\45c0d213-7334-4bb0-916d-f40b491445c7_assembly_idle.mp4
2025-12-04 12:32:49,281 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 12:32:49,573 INFO Local-only load failed for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 12:32:49,574 INFO Attempting to download model gpt-4o-vlm (task=image-to-text)...
2025-12-04 12:32:49,794 INFO Failed to download/load captioner for gpt-4o-vlm on device=-1: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 12:32:49,794 INFO Attempting to download/load gpt-4o-vlm on CPU...
2025-12-04 12:32:50,028 INFO Failed to download/load captioner for gpt-4o-vlm on CPU: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 12:33:07,680 INFO Saved VLM upload to vlm_uploads\0a991d1e-7473-4a50-b777-0b4926378bfe_assembly_idle.mp4
2025-12-04 12:36:32,026 INFO Probing 2 local VLM models...
2025-12-04 12:36:32,026 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:41:46,177 INFO Probing 2 local VLM models...
2025-12-04 12:41:46,177 INFO Probing 2 local VLM models...
2025-12-04 12:41:46,177 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:41:50,357 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:41:59,769 INFO Probing 2 local VLM models...
2025-12-04 12:41:59,773 INFO Probing 2 local VLM models...
2025-12-04 12:41:59,773 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:42:03,481 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:44:27,804 INFO Saved VLM upload to vlm_uploads\7175d846-c73e-418f-a164-ff09342d478c_assembly_idle.mp4
2025-12-04 12:44:27,931 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 12:44:29,732 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 12:44:29,732 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 12:44:31,885 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 12:44:38,618 INFO Using Ollama CLI model strat for text LLM
2025-12-04 12:57:12,306 INFO Saved VLM upload to vlm_uploads\fb423ebb-eea3-4a7d-8925-6647f7a42311_assembly_idle.mp4
2025-12-04 13:13:47,695 INFO Saved VLM upload to vlm_uploads\13a23b0b-807b-4050-b7f4-dd798c68ba24_assembly_idle.mp4
2025-12-04 13:22:47,428 INFO Probing 0 local VLM models...
2025-12-04 13:22:47,428 INFO Probing 0 local VLM models...
2025-12-04 13:22:50,425 INFO Probing 0 local VLM models...
2025-12-04 13:22:50,425 INFO Probing 0 local VLM models...
2025-12-04 13:22:52,096 INFO Probing 0 local VLM models...
2025-12-04 13:22:52,096 INFO Probing 0 local VLM models...
2025-12-04 13:23:02,267 INFO Probing 0 local VLM models...
2025-12-04 13:23:02,267 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:26:17,622 INFO Probing 3 local VLM models...
2025-12-04 13:26:17,622 INFO Probing 3 local VLM models...
2025-12-04 13:26:17,623 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:26:21,648 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:26:35,068 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:26:35,096 INFO Probing 3 local VLM models...
2025-12-04 13:26:35,096 INFO Probing 3 local VLM models...
2025-12-04 13:26:35,097 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:26:38,817 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:26:50,586 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:26:57,499 INFO Saved VLM upload to vlm_uploads\0ac216a7-fdfc-4da4-b8dd-fffa771adc37_assembly_idle.mp4
2025-12-04 13:27:17,608 INFO Probing 3 local VLM models...
2025-12-04 13:27:17,608 INFO Probing 3 local VLM models...
2025-12-04 13:27:17,608 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:27:20,635 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:27:30,284 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:27:30,311 INFO Probing 3 local VLM models...
2025-12-04 13:27:30,311 INFO Probing 3 local VLM models...
2025-12-04 13:27:30,311 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:27:33,174 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:27:42,133 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:29:55,734 INFO Probing 3 local VLM models...
2025-12-04 13:29:55,749 INFO Probing 3 local VLM models...
2025-12-04 13:29:55,749 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:30:02,059 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:30:10,911 INFO Probing Entry {'id': 'microsoft/Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:30:22,244 INFO Probing 3 local VLM models...
2025-12-04 13:30:22,245 INFO Probing 3 local VLM models...
2025-12-04 13:30:22,245 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:30:26,062 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:30:34,911 INFO Probing Entry {'id': 'microsoft/Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:31:57,273 INFO Probing 3 local VLM models...
2025-12-04 13:31:57,273 INFO Probing 3 local VLM models...
2025-12-04 13:31:57,273 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:32:01,934 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:32:12,606 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:33:46,823 INFO Probing 3 local VLM models...
2025-12-04 13:33:46,823 INFO Probing 3 local VLM models...
2025-12-04 13:33:46,823 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:33:50,686 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:33:59,731 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:34:36,593 INFO Saved VLM upload to vlm_uploads\a2248258-167f-4dd5-b085-1fd7c758de21_assembly_idle.mp4
2025-12-04 13:38:41,919 INFO Probing 3 local VLM models...
2025-12-04 13:38:41,919 INFO Probing 3 local VLM models...
2025-12-04 13:38:41,935 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:38:46,030 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:39:02,119 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:39:28,784 INFO Probing 3 local VLM models...
2025-12-04 13:39:28,784 INFO Probing 3 local VLM models...
2025-12-04 13:39:28,784 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:39:32,242 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:39:40,242 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:40:08,472 INFO Probing 3 local VLM models...
2025-12-04 13:40:08,472 INFO Probing 3 local VLM models...
2025-12-04 13:40:08,472 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:40:15,539 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:40:23,870 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base', 'task': 'image-to-text'}
2025-12-04 13:40:33,810 INFO Probing 3 local VLM models...
2025-12-04 13:40:33,810 INFO Probing 3 local VLM models...
2025-12-04 13:40:33,812 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:40:37,489 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:40:54,511 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base', 'task': 'image-to-text'}
2025-12-04 13:41:06,645 INFO Probing 3 local VLM models...
2025-12-04 13:41:06,645 INFO Probing 3 local VLM models...
2025-12-04 13:41:06,645 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:41:10,429 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:41:18,680 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base', 'task': 'image-to-text'}
2025-12-04 13:41:29,144 INFO Saved VLM upload to vlm_uploads\d84021f0-c4ef-4a29-b8cd-ff66754d6922_assembly_idle.mp4
2025-12-04 13:41:29,252 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 13:41:39,231 INFO Local-only load failed for microsoft/Florence-2-base: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:41:39,231 INFO Attempting to download model microsoft/Florence-2-base (task=image-to-text)...
2025-12-04 13:41:49,062 INFO Failed to download/load captioner for microsoft/Florence-2-base on device=-1: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:41:49,062 INFO Attempting to download/load microsoft/Florence-2-base on CPU...
2025-12-04 13:41:58,913 INFO Failed to download/load captioner for microsoft/Florence-2-base on CPU: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:51:57,847 INFO Saved VLM upload to vlm_uploads\80ddbe50-6e7e-4fa6-aaa1-d625202f26b7_assembly_idle.mp4
2025-12-04 13:52:23,176 INFO Local-only load failed for microsoft/Florence-2-base: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:52:23,176 INFO Attempting to download model microsoft/Florence-2-base (task=image-to-text)...
2025-12-04 13:52:33,022 INFO Failed to download/load captioner for microsoft/Florence-2-base on device=0: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:52:33,022 INFO Attempting to download/load microsoft/Florence-2-base on CPU...
2025-12-04 13:52:43,514 INFO Failed to download/load captioner for microsoft/Florence-2-base on CPU: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:58:08,631 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 13:58:08,636 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 13:58:32,266 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 13:58:32,282 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 13:58:34,477 INFO Downloaded and loaded captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU
2025-12-04 13:59:36,300 INFO Saved VLM upload to vlm_uploads\ae61bc26-6952-4638-830c-56e85dac7434_assembly_idle.mp4
2025-12-04 14:01:52,724 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:01:52,724 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=vision-text-generation)...
2025-12-04 14:01:52,988 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:01:52,988 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 14:01:53,242 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:02:31,327 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:02:31,327 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=vision-text-generation)...
2025-12-04 14:02:31,575 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:02:31,575 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 14:02:31,831 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:23:36,384 INFO Local-only load failed for microsoft/Florence-2-base: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 14:23:36,384 INFO Attempting to download model microsoft/Florence-2-base (task=image-to-text)...
2025-12-04 14:23:46,445 INFO Failed to download/load captioner for microsoft/Florence-2-base on device=0: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 14:23:46,445 INFO Attempting to download/load microsoft/Florence-2-base on CPU...
2025-12-04 14:23:56,442 INFO Failed to download/load captioner for microsoft/Florence-2-base on CPU: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 14:25:49,482 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 14:25:49,482 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 14:26:16,898 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 14:26:16,904 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 14:26:19,001 INFO Downloaded and loaded captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU
2025-12-04 14:26:37,745 INFO Saved VLM upload to vlm_uploads\94ccc802-bc5e-421e-81f7-012dbbffb0ab_assembly_idle.mp4
2025-12-04 14:50:52,504 INFO Loaded captioner for model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-text-to-text) from local cache on device=0
2025-12-04 14:51:04,481 INFO Saved VLM upload to vlm_uploads\60fc47ea-c820-4f59-aace-8b029ea78eea_assembly_idle.mp4
2025-12-04 14:51:04,752 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 14:51:09,974 INFO Loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text) from local cache on device=-1
2025-12-04 15:22:55,392 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:23:01,948 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:37:37,027 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:37:42,691 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:40:42,436 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:40:48,785 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:47:14,989 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:47:21,263 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:53:56,137 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 15:53:56,138 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 15:56:00,072 INFO Instantiating model architecture: CLIP
2025-12-04 15:56:07,269 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 15:56:16,978 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 15:56:16,994 INFO Model ViT-L-14 creation process complete.
2025-12-04 15:56:22,798 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:00:30,842 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:00:30,842 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:00:31,204 INFO Instantiating model architecture: CLIP
2025-12-04 16:00:33,179 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:00:35,209 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:00:35,209 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:00:36,118 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:01:40,865 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:01:40,865 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:01:41,215 INFO Instantiating model architecture: CLIP
2025-12-04 16:01:43,138 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:01:45,045 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:01:45,045 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:01:45,922 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:02:46,809 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:02:46,809 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:02:47,072 INFO Instantiating model architecture: CLIP
2025-12-04 16:02:48,975 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:02:51,068 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:02:51,074 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:02:51,994 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:05:27,751 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:05:27,751 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:05:28,025 INFO Instantiating model architecture: CLIP
2025-12-04 16:05:29,875 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:05:31,923 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:05:31,939 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:14:21,903 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.
2025-12-04 16:23:25,177 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:23:25,177 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:23:25,413 INFO Instantiating model architecture: CLIP
2025-12-04 16:23:27,309 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:23:30,631 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:23:30,647 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:23:53,797 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b using official pattern
2025-12-04 17:23:53,797 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:23:53,797 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:23:54,247 INFO Instantiating model architecture: CLIP
2025-12-04 17:23:56,443 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:23:59,422 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 17:23:59,456 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:24:00,476 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 17:24:00,476 ERROR OpenFlamingo compatibility issue: create_model_and_transforms does not pass trust_remote_code=True to underlying transformers calls
2025-12-04 17:24:00,476 ERROR This is a known limitation of the OpenFlamingo library when using MPT models
2025-12-04 17:28:42,373 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:28:42,373 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:28:46,606 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:28:54,204 INFO Saved VLM upload to vlm_uploads\2e65e00b-bceb-4057-9eaf-d41137d6730e_assembly_idle.mp4
2025-12-04 17:38:45,829 ERROR Error during caption generation with Salesforce/blip-image-captioning-large: mean must have 1 elements if it is an iterable, got 3
2025-12-04 17:43:54,266 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:43:54,270 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:43:59,678 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:44:05,833 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b using official pattern
2025-12-04 17:44:05,833 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:44:05,833 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:44:06,065 INFO Instantiating model architecture: CLIP
2025-12-04 17:44:08,733 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:44:12,541 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 17:44:12,545 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:44:13,623 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 17:44:13,637 ERROR OpenFlamingo compatibility issue: create_model_and_transforms does not pass trust_remote_code=True to underlying transformers calls
2025-12-04 17:44:13,638 ERROR This is a known limitation of the OpenFlamingo library when using MPT models
2025-12-04 17:49:03,658 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b using official pattern
2025-12-04 17:49:03,660 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:49:03,660 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:49:04,045 INFO Instantiating model architecture: CLIP
2025-12-04 17:49:07,761 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:49:25,812 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with monkey-patching workaround
2025-12-04 17:49:25,812 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:49:25,812 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:49:26,098 INFO Instantiating model architecture: CLIP
2025-12-04 17:49:28,083 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:49:30,591 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 17:49:30,591 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:49:36,649 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.
2025-12-04 17:49:36,649 ERROR OpenFlamingo unknown error: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.
2025-12-04 17:50:31,436 INFO Saved VLM upload to vlm_uploads\701cf00e-b0e3-41b6-8551-9bda15f47b0a_assembly_idle.mp4
2025-12-04 17:50:46,641 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with monkey-patching workaround
2025-12-04 17:50:46,642 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:50:46,642 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:50:46,978 INFO Instantiating model architecture: CLIP
2025-12-04 17:50:49,122 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:50:49,133 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:50:49,584 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-12-04 17:50:49,587 ERROR OpenFlamingo unknown error: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-12-04 17:50:52,077 INFO Saved VLM upload to vlm_uploads\4ff99486-2d15-4f85-a37e-7d4263dce030_assembly_idle.mp4
2025-12-04 17:50:55,452 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:50:57,486 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:50:57,486 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:51:02,362 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:51:11,378 INFO Saved VLM upload to vlm_uploads\e4531582-e4f2-4865-b4b5-f4e5f1360777_assembly_idle.mp4
2025-12-05 09:42:33,068 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 09:42:33,068 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 09:42:36,440 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 09:58:16,602 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 09:58:16,605 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 09:58:19,484 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 09:58:26,967 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 09:58:26,967 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 09:58:26,967 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 09:58:27,226 INFO Instantiating model architecture: CLIP
2025-12-05 09:58:29,359 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 09:58:33,626 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 09:58:33,626 INFO Model ViT-L-14 creation process complete.
2025-12-05 09:58:39,038 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:32:33,508 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:32:33,511 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:32:33,511 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:32:33,883 INFO Instantiating model architecture: CLIP
2025-12-05 10:32:35,910 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:32:37,694 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:32:37,694 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:32:38,548 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: dictionary changed size during iteration
2025-12-05 10:32:38,548 ERROR OpenFlamingo unknown error: dictionary changed size during iteration
2025-12-05 10:33:15,710 INFO Local-only load failed for force_cache_clear_dummy_model: force_cache_clear_dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-05 10:33:15,710 INFO Attempting to download model force_cache_clear_dummy_model (task=image-to-text)...
2025-12-05 10:33:15,957 INFO Failed to download/load captioner for force_cache_clear_dummy_model on device=0: force_cache_clear_dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-05 10:33:15,957 INFO Attempting to download/load force_cache_clear_dummy_model on CPU...
2025-12-05 10:33:16,185 INFO Failed to download/load captioner for force_cache_clear_dummy_model on CPU: force_cache_clear_dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-05 10:33:44,790 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:33:44,790 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:33:44,794 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:33:45,123 INFO Instantiating model architecture: CLIP
2025-12-05 10:33:46,990 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:33:49,251 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:33:49,251 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:34:11,261 INFO JAX version 0.7.1 available.
2025-12-05 10:34:23,008 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:35:31,844 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:35:31,844 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:35:31,844 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:35:32,636 INFO Instantiating model architecture: CLIP
2025-12-05 10:35:34,781 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:35:43,942 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:35:43,942 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:36:07,999 INFO JAX version 0.7.1 available.
2025-12-05 10:36:21,508 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:37:52,702 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:37:52,702 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:37:52,702 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:37:53,078 INFO Instantiating model architecture: CLIP
2025-12-05 10:37:55,035 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:37:56,912 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:37:56,912 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:38:04,641 INFO JAX version 0.7.1 available.
2025-12-05 10:38:14,471 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:39:22,418 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:39:22,418 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:39:22,418 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:39:22,791 INFO Instantiating model architecture: CLIP
2025-12-05 10:39:24,758 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:39:26,512 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:39:26,513 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:39:33,682 INFO JAX version 0.7.1 available.
2025-12-05 10:39:40,628 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:45:33,806 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:45:33,806 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:45:33,806 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:45:34,185 INFO Instantiating model architecture: CLIP
2025-12-05 10:45:36,427 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:45:39,602 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:45:39,605 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:45:50,034 INFO JAX version 0.7.1 available.
2025-12-05 10:45:57,264 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:46:04,559 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 10:46:04,560 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 10:46:08,751 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 12:28:25,278 INFO Saved VLM upload to vlm_uploads\8bab45e5-77f4-4111-8df0-941ccb667301_assembly_idle.mp4
2025-12-05 12:28:52,276 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 12:28:52,278 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 13:00:10,004 INFO Saved VLM upload to vlm_uploads\6f79afa3-a8dc-4eea-8867-a167af5265c3_assembly_idle.mp4
2025-12-05 13:00:24,311 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 13:00:24,311 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 13:00:27,729 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 13:00:40,074 INFO Using Ollama CLI model qwen3:0.6b for text LLM
2025-12-05 13:00:48,656 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The context is a close-up of a person holding a remote control in a factory.

First, I need to think about the action here. The person is holding a remote control. Remote controls are typically used to control devices like televisions, computers, or other electronics. In a factory, they might be working on machinery or machinery parts. So holding a remote control suggests that the person is engaged in an activity related to technology or machinery.

Now, the question is whether this is active work or idle. Active work usually means someone is doing something productive, like manufacturing or working on a project. Idle would mean they're not actively doing anything, maybe resting or unoccupied. Since the person is holding a remote control, which is a tool used for controlling equipment, it's clear they are actively involved in the work. Therefore, the correct response should be "work".
...done thinking.

work
