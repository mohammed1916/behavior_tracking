2025-12-03 17:07:16,510 INFO Probing 2 local VLM models...
2025-12-03 17:07:16,510 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:07:23,904 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:07:36,355 INFO Probing 2 local VLM models...
2025-12-03 17:07:36,355 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:07:40,771 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:07:50,452 INFO Probing 2 local VLM models...
2025-12-03 17:07:50,452 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:07:58,120 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:08:13,603 INFO Probing 2 local VLM models...
2025-12-03 17:08:13,603 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:08:17,541 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:08:26,872 INFO Probing 2 local VLM models...
2025-12-03 17:08:26,872 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:08:30,866 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:13:32,144 INFO Probing 2 local VLM models...
2025-12-03 17:13:32,144 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:13:37,548 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:22:30,892 INFO Probing 2 local VLM models...
2025-12-03 17:22:30,892 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:22:35,395 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:24:07,414 INFO Probing 2 local VLM models...
2025-12-03 17:24:07,414 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:24:09,924 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:24:24,150 INFO Probing 2 local VLM models...
2025-12-03 17:24:24,150 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP — Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:24:28,145 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:35:03,071 INFO Probing 2 local VLM models...
2025-12-03 17:35:03,071 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-03 17:35:07,612 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-03 17:36:39,645 INFO Saved VLM upload to vlm_uploads\b0df66e5-7989-4586-b8e3-dbe12b03b919_assembly_idle.mp4
2025-12-03 17:45:56,982 INFO Saved VLM upload to vlm_uploads\2c362c0d-a6f3-4111-b506-a5a0b29eaa3e_assembly_idle.mp4
2025-12-03 17:55:04,161 INFO Saved VLM upload to vlm_uploads\5ccbd58f-9cac-49da-ac91-59177f73a8b6_assembly_idle.mp4
2025-12-04 09:11:45,231 INFO Probing 2 local VLM models...
2025-12-04 09:11:45,231 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:11:52,660 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:12:18,122 INFO Probing 2 local VLM models...
2025-12-04 09:12:18,122 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:12:24,369 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:15:54,711 INFO Saved VLM upload to vlm_uploads\364d2121-5985-4a75-bc84-6f64d1d88962_assembly_idle.mp4
2025-12-04 09:16:21,821 INFO Failed to load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 09:16:37,965 INFO Saved VLM upload to vlm_uploads\f9b86c3c-11c1-4198-91fb-f0ed2ffdabe8_assembly_idle.mp4
2025-12-04 09:16:45,670 INFO Saved VLM upload to vlm_uploads\f57ea612-804a-4974-8f79-6e4f394a5eb0_assembly_idle.mp4
2025-12-04 09:16:54,080 INFO Failed to load captioner for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 09:17:15,612 INFO Saved VLM upload to vlm_uploads\2cb5a2a8-4c65-4915-9292-4ad081efafae_assembly_idle.mp4
2025-12-04 09:39:30,473 INFO Probing 2 local VLM models...
2025-12-04 09:39:30,473 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:39:36,668 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:39:46,525 INFO Probing 2 local VLM models...
2025-12-04 09:39:46,525 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 09:39:50,279 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 09:39:59,918 INFO Saved VLM upload to vlm_uploads\32ed470f-28cd-48fd-a339-19b70d17ce10_assembly_idle.mp4
2025-12-04 09:40:00,307 INFO Failed to load captioner for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 09:41:48,479 INFO Saved VLM upload to vlm_uploads\c0a7c772-0ee7-41a8-ae73-91dd0e1e940a_assembly_idle.mp4
2025-12-04 10:00:12,452 INFO Saved VLM upload to vlm_uploads\12efb212-d2d1-43f6-9844-0832e4789f07_assembly_idle.mp4
2025-12-04 10:25:05,193 INFO Saved VLM upload to vlm_uploads\5e9b57e2-7bc3-4c9d-8a27-32200892a7ee_assembly_idle.mp4
2025-12-04 10:25:27,486 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 10:25:27,489 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 10:25:53,304 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 10:42:28,444 INFO Probing 2 local VLM models...
2025-12-04 10:42:28,451 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 10:42:31,351 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 10:42:33,973 INFO Probing 2 local VLM models...
2025-12-04 10:42:33,973 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 10:42:38,457 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 10:44:46,606 INFO Saved VLM upload to vlm_uploads\b46ecd4c-02e6-4124-a9cf-4274e1a70eab_assembly_idle.mp4
2025-12-04 11:11:11,683 INFO Probing 2 local VLM models...
2025-12-04 11:11:11,683 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 11:11:17,640 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 11:16:44,438 WARNING Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-12-04 11:29:45,740 INFO Saved VLM upload to vlm_uploads\44f737ea-c3f7-4326-80b2-d7c1c7a79c60_assembly_idle.mp4
2025-12-04 11:29:50,027 INFO Local-only load failed for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 11:29:50,027 INFO Attempting to download model gpt-4o-vlm (task=image-to-text)...
2025-12-04 11:29:50,252 INFO Failed to download/load captioner for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 11:43:12,529 INFO Probing 2 local VLM models...
2025-12-04 11:43:12,545 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 11:43:17,430 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 11:43:28,254 INFO Probing 2 local VLM models...
2025-12-04 11:43:28,255 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 11:43:31,990 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 11:45:17,917 INFO Saved VLM upload to vlm_uploads\8f521876-8c05-434e-a7d5-4c7196c023a2_assembly_idle.mp4
2025-12-04 11:45:40,357 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:45:40,389 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 11:46:09,241 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:49:30,489 INFO Saved VLM upload to vlm_uploads\6f72def3-5a3f-41d1-b40a-d4db07a44c1e_assembly_idle.mp4
2025-12-04 11:49:51,352 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:49:51,352 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 11:50:15,806 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:50:15,838 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 11:50:18,079 INFO Downloaded and loaded captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU
2025-12-04 11:51:31,937 INFO Saved VLM upload to vlm_uploads\02f9c9ea-7cb1-418e-8c09-1fbb7385bd26_assembly_idle.mp4
2025-12-04 11:51:35,094 INFO Local-only load failed for Salesforce/blip-image-captioning-large: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:51:35,094 INFO Detected CUDA-related error while loading Salesforce/blip-image-captioning-large, attempting CPU fallback...
2025-12-04 11:51:39,008 INFO CPU fallback failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:51:39,012 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 11:51:40,648 INFO Failed to download/load captioner for Salesforce/blip-image-captioning-large on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 11:57:55,391 INFO Saved VLM upload to vlm_uploads\cc4d8d18-d34f-426b-afb6-2db0b1cd454c_assembly_idle.mp4
2025-12-04 11:57:59,248 INFO Saved VLM upload to vlm_uploads\30bfd897-e8a8-407b-943a-7b05ee2d4922_assembly_idle.mp4
2025-12-04 11:58:27,608 INFO Saved VLM upload to vlm_uploads\5592977a-6b4c-4f26-8c7d-8b021da9cf5b_assembly_idle.mp4
2025-12-04 11:58:51,823 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 11:58:51,823 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 11:58:55,955 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 12:22:31,369 INFO Probing 2 local VLM models...
2025-12-04 12:22:31,369 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:22:34,518 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:22:59,135 INFO Probing 2 local VLM models...
2025-12-04 12:22:59,154 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:23:07,519 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:23:26,392 INFO Saved VLM upload to vlm_uploads\aefd6448-d69f-477f-bc61-4a4016d93ac8_assembly_idle.mp4
2025-12-04 12:23:27,324 INFO Using Ollama CLI model strat for text LLM
2025-12-04 12:25:23,508 INFO Probing 2 local VLM models...
2025-12-04 12:25:23,508 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:25:30,322 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:25:47,124 INFO Probing 2 local VLM models...
2025-12-04 12:25:47,124 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:25:53,428 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:26:15,369 INFO Saved VLM upload to vlm_uploads\fe3250be-de97-4d32-81e5-856980a3065d_assembly_idle.mp4
2025-12-04 12:28:30,666 INFO Saved VLM upload to vlm_uploads\c6862a08-9585-4e46-8b15-9f8b3cc6c0fe_assembly_idle.mp4
2025-12-04 12:29:08,545 INFO Saved VLM upload to vlm_uploads\6b464b72-c690-4c49-987d-ac143f638aa0_assembly_idle.mp4
2025-12-04 12:29:25,119 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 12:29:25,119 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 12:29:27,699 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 12:32:18,920 INFO Probing 2 local VLM models...
2025-12-04 12:32:18,920 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:32:21,436 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:32:35,574 INFO Probing 2 local VLM models...
2025-12-04 12:32:35,575 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:32:39,214 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:32:49,167 INFO Saved VLM upload to vlm_uploads\45c0d213-7334-4bb0-916d-f40b491445c7_assembly_idle.mp4
2025-12-04 12:32:49,281 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 12:32:49,573 INFO Local-only load failed for gpt-4o-vlm: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 12:32:49,574 INFO Attempting to download model gpt-4o-vlm (task=image-to-text)...
2025-12-04 12:32:49,794 INFO Failed to download/load captioner for gpt-4o-vlm on device=-1: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 12:32:49,794 INFO Attempting to download/load gpt-4o-vlm on CPU...
2025-12-04 12:32:50,028 INFO Failed to download/load captioner for gpt-4o-vlm on CPU: gpt-4o-vlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-04 12:33:07,680 INFO Saved VLM upload to vlm_uploads\0a991d1e-7473-4a50-b777-0b4926378bfe_assembly_idle.mp4
2025-12-04 12:36:32,026 INFO Probing 2 local VLM models...
2025-12-04 12:36:32,026 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:41:46,177 INFO Probing 2 local VLM models...
2025-12-04 12:41:46,177 INFO Probing 2 local VLM models...
2025-12-04 12:41:46,177 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:41:50,357 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:41:59,769 INFO Probing 2 local VLM models...
2025-12-04 12:41:59,773 INFO Probing 2 local VLM models...
2025-12-04 12:41:59,773 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 12:42:03,481 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 12:44:27,804 INFO Saved VLM upload to vlm_uploads\7175d846-c73e-418f-a164-ff09342d478c_assembly_idle.mp4
2025-12-04 12:44:27,931 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 12:44:29,732 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 12:44:29,732 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 12:44:31,885 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 12:44:38,618 INFO Using Ollama CLI model strat for text LLM
2025-12-04 12:57:12,306 INFO Saved VLM upload to vlm_uploads\fb423ebb-eea3-4a7d-8925-6647f7a42311_assembly_idle.mp4
2025-12-04 13:13:47,695 INFO Saved VLM upload to vlm_uploads\13a23b0b-807b-4050-b7f4-dd798c68ba24_assembly_idle.mp4
2025-12-04 13:22:47,428 INFO Probing 0 local VLM models...
2025-12-04 13:22:47,428 INFO Probing 0 local VLM models...
2025-12-04 13:22:50,425 INFO Probing 0 local VLM models...
2025-12-04 13:22:50,425 INFO Probing 0 local VLM models...
2025-12-04 13:22:52,096 INFO Probing 0 local VLM models...
2025-12-04 13:22:52,096 INFO Probing 0 local VLM models...
2025-12-04 13:23:02,267 INFO Probing 0 local VLM models...
2025-12-04 13:23:02,267 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:25:25,013 INFO Probing 0 local VLM models...
2025-12-04 13:26:17,622 INFO Probing 3 local VLM models...
2025-12-04 13:26:17,622 INFO Probing 3 local VLM models...
2025-12-04 13:26:17,623 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:26:21,648 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:26:35,068 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:26:35,096 INFO Probing 3 local VLM models...
2025-12-04 13:26:35,096 INFO Probing 3 local VLM models...
2025-12-04 13:26:35,097 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:26:38,817 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:26:50,586 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:26:57,499 INFO Saved VLM upload to vlm_uploads\0ac216a7-fdfc-4da4-b8dd-fffa771adc37_assembly_idle.mp4
2025-12-04 13:27:17,608 INFO Probing 3 local VLM models...
2025-12-04 13:27:17,608 INFO Probing 3 local VLM models...
2025-12-04 13:27:17,608 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:27:20,635 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:27:30,284 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:27:30,311 INFO Probing 3 local VLM models...
2025-12-04 13:27:30,311 INFO Probing 3 local VLM models...
2025-12-04 13:27:30,311 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:27:33,174 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:27:42,133 INFO Probing Entry {'id': 'models--microsoft--Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:29:55,734 INFO Probing 3 local VLM models...
2025-12-04 13:29:55,749 INFO Probing 3 local VLM models...
2025-12-04 13:29:55,749 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:30:02,059 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:30:10,911 INFO Probing Entry {'id': 'microsoft/Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:30:22,244 INFO Probing 3 local VLM models...
2025-12-04 13:30:22,245 INFO Probing 3 local VLM models...
2025-12-04 13:30:22,245 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:30:26,062 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:30:34,911 INFO Probing Entry {'id': 'microsoft/Florence-2-base-ft', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:31:57,273 INFO Probing 3 local VLM models...
2025-12-04 13:31:57,273 INFO Probing 3 local VLM models...
2025-12-04 13:31:57,273 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:32:01,934 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:32:12,606 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:33:46,823 INFO Probing 3 local VLM models...
2025-12-04 13:33:46,823 INFO Probing 3 local VLM models...
2025-12-04 13:33:46,823 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:33:50,686 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:33:59,731 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:34:36,593 INFO Saved VLM upload to vlm_uploads\a2248258-167f-4dd5-b085-1fd7c758de21_assembly_idle.mp4
2025-12-04 13:38:41,919 INFO Probing 3 local VLM models...
2025-12-04 13:38:41,919 INFO Probing 3 local VLM models...
2025-12-04 13:38:41,935 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:38:46,030 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:39:02,119 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:39:28,784 INFO Probing 3 local VLM models...
2025-12-04 13:39:28,784 INFO Probing 3 local VLM models...
2025-12-04 13:39:28,784 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:39:32,242 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:39:40,242 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base-ft', 'task': 'image-to-text'}
2025-12-04 13:40:08,472 INFO Probing 3 local VLM models...
2025-12-04 13:40:08,472 INFO Probing 3 local VLM models...
2025-12-04 13:40:08,472 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:40:15,539 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:40:23,870 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base', 'task': 'image-to-text'}
2025-12-04 13:40:33,810 INFO Probing 3 local VLM models...
2025-12-04 13:40:33,810 INFO Probing 3 local VLM models...
2025-12-04 13:40:33,812 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:40:37,489 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:40:54,511 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base', 'task': 'image-to-text'}
2025-12-04 13:41:06,645 INFO Probing 3 local VLM models...
2025-12-04 13:41:06,645 INFO Probing 3 local VLM models...
2025-12-04 13:41:06,645 INFO Probing Entry {'id': 'Salesforce/blip-image-captioning-large', 'name': 'BLIP - Salesforce/blip-image-captioning-large', 'task': 'image-to-text'}
2025-12-04 13:41:10,429 INFO Probing Entry {'id': 'omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321', 'name': 'Qwen2.5-VL', 'task': 'image-to-text'}
2025-12-04 13:41:18,680 INFO Probing Entry {'id': 'microsoft/Florence-2-base', 'name': 'Florence-2-base', 'task': 'image-to-text'}
2025-12-04 13:41:29,144 INFO Saved VLM upload to vlm_uploads\d84021f0-c4ef-4a29-b8cd-ff66754d6922_assembly_idle.mp4
2025-12-04 13:41:29,252 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 13:41:39,231 INFO Local-only load failed for microsoft/Florence-2-base: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:41:39,231 INFO Attempting to download model microsoft/Florence-2-base (task=image-to-text)...
2025-12-04 13:41:49,062 INFO Failed to download/load captioner for microsoft/Florence-2-base on device=-1: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:41:49,062 INFO Attempting to download/load microsoft/Florence-2-base on CPU...
2025-12-04 13:41:58,913 INFO Failed to download/load captioner for microsoft/Florence-2-base on CPU: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:51:57,847 INFO Saved VLM upload to vlm_uploads\80ddbe50-6e7e-4fa6-aaa1-d625202f26b7_assembly_idle.mp4
2025-12-04 13:52:23,176 INFO Local-only load failed for microsoft/Florence-2-base: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:52:23,176 INFO Attempting to download model microsoft/Florence-2-base (task=image-to-text)...
2025-12-04 13:52:33,022 INFO Failed to download/load captioner for microsoft/Florence-2-base on device=0: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:52:33,022 INFO Attempting to download/load microsoft/Florence-2-base on CPU...
2025-12-04 13:52:43,514 INFO Failed to download/load captioner for microsoft/Florence-2-base on CPU: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 13:58:08,631 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 13:58:08,636 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 13:58:32,266 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 13:58:32,282 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 13:58:34,477 INFO Downloaded and loaded captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU
2025-12-04 13:59:36,300 INFO Saved VLM upload to vlm_uploads\ae61bc26-6952-4638-830c-56e85dac7434_assembly_idle.mp4
2025-12-04 14:01:52,724 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:01:52,724 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=vision-text-generation)...
2025-12-04 14:01:52,988 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:01:52,988 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 14:01:53,242 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:02:31,327 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:02:31,327 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=vision-text-generation)...
2025-12-04 14:02:31,575 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:02:31,575 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 14:02:31,831 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU: "Unknown task vision-text-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
2025-12-04 14:23:36,384 INFO Local-only load failed for microsoft/Florence-2-base: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 14:23:36,384 INFO Attempting to download model microsoft/Florence-2-base (task=image-to-text)...
2025-12-04 14:23:46,445 INFO Failed to download/load captioner for microsoft/Florence-2-base on device=0: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 14:23:46,445 INFO Attempting to download/load microsoft/Florence-2-base on CPU...
2025-12-04 14:23:56,442 INFO Failed to download/load captioner for microsoft/Florence-2-base on CPU: <class 'transformers.models.florence2.configuration_florence2.Florence2Config'>
2025-12-04 14:25:49,482 INFO Local-only load failed for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 14:25:49,482 INFO Attempting to download model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-to-text)...
2025-12-04 14:26:16,898 INFO Failed to download/load captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on device=0: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-12-04 14:26:16,904 INFO Attempting to download/load omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU...
2025-12-04 14:26:19,001 INFO Downloaded and loaded captioner for omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 on CPU
2025-12-04 14:26:37,745 INFO Saved VLM upload to vlm_uploads\94ccc802-bc5e-421e-81f7-012dbbffb0ab_assembly_idle.mp4
2025-12-04 14:50:52,504 INFO Loaded captioner for model omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321 (task=image-text-to-text) from local cache on device=0
2025-12-04 14:51:04,481 INFO Saved VLM upload to vlm_uploads\60fc47ea-c820-4f59-aace-8b029ea78eea_assembly_idle.mp4
2025-12-04 14:51:04,752 INFO GPU available but free memory (0 bytes) < 2147483648 bytes; preferring CPU to avoid OOM
2025-12-04 14:51:09,974 INFO Loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text) from local cache on device=-1
2025-12-04 15:22:55,392 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:23:01,948 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:37:37,027 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:37:42,691 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:40:42,436 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:40:48,785 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:47:14,989 INFO Local-only load failed for openflamingo/OpenFlamingo-3B-vitl-mpt1b: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
2025-12-04 15:47:21,263 ERROR Failed to download OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Unrecognized processing class in openflamingo/OpenFlamingo-3B-vitl-mpt1b. Can't instantiate a processor, a tokenizer, an image processor or a feature extractor for this model. Make sure the repository contains the files of at least one of those processing classes.
2025-12-04 15:53:56,137 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 15:53:56,138 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 15:56:00,072 INFO Instantiating model architecture: CLIP
2025-12-04 15:56:07,269 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 15:56:16,978 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 15:56:16,994 INFO Model ViT-L-14 creation process complete.
2025-12-04 15:56:22,798 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:00:30,842 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:00:30,842 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:00:31,204 INFO Instantiating model architecture: CLIP
2025-12-04 16:00:33,179 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:00:35,209 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:00:35,209 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:00:36,118 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:01:40,865 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:01:40,865 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:01:41,215 INFO Instantiating model architecture: CLIP
2025-12-04 16:01:43,138 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:01:45,045 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:01:45,045 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:01:45,922 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:02:46,809 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:02:46,809 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:02:47,072 INFO Instantiating model architecture: CLIP
2025-12-04 16:02:48,975 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:02:51,068 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:02:51,074 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:02:51,994 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 16:05:27,751 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:05:27,751 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:05:28,025 INFO Instantiating model architecture: CLIP
2025-12-04 16:05:29,875 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:05:31,923 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:05:31,939 INFO Model ViT-L-14 creation process complete.
2025-12-04 16:14:21,903 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.
2025-12-04 16:23:25,177 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 16:23:25,177 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 16:23:25,413 INFO Instantiating model architecture: CLIP
2025-12-04 16:23:27,309 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 16:23:30,631 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 16:23:30,647 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:23:53,797 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b using official pattern
2025-12-04 17:23:53,797 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:23:53,797 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:23:54,247 INFO Instantiating model architecture: CLIP
2025-12-04 17:23:56,443 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:23:59,422 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 17:23:59,456 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:24:00,476 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 17:24:00,476 ERROR OpenFlamingo compatibility issue: create_model_and_transforms does not pass trust_remote_code=True to underlying transformers calls
2025-12-04 17:24:00,476 ERROR This is a known limitation of the OpenFlamingo library when using MPT models
2025-12-04 17:28:42,373 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:28:42,373 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:28:46,606 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:28:54,204 INFO Saved VLM upload to vlm_uploads\2e65e00b-bceb-4057-9eaf-d41137d6730e_assembly_idle.mp4
2025-12-04 17:38:45,829 ERROR Error during caption generation with Salesforce/blip-image-captioning-large: mean must have 1 elements if it is an iterable, got 3
2025-12-04 17:43:54,266 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:43:54,270 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:43:59,678 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:44:05,833 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b using official pattern
2025-12-04 17:44:05,833 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:44:05,833 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:44:06,065 INFO Instantiating model architecture: CLIP
2025-12-04 17:44:08,733 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:44:12,541 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 17:44:12,545 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:44:13,623 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: The repository anas-awadalla/mpt-1b-redpajama-200b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b .
 You can inspect the repository content at https://hf.co/anas-awadalla/mpt-1b-redpajama-200b.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-12-04 17:44:13,637 ERROR OpenFlamingo compatibility issue: create_model_and_transforms does not pass trust_remote_code=True to underlying transformers calls
2025-12-04 17:44:13,638 ERROR This is a known limitation of the OpenFlamingo library when using MPT models
2025-12-04 17:49:03,658 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b using official pattern
2025-12-04 17:49:03,660 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:49:03,660 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:49:04,045 INFO Instantiating model architecture: CLIP
2025-12-04 17:49:07,761 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:49:25,812 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with monkey-patching workaround
2025-12-04 17:49:25,812 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:49:25,812 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:49:26,098 INFO Instantiating model architecture: CLIP
2025-12-04 17:49:28,083 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-04 17:49:30,591 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-04 17:49:30,591 INFO Model ViT-L-14 creation process complete.
2025-12-04 17:49:36,649 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.
2025-12-04 17:49:36,649 ERROR OpenFlamingo unknown error: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.
2025-12-04 17:50:31,436 INFO Saved VLM upload to vlm_uploads\701cf00e-b0e3-41b6-8551-9bda15f47b0a_assembly_idle.mp4
2025-12-04 17:50:46,641 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with monkey-patching workaround
2025-12-04 17:50:46,642 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-04 17:50:46,642 INFO Loaded built-in ViT-L-14 model config.
2025-12-04 17:50:46,978 INFO Instantiating model architecture: CLIP
2025-12-04 17:50:49,122 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:50:49,133 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:50:49,584 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-12-04 17:50:49,587 ERROR OpenFlamingo unknown error: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-12-04 17:50:52,077 INFO Saved VLM upload to vlm_uploads\4ff99486-2d15-4f85-a37e-7d4263dce030_assembly_idle.mp4
2025-12-04 17:50:55,452 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:50:57,486 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-04 17:50:57,486 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-04 17:51:02,362 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-04 17:51:11,378 INFO Saved VLM upload to vlm_uploads\e4531582-e4f2-4865-b4b5-f4e5f1360777_assembly_idle.mp4
2025-12-05 09:42:33,068 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 09:42:33,068 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 09:42:36,440 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 09:58:16,602 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 09:58:16,605 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 09:58:19,484 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 09:58:26,967 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 09:58:26,967 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 09:58:26,967 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 09:58:27,226 INFO Instantiating model architecture: CLIP
2025-12-05 09:58:29,359 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 09:58:33,626 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 09:58:33,626 INFO Model ViT-L-14 creation process complete.
2025-12-05 09:58:39,038 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:32:33,508 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:32:33,511 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:32:33,511 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:32:33,883 INFO Instantiating model architecture: CLIP
2025-12-05 10:32:35,910 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:32:37,694 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:32:37,694 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:32:38,548 ERROR Failed to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b: dictionary changed size during iteration
2025-12-05 10:32:38,548 ERROR OpenFlamingo unknown error: dictionary changed size during iteration
2025-12-05 10:33:15,710 INFO Local-only load failed for force_cache_clear_dummy_model: force_cache_clear_dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-05 10:33:15,710 INFO Attempting to download model force_cache_clear_dummy_model (task=image-to-text)...
2025-12-05 10:33:15,957 INFO Failed to download/load captioner for force_cache_clear_dummy_model on device=0: force_cache_clear_dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-05 10:33:15,957 INFO Attempting to download/load force_cache_clear_dummy_model on CPU...
2025-12-05 10:33:16,185 INFO Failed to download/load captioner for force_cache_clear_dummy_model on CPU: force_cache_clear_dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
2025-12-05 10:33:44,790 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:33:44,790 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:33:44,794 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:33:45,123 INFO Instantiating model architecture: CLIP
2025-12-05 10:33:46,990 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:33:49,251 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:33:49,251 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:34:11,261 INFO JAX version 0.7.1 available.
2025-12-05 10:34:23,008 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:35:31,844 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:35:31,844 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:35:31,844 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:35:32,636 INFO Instantiating model architecture: CLIP
2025-12-05 10:35:34,781 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:35:43,942 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:35:43,942 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:36:07,999 INFO JAX version 0.7.1 available.
2025-12-05 10:36:21,508 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:37:52,702 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:37:52,702 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:37:52,702 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:37:53,078 INFO Instantiating model architecture: CLIP
2025-12-05 10:37:55,035 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:37:56,912 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:37:56,912 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:38:04,641 INFO JAX version 0.7.1 available.
2025-12-05 10:38:14,471 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:39:22,418 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:39:22,418 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:39:22,418 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:39:22,791 INFO Instantiating model architecture: CLIP
2025-12-05 10:39:24,758 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:39:26,512 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:39:26,513 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:39:33,682 INFO JAX version 0.7.1 available.
2025-12-05 10:39:40,628 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:45:33,806 INFO Attempting to load OpenFlamingo model openflamingo/OpenFlamingo-3B-vitl-mpt1b with comprehensive monkey-patching
2025-12-05 10:45:33,806 INFO Parsing model identifier. Schema: None, Identifier: ViT-L-14
2025-12-05 10:45:33,806 INFO Loaded built-in ViT-L-14 model config.
2025-12-05 10:45:34,185 INFO Instantiating model architecture: CLIP
2025-12-05 10:45:36,427 INFO Loading full pretrained weights from: C:\Users\BBBS-AI-01\d\models\hub\models--timm--vit_large_patch14_clip_224.openai\snapshots\18d0535469bb561bf468d76c1d73aa35156c922b\open_clip_model.safetensors
2025-12-05 10:45:39,602 INFO Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}
2025-12-05 10:45:39,605 INFO Model ViT-L-14 creation process complete.
2025-12-05 10:45:50,034 INFO JAX version 0.7.1 available.
2025-12-05 10:45:57,264 ERROR OpenFlamingo compatibility issue: MosaicGPT model has embedding method compatibility issues
2025-12-05 10:46:04,559 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 10:46:04,560 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 10:46:08,751 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 12:28:25,278 INFO Saved VLM upload to vlm_uploads\8bab45e5-77f4-4111-8df0-941ccb667301_assembly_idle.mp4
2025-12-05 12:28:52,276 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 12:28:52,278 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 13:00:10,004 INFO Saved VLM upload to vlm_uploads\6f79afa3-a8dc-4eea-8867-a167af5265c3_assembly_idle.mp4
2025-12-05 13:00:24,311 INFO Local-only load failed for Salesforce/blip-image-captioning-large: ImageToTextPipeline._sanitize_parameters() got an unexpected keyword argument 'local_files_only'
2025-12-05 13:00:24,311 INFO Attempting to download model Salesforce/blip-image-captioning-large (task=image-to-text)...
2025-12-05 13:00:27,729 INFO Downloaded and loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text)
2025-12-05 13:00:40,074 INFO Using Ollama CLI model qwen3:0.6b for text LLM
2025-12-05 13:00:48,656 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The context is a close-up of a person holding a remote control in a factory.

First, I need to think about the action here. The person is holding a remote control. Remote controls are typically used to control devices like televisions, computers, or other electronics. In a factory, they might be working on machinery or machinery parts. So holding a remote control suggests that the person is engaged in an activity related to technology or machinery.

Now, the question is whether this is active work or idle. Active work usually means someone is doing something productive, like manufacturing or working on a project. Idle would mean they're not actively doing anything, maybe resting or unoccupied. Since the person is holding a remote control, which is a tool used for controlling equipment, it's clear they are actively involved in the work. Therefore, the correct response should be "work".
...done thinking.

work
2025-12-05 13:05:08,921 INFO Saved VLM upload to vlm_uploads\4e1bbc00-247c-439c-b0c0-47c25bd47a1b_assembly_idle.mp4
2025-12-05 13:05:13,243 INFO Loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text) from local cache on device=-1
2025-12-05 13:05:26,221 INFO Using Ollama CLI model qwen3:0.6b for text LLM
2025-12-05 13:05:29,105 INFO Ollama decoded output: Thinking...
Okay, so I need to figure out whether the caption describes active work or idle work. The user provided the context: a close-up of a person holding a remote control in a factory. Let me break this down.

First, the key elements here are a person in a factory. A remote control is an object typically associated with a home or office, not a factory. Factories are places where manufacturing takes place, but the remote control is an external device used in a home setting. The person is holding it, which suggests they are using it for a purpose beyond just idle. 

Active work would mean the person is engaged in a task, like in a factory. But remote controls are more like tools used in the home. So even though it's a factory, the person isn't using it for work in the traditional sense. Wait, but maybe in the context of a factory, remote control could be part of the work environment. However, the term 'idle' usually refers to being not doing anything, which would be a different scenario. 

Alternatively, maybe the person is using the remote control to operate machinery in the factory. If the person is in a factory, they might be operating equipment, which would be active work. But the remote control itself is an object, not a person. So the person is holding it, implying they are actively using it. Therefore, the caption is describing active work. 

I think the answer is work because the person is using a remote control in a factory setting, which is part of their active work. Even though the remote control is an object, the person's action indicates they are engaged.
...done thinking.

work
2025-12-05 13:05:35,915 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The context prompt says: "Arafed man working on a computer with a small machine."

First, I need to parse the words. The caption mentions "working on a computer with a small machine." The key here is the action of working. "Working" is definitely an active action, which implies that the man is engaged in some form of work. There's no mention of being idle or resting, so the answer should be "work." I should make sure there's no other possibility, but "idle" doesn't fit here. So the correct response is work.
...done thinking.

work
2025-12-05 13:05:45,904 INFO Ollama decoded output: Thinking...
Okay, let me try to figure this out. The user wants to know if the caption describes active work or idle. The caption says, "Arafed man working on a motorcycle in a factory."

First, I need to break down the words. "Arafed" might be a typo, maybe "Arafid"? Or perhaps it's a name. But regardless, the main action here is working on a motorcycle. The context mentions a factory, which is a place where work is done. So the man is in a factory, which implies he's engaged in a job there.

Now, thinking about the keywords: working, motorcycle, factory. All of these points towards being in the process of work. There's no mention of idle, like resting or not working. So it's not idle. Therefore, the answer should be work.
...done thinking.

work
2025-12-05 13:05:52,190 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed image of a man working on a computer motherboard."

First, I need to understand the context. The word "rafed" is a term from the film "Rajiv Gandhi: The Raft of Adventure," which is a movie about a raft. But here it's an image of a man working on a computer. The term "rafed" might be a red herring, but I should focus on the main content.

Next, the man is working on a computer, which is a typical activity associated with work. The word "working" directly indicates an active state. Even though "rafed" is a movie, the caption's main subject is the man's action, not the movie itself. So, the answer should be "work" because it's about work activity.
...done thinking.

work
2025-12-05 13:05:58,270 INFO Ollama decoded output: Thinking...
Okay, let me figure this out. The user wants to know if the caption describes active work or idle. The caption says, "there is a man working on a computer that is on a table."

First, I need to parse the words. The key part here is "working on a computer." That clearly indicates that the man is actively engaged in an activity. The computer is mentioned as being on a table, but that's just a location, not the activity itself. So the main action is working, which is active. There's no mention of him being idle, like sleeping or taking a break. So the answer should be work.
...done thinking.

work
2025-12-05 13:06:04,153 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle. The caption says, "there is a man that is working on a computer in a factory." So first, I need to break down the sentence.

The key here is identifying whether the man is engaged in a task or not. The phrase "working on a computer" clearly indicates that he's actively doing something. Factories are workplaces, so the context is about a worker in a factory. The presence of a computer suggests he's using it for work, not idle. So the answer should be "work" because it's about his current activity. I don't see any other words like "idle" here. Let me double-check to make sure I'm not missing anything. Yeah, the caption directly states he's working, so it's definitely work.
...done thinking.

work
2025-12-05 13:06:10,209 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle work. The caption is "Arafed man in a factory with a machine in the background."

First, I need to figure out if the man is working or idle. The term "factory" is a place where people work, so that's definitely a work context. Arafed is a name, but I think that's a common name for a man in a factory. The machine in the background also points to a work environment. There's no mention of him being idle or away from his job. So the answer should be "work."
...done thinking.

work
2025-12-05 13:06:27,956 INFO Ollama decoded output: Thinking...
Okay, let's see. The caption says, "Arafed man in a factory with a laptop and a pair of gloves." Hmm, the word "rafed" is probably a typo for "rafid" or maybe "rafed" is from a different language. Wait, maybe it's from a specific context. The user is asking if this caption describes active work or idle.

So, the key here is to determine if the man is working or idle. The presence of a laptop and gloves suggests he's in the factory, which is a place of work. A laptop is a tool he's using, and gloves could be an activity like working. So, even though the caption is in Arabic, the main action is working in a factory. Therefore, it's an active work scenario. The answer should be "work".
...done thinking.

work
2025-12-05 13:06:37,511 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle work. The caption given is: "there is a computer monitor sitting on a desk with a mouse."

First, I need to think about the context. The key elements here are a computer monitor and a mouse. Normally, when someone mentions a computer, it's usually in use, right? Like a person using it, maybe in a work environment. But the caption says it's sitting on a desk with a mouse. Wait, sitting there? That doesn't sound like it's being used. If the monitor is on a desk, maybe it's just a static display, not actively being used. But sometimes people use computers in different ways. Like, maybe they're just looking at it, or maybe they're taking a break. But the word "sitting" suggests it's in place, not moving. So the main point here is the computer being on the desk, not in motion. So the answer should be idle, right? Because it's not moving, just being there. I think that's it. No other factors to consider here. The caption is about the presence of the computer, not its activity.
...done thinking.

idle
2025-12-05 13:06:47,016 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle. The caption is "arafed worker working on a computer in a factory." First, I need to figure out what "arafed" refers to. Arafed is a type of wood, right? So the caption is about a worker using an arafed (wood) tool in a factory.

Now, the key here is to identify if the action is active or idle. Since the worker is "working on a computer in a factory," that's clearly a productive activity. There's no mention of being idle or resting. The word "working" directly indicates active work. Therefore, the correct response should be "work."
...done thinking.

work
2025-12-05 13:07:17,430 INFO Ollama decoded output: Thinking...
Okay, let me try to figure this out. The user wants to know if the caption describes active work or idle. The caption is "Arafed worker in a factory working on a computer screen." First, I need to break down the words. "Arafed" probably means "worker" or "worker in the factory." Then "working on a computer screen." So, the main action here is working on a computer. 

Now, the question is whether this is active work or idle. Active work would mean someone working actively, like in a job. Idle would be when someone is not actively doing something. Since the caption mentions the worker is doing something, it's definitely active. There's no mention of being idle, so the answer should be work.
...done thinking.

work
2025-12-05 13:07:35,529 INFO Ollama decoded output: Thinking...
Okay, let me try to figure this out. The user wants to know if the caption describes active work or idle. The caption is "Arafed man in a blue shirt working on two computer monitors." 

First, I need to break down the words. "Arafed" probably means "a man" or "a man in a blue shirt." Then "working" indicates activity. The next part is "on two computer monitors," which clearly shows he's doing something. So putting it all together, it's about someone actively working on computers. 

I should check if there's any ambiguity. Maybe "Arafed" could be a typo or a specific term, but in context, the main point is about work. The word "working" directly points to active work. So the answer should be "work."
...done thinking.

work
2025-12-05 13:07:52,468 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed worker in a factory working on a piece of equipment."

First, I need to parse the words. The key words here are "worker," "factory," "working," and "equipment." The presence of "working" immediately suggests that the worker is actively doing something. "Factory" is a setting, but it's part of the context, not the main action. "Arafed" might be a typo or a specific term, but I think it's a correct spelling. 

So, "working on a piece of equipment" clearly indicates that the worker is engaged in a productive activity. Active work implies being in motion, doing something, which matches the description. Idle would mean not doing anything, but here the worker is actively involved. Therefore, the correct response should be "work."
...done thinking.

work
2025-12-05 13:08:10,500 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed man in a factory with a machine and a box." First, I need to parse the words. The man is in a factory, which is a place of work. There's a machine and a box, which are common elements in factories. So, the key here is to identify if the man is actively engaged in a job or just idle. Since the context mentions a factory, it's a place of work, so the answer should be work. I don't see any words like "idle" here. The caption clearly points to a workplace setting.
...done thinking.

work
2025-12-05 13:08:29,377 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed man standing in a factory with a lot of machines." First, I need to parse the words. "Arafed" probably refers to a man, so maybe he's a male figure. "Standing in a factory" suggests he's present in a workplace setting. "With a lot of machines" points to machinery and production. 

Now, considering the context, being in a factory implies being actively involved in production, so that's work. "Idle" would mean not doing anything, but here he's there, so it's not idle. So the answer should be work.
...done thinking.

work
2025-12-05 13:08:44,121 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed man in a factory with a laptop and a phone." First, I need to parse the words. The main subject is a man in a factory. Factories are places of work, so that's definitely about work. Then there's a laptop and a phone, which are tools used in the factory. So, the presence of these items indicates he's actively engaged in his job. Therefore, it's about work. The response should be "work."
...done thinking.

work
2025-12-05 13:09:04,026 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle. The caption is "Arafed man in a blue shirt holding a bag of plastic." Hmm.

First, I need to figure out if the man is doing something active or idle. The key here is the action described. The caption says he's holding a bag of plastic. That sounds like he's carrying something, which is a physical activity. So, he's moving and using his hands, which makes me think of work. Words like "holding," "carrying," "working," or "busy" would indicate that. But wait, "idle" usually means not active, like just sitting or lying. So in this case, the man is actively doing something, so it's work. The response should be either "work" or "idle". Let me double-check. Yes, the action is clearly active. So the answer should be "work".
...done thinking.

work
2025-12-05 13:09:21,145 INFO Ollama decoded output: Thinking...
Okay, let's see. The caption says, "Arafed man in a blue shirt is looking at a computer screen." First, I need to figure out if this is describing active work or idle. 

So, the man is looking at a computer screen. Usually, looking at a computer screen implies being engaged in a computer-related activity, which is definitely work. Active work usually involves doing something actively, like coding, working on a project, or using technology. Idle would mean he's not actively doing anything, maybe resting or not using it. 

Wait, but the term "arafed" is a name, so maybe there's a cultural reference. But regardless, the key action here is looking at the computer. So even if it's a name, the action is active. Therefore, it's work. The answer should be work.
...done thinking.

work
2025-12-05 13:09:37,951 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle. The caption is: "Arafed man standing in a factory with a lot of electronics."

First, I need to understand the context. The word "rafed" is a name, so maybe it's a person's name. The man is standing in a factory, which is a place of work. There's a lot of electronics, which suggests there's a lot of machinery or products being made. So, the man is present in a workplace setting, which is definitely associated with work. The key words here are "factory" and "electronics," both indicating active participation in a productive environment. Therefore, the caption is describing an active work scenario. The response should be "work" or "idle." So the answer should be work.
...done thinking.

work
2025-12-05 13:09:56,555 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed man in a blue shirt and protective mask working on a machine."

First, I need to parse the words. The key elements here are "working" and "machine." The term "working" directly indicates that the man is engaged in a task. "Machine" is a common object associated with work. So, the main action here is work. There's no mention of being idle, like "sleeping" or "idle hands." Therefore, the correct response should be "work."
...done thinking.

work
2025-12-05 13:10:11,950 INFO Ollama decoded output: Thinking...
Okay, let me try to figure this out. The user wants to know if the caption describes active work or idle. The caption is "Arafed man in a factory working on a computer monitor." First, I need to break down the words. The key elements here are "factory" and "computer monitor." 

So, the man is in a factory, which is a place of work. Then he's working on a computer monitor. That sounds like he's actively engaged in a task. The word "working" directly indicates that he's doing something, not just idle. There's no mention of him being idle, like sleeping or resting. Therefore, it's clear that the caption is about him being actively involved in his job. 

I should make sure there's no other possible interpretation. Maybe he's in a different location, but the context says a factory, which is a place of work. Plus, the specific action of working on the monitor adds clarity. So the answer should be work.
...done thinking.

work
2025-12-05 13:10:28,907 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle. The caption is: "Arafed man in a factory looking at a monitor and a cell phone."

First, I need to parse the key elements. The main subject is a man in a factory. Factories are places of work, so that's a strong indicator. The man is looking at a monitor and a cell phone. Monitors and cell phones are common in factories, especially if someone is in the office or at work. It's not just idle, because working at a factory would involve such activities. The term "idle" usually refers to someone not actively doing something, like sleeping or resting. Since the man is actively engaged in the work environment with monitors and phones, it's definitely active. So the answer should be work.
...done thinking.

work
2025-12-05 13:10:45,720 INFO Ollama decoded output: Thinking...
Okay, let's see. The caption is "Arafed man standing in a factory with a lot of machines". The user wants to know if this describes active work or idle.

First, I need to break down the words. "Arafed" is probably a transliteration or a name, so it's not a person's name. Then "man" is the subject. "Standing in a factory" sounds like he's working there. "With a lot of machines" suggests there are machines, which are part of a factory setting. So the man is actively involved in the factory, doing something. 

Work implies active participation, while idle would mean he's not doing anything. So the answer should be work. I don't see any other words that indicate he's idle, just the presence of machines and the factory. Yep, that makes sense.
...done thinking.

work
2025-12-05 13:11:01,399 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed man in a blue shirt and mask standing in a factory." 

First, I need to figure out what "rafed" refers to. Wait, "rafed" might be a typo or a specific term. In some contexts, "rafed" could stand for "raff" or "raf" as in "raf" meaning to get something done. But in the context of a caption, maybe it's part of a larger term. However, the main part of the caption is about a man in a blue shirt and mask standing in a factory. 

The key here is to focus on the actions mentioned. The man is "standing in a factory," which clearly indicates movement and activity. There's no mention of him working, resting, or being idle. So, even though "rafed" could imply he's getting something done, the main action is that he's physically present in the factory, which is an active state. 

Therefore, the correct response should be "work" because the caption highlights active involvement in the factory setting.
...done thinking.

work
2025-12-05 13:11:10,043 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to figure out if the caption describes active work or idle. The caption is "Arafed man standing in a factory with a lot of electronics."

First, I need to understand the context. The man is described as standing in a factory, which is a place of work. There's mention of electronics, which are products or components used in factories. So, the key elements here are work-related activities. The word "factory" points to an industrial setting, and the man is present there. 

Now, the question is whether this caption is about being active or idle. Since the man is in a workplace and has electronics, it's clearly about work. There's no indication of being idle, like resting or not engaged. So, the correct answer should be "work."
...done thinking.

work
2025-12-05 13:11:15,843 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants to know if the caption describes active work or idle. The caption is "Arafed man working in a factory with a lot of electronics."

First, I need to break down the words. "Arafed" probably refers to the man, maybe his name. "Working" clearly indicates that he's engaged in work. "Factory" and "electronics" are all parts of a job environment. So, the main action here is working. There's no mention of being idle or resting, so the answer should be work.
...done thinking.

work
2025-12-05 13:11:21,935 INFO Ollama decoded output: Thinking...
Okay, so I need to figure out if the caption describes active work or idle. The context is "workers in a factory working on a computer screen." Let me break this down.

First, the caption mentions "workers" and "a factory," which are all places where people work. Then there's "working on a computer screen." That clearly indicates they are engaged in a specific activity: programming or something similar. The key here is the action of working. So, the workers are actively doing something, not idle. 

I don't think there's any mention of them being idle, like "sleeping" or "waiting." So the answer should be "work" because they are actively engaged in the task. Let me double-check if there's any other possible interpretation. Maybe if they were just resting, but no, "working" directly relates to activity. Yep, I'm pretty confident it's work.
...done thinking.

work
2025-12-05 13:11:27,664 INFO Ollama decoded output: Thinking...
Okay, let's see. The caption says, "Arafed worker in a factory cutting a piece of green apple." The key here is to figure out if it's describing an active or idle situation.

First, "arafed" is probably a typo for "arafid," which I think is the Indonesian word for "worker." So the caption is about a worker in a factory doing something. The action here is cutting an apple, which is a task. Typically, if someone is doing a physical task, that's considered active. Even if it's a machine doing it, like an apple cutter, it's still active. So the worker is actively involved in the work. Therefore, the answer should be "work."
...done thinking.

work
2025-12-05 13:11:34,486 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to determine if the caption describes active work or idle. The caption is "Arafed worker in a factory checking a box with a face mask." First, I need to break down the words. The key elements here are "worker," "factory," "checking a box," and "face mask." 

Now, "worker" usually implies someone actively doing something, right? Like a person in a job. "Factory" is a place where workers work, so that's part of the context. "Checking a box" sounds like they're working on something, maybe preparing for something. "Face mask" could indicate they're wearing a protective gear, which also shows they're in a job-related scenario. 

Putting this together, all the words point to someone in a factory doing a specific job. There's no mention of being idle or resting. So, even though they're wearing a mask, the main action is working. Therefore, the caption is about active work.
...done thinking.

work
2025-12-05 13:11:42,289 INFO Ollama decoded output: Thinking...
Okay, so the user wants me to determine if the caption describes active work or idle. The caption is "Arafed worker in a factory working on a laptop computer." Let me break this down.

First, the key words here are "worker," "factory," and "laptop." Arafed is a term I remember from some language, maybe related to a specific role or job. In a factory setting, workers are typically involved in production or tasks. A laptop computer is a device they're using, which suggests they're actively engaged in work. There's no mention of being idle, like resting or taking a break. So, the focus is on the ongoing activity of the worker. Therefore, the correct response should be "work."
...done thinking.

work
2025-12-05 17:23:31,495 INFO Saved VLM upload to vlm_uploads\4418e2a3-a1c4-45d3-9f2d-accb9c227491_assembly_idle.mp4
2025-12-05 17:23:33,996 INFO Loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text) from local cache on device=-1
2025-12-05 17:23:51,547 INFO Using Ollama CLI model qwen3:0.6b for text LLM
2025-12-05 17:23:55,927 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "a close up of a person holding a remote control in a factory". They want me to classify it as either "work" or "idle". 

First, I need to analyze the context. The caption mentions a person in a factory with a remote control. Remote controls are typically associated with technology, work environments, or even some activities in a workplace. However, the key here is the setting. A remote control is part of a computer or home electronics device, which is more related to work or technology rather than idle time.

Idle time usually refers to being unoccupied or not actively engaged, like sitting in a room or not doing anything. The person holding a remote control in a factory sounds like they're engaged in an activity related to technology, which is part of work. So the correct classification here is "work". 

I should make sure I'm not missing any other possibilities. Maybe if the person was idle, but the context clearly points to work. No other words or explanations are needed here. The answer has to be exactly one word, so "work" is the correct choice.
...done thinking.

work
2025-12-05 17:24:11,080 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man working on a computer with a small machine". The task is to classify this as either "work" or "idle". 

First, I need to analyze the content. The caption mentions a man working on a computer and having a small machine. That sounds like he's actively engaged in a task. Work typically refers to someone doing something productive, like working on a computer, which fits the context. The other option, idle, would mean he's not actively doing anything, which doesn't match the description. So the correct classification should be "work". 

I should make sure there's no other factors. The man's actions are clear, so there's no ambiguity. No mention of being idle, so the answer is definitely work.
...done thinking.

work
2025-12-05 17:24:23,425 INFO Ollama decoded output: Thinking...
Okay, let me try to figure this out. The user provided a caption: "arafed man working on a motorcycle in a factory". The task is to classify this as either "work" or "idle". 

First, I need to analyze the content. The caption mentions a man working on a motorcycle, which implies he's actively engaged in some activity. The location is a factory, so it's not just idle. "Working" clearly indicates that he's doing something productive. There's no mention of being idle, like sleeping or resting. So the key word here is "working". The user wants a single word, so I have to choose "work" instead of "idle". I should make sure not to add any extra text or explanations, just the word.
...done thinking.

work
2025-12-05 17:24:39,588 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed image of a man working on a computer motherboard". They want me to classify it as either "work" or "idle".

First, I need to analyze the caption. The main subject is a man working on a computer motherboard. The key action here is "working". The term "computer motherboard" suggests active involvement in the task. 

Now, considering the options, "work" directly refers to being engaged in a productive activity, which matches the description. "Idle" would mean not doing anything, which doesn't fit here. There's no mention of being passive or inactive, so "work" is the correct classification. I should just respond with "work" without any additional text.
...done thinking.

work
2025-12-05 17:24:54,553 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "there is a man working on a computer that is on a table." And I need to classify it as either "work" or "idle." 

First, I should analyze the content. The caption mentions a man working on a computer. The key action here is working. The computer is on a table, which suggests it's in a workspace or a location where someone is actively engaged. So the main activity is working. 

Now, the classification is straightforward. The word "work" directly relates to the action described. Even though there's a mention of the computer being on a table, the primary focus is on the man's work activity. There's no indication of being idle here. So the correct classification is "work." 

I should make sure there's no ambiguity. The context doesn't mention anything about being idle, so "work" is the right choice. No need to add any extra text or explanation. Just the single word.
...done thinking.

work
2025-12-05 17:24:59,671 INFO Ollama decoded output: Thinking...
Okay, let me try to figure this out. The user provided a caption: "there is a man that is working on a computer in a factory." And they want me to respond with either "work" or "idle".

First, I need to determine if the context is indicating that the man is working or idle. The caption mentions "working on a computer in a factory." That clearly points to the man being actively engaged in work. There's no mention of him being idle or distracted. So the main action here is work. The other word, "idle," doesn't fit because idle would mean not doing anything, which contradicts the context. Therefore, the correct response should be "work."
...done thinking.

work
2025-12-05 17:25:05,491 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "Arafed man in a factory with a machine in the background." The task is to classify the caption with either "work" or "idle." 

First, I need to determine if the caption is about work or idle. The term "rafed" is a slang for a man in a factory, which is common in the context of work environments. Arafed is often associated with workers, so this seems like a description of a factory worker. 

The caption mentions a man in a factory and a machine, which are typical elements of a workplace setting. There's no mention of being idle, like a person at home or a student. Therefore, the correct classification should be "work." 

I should make sure there's no other possible interpretation. The word "machine" is part of the factory, so that's consistent with work. No other elements would fit the idle category. So, the answer is definitely "work."
...done thinking.

work
2025-12-05 17:25:11,233 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man in a factory with a laptop and a pair of gloves." They want me to classify it as either "work" or "idle." 

First, I need to analyze the context. The caption mentions a man in a factory, a laptop, and gloves. These elements are all related to work environments, specifically manufacturing or industrial settings. The presence of a laptop and gloves suggests he's working on a task, possibly in a factory. 

Now, considering the word "work," it directly relates to the context of labor or employment. The other option, "idle," would imply a person not engaged in any activity, which doesn't fit the scenario here. There's no mention of being idle or not working, so the correct classification is clearly "work." 

I should make sure not to add any extra text or explanations, just the word. No punctuation or other elements are allowed. So the final answer is "work."
...done thinking.

work
2025-12-05 17:25:16,703 INFO Ollama decoded output: Thinking...
Okay, let me see. The user provided a caption: "there is a computer monitor sitting on a desk with a mouse." They want me to respond with either "work" or "idle" as the classifier.

First, I need to analyze the context. The caption mentions a computer monitor, mouse, and desk. These elements are all related to technology. The word "monitor" and "mouse" are part of a computer setup. 

Now, considering the context, the main idea here is about a computer setup. The term "work" typically refers to activities or tasks, while "idle" relates to being inactive. Since the caption describes a physical setup of a computer, it's not about being idle or working. 

I should make sure there's no ambiguity. The presence of a monitor and mouse doesn't imply being idle; instead, it shows an active state. Therefore, the correct classifier is "work."
...done thinking.

work
2025-12-05 17:25:22,275 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed worker working on a computer in a factory." The task is to classify this as either "work" or "idle". 

First, I need to understand the context. The caption mentions a worker, a computer, and a factory. Typically, a worker is someone engaged in work-related tasks. The word "worker" directly relates to work. Even though there's a computer and factory, those are part of the work environment. So the main element here is the worker performing work. 

I should check if there's any indication that the person is idle. The caption doesn't mention anything about being idle or resting. All actions are in the context of work. Therefore, the correct classification is "work". I need to make sure not to add any other text or explanation, just the word. Yep, that's it.
...done thinking.

work
2025-12-05 17:25:27,648 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to classify the caption as either "work" or "idle". The caption is "arafed worker in a factory working on a computer screen". 

First, I need to figure out if the caption is about work or idle. The key words here are "worker", "factory", "computer screen". All these elements point to a job-related scenario, like a worker in a factory. There's no mention of being idle, so it's definitely about work. 

I should make sure there's no other context I'm missing. The user specified to use exactly one word, so no need to add anything else. Just the word "work". Alright, that's it.
...done thinking.

work
2025-12-05 17:25:32,917 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man in a blue shirt working on two computer monitors". The task is to classify this as either "work" or "idle". 

First, I need to analyze the content. The caption mentions a man working on computers, which immediately suggests activity. There's no mention of being idle, like sleeping or resting. The key word here is "working", so it's clearly related to work. 

I should check if there's any ambiguity. The caption could be a picture, but the main focus is on the man's actions. Since the main action is work, the correct classification should be "work". I can't add any extra text or explanation here.
...done thinking.

work
2025-12-05 17:25:38,242 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed worker in a factory working on a piece of equipment". They want me to respond with either "work" or "idle". But wait, the instruction says to be a strict classifier. So I need to decide exactly one word, not add any text. 

First, the caption mentions a worker in a factory, working on equipment. That sounds like a job-related scenario. The word "work" directly relates to the worker's activity. "Idle" would imply being inactive, which doesn't fit here. So the correct classification is "work". I should make sure not to add any extra text or explanations, just the word. Yep, that's it.
...done thinking.

work
2025-12-05 17:25:43,398 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man in a factory with a machine and a box." I need to figure out if this is a work or idle scenario. The key here is to think about the context. The caption mentions a man in a factory, which is a typical workplace setting. Arafed is a type of man, so that's fitting for a work environment. There's also a machine and a box, which are common elements in factories. So, it's not about being idle, which would be something like a casual or free time situation. Therefore, the correct classification is "work."
...done thinking.

work
2025-12-05 17:25:48,913 INFO Ollama decoded output: Thinking...
Okay, let me see. The user provided a caption: "arafed man standing in a factory with a lot of machines." They want me to classify it as either "work" or "idle." 

First, I need to analyze the caption. The key elements here are a man in a factory with machines. That's clearly related to industrial activity. The word "factory" and "machines" point to a workplace setting. If I were to classify this, it's about being in a place that's actively working. So the correct classification should be "work." 

I don't see any mention of being idle or unrelated. The caption is focused on the environment of a factory, so the answer is definitely "work."
...done thinking.

work
2025-12-05 17:25:54,268 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to classify the given caption into either "work" or "idle". The caption is "arafed man in a factory with a laptop and a phone". Hmm, first, I need to figure out what the caption is about.

The caption mentions a man in a factory with a laptop and phone. That's a typical scenario, like a worker in an office or factory. So the main subject is a person working. The word "work" refers to being employed, while "idle" means not doing anything. Since the caption describes a person actively involved in a job, the correct classification should be "work". I don't need to add any other text or explanation, just the single word.
...done thinking.

work
2025-12-05 17:25:59,909 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "Arafed man in a blue shirt holding a bag of plastic." And they want me to classify it as either "work" or "idle". 

First, I need to figure out if this caption is related to work or idle. The caption mentions a man in a blue shirt and holding a bag of plastic. That sounds like a person in a work environment, maybe a job. However, the term "idle" usually refers to someone not actively engaged or not doing anything. But the caption describes someone actively doing something, so it's not idle. 

So, the key here is the action described. The man is holding a bag, which indicates he's actively involved. Therefore, the correct classification is "work". I should make sure there's no other possible interpretation. The words don't suggest any idle elements, so the answer is definitely work.
...done thinking.

work
2025-12-05 17:26:05,293 INFO Ollama decoded output: Thinking...
Okay, the user provided a caption: "arafed man in a blue shirt is looking at a computer screen". I need to classify it as either "work" or "idle". Let me think. The caption mentions a man in a blue shirt and looking at a computer screen. Those are typical elements of a job or a workplace setting. The word "computer screen" suggests activity at a workplace. So the correct classification is "work". I should make sure not to add any extra text or explanations. Yep, just the word "work".
...done thinking.

work
2025-12-05 17:26:11,208 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to classify the caption as either "work" or "idle". The caption says, "arafed man standing in a factory with a lot of electronics". Hmm, "arafed" probably refers to a specific person, maybe a person from a specific region or culture. The context mentions a factory with electronics, which are industrial equipment. So, the man is in a factory, which is an industrial setting. That sounds like a job or an employee. So, the correct classification should be "work". I don't see any mention of being idle or sleeping here. Yep, that's it.
...done thinking.

work
2025-12-05 17:26:33,322 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "Arafed man in a blue shirt and protective mask working on a machine". The task is to classify the response as either "work" or "idle". 

First, I need to understand the context. The caption mentions a man with a blue shirt, a mask, and working on a machine. It's clear that he's actively engaged in a task. So, the key here is the presence of "working" and "machine". 

Since the caption directly states that he's working on a machine, it's a clear indication of an active state. The word "work" directly relates to the action described. There's no mention of being idle, so the correct classification is "work". I should make sure not to add any extra text or explanation, just the single word. Yep, that's it.
...done thinking.

work
2025-12-05 17:26:54,672 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "Arafed man in a factory working on a computer monitor." And they want me to respond with either "work" or "idle". But wait, the instruction says to be a strict classifier, so exactly one word. 

First, I need to analyze the caption. The key elements here are a man in a factory, working on a computer monitor. That's clearly showing activity. So, the main action is working. Even though there's a "man" and "monitor," the primary action is "working." So the correct classification should be "work." I shouldn't add any other text, just the word. Let me double-check to make sure there's no other context I'm missing. No, the caption is straightforward. So the answer must be "work."
...done thinking.

work
2025-12-05 17:27:14,204 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption and wants me to classify it as either "work" or "idle". The caption says, "Arafed man in a factory looking at a monitor and a cell phone." 

First, I need to figure out if this is a work caption or idle. The term "Arafed" might be a typo or a specific term. Wait, "Arafed" sounds like "Arafid" or "Arafid" in Arabic. Maybe the caption is in Arabic, but the user wrote it in English? Or maybe there's a mix-up. But regardless, the main thing is the context.

The man is in a factory, looking at a monitor and a cell phone. That's a typical scene, like a worker in a workplace. So, it's about a person in a job setting, which is work. The other option is idle, meaning not occupied, but the caption describes someone actively engaged. So the correct classification is "work". No need to add anything else.
...done thinking.

work
2025-12-05 17:27:35,007 INFO Ollama decoded output: Thinking...
Okay, let's see. The user wants me to classify the given caption as either "work" or "idle". The caption is "arafed man standing in a factory with a lot of machines". Hmm, first, I need to understand the context. The man is in a factory, with machines around him. So, is this a scene of work or idle?

Work usually refers to activities done in a job, while idle would mean not doing anything. The caption mentions a man in a factory, which is a place where work is being done. Machines are part of the factory, so it's clearly a scenario where work is happening. There's no mention of being idle, so I can't just say idle. Therefore, the correct classification should be "work".
...done thinking.

work
2025-12-05 17:27:56,067 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man in a blue shirt and mask standing in a factory." They want me to classify it as either "work" or "idle." 

First, I need to understand the context. The caption mentions a man in a blue shirt and mask, standing in a factory. That sounds like a scene from a movie or a fictional scenario. The key elements here are a man, a blue shirt, a mask, and a factory. These elements are common in movies or shows, so the caption is describing a scene from a story or a movie.

Now, the user is asking for a strict classifier, so I have to choose between "work" or "idle." The term "idle" usually refers to someone who is not actively engaged or occupied, like a person at a desk. In contrast, "work" indicates being occupied with tasks. Since the caption describes a man in a factory, which is an activity, it's more likely to be associated with work rather than idle.

I should make sure I'm not missing any context. The man is in a factory, so the activity is working. Therefore, the correct classification is "work."
...done thinking.

work
2025-12-05 17:28:11,552 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man standing in a factory with a lot of electronics". The task is to classify it as either "work" or "idle". The user wants a single lowercase word, no punctuation or explanation.

First, I need to analyze the caption. The man is described as "arafed" which I think is a term from "The Arf" movie, a comedy. The context is a factory with electronics, which suggests a work environment. The words "factory" and "electronics" point to a job-related setting. So, the caption is about a person in a job, not idle. Therefore, the correct classification is "work". No need to add anything else.
...done thinking.

work
2025-12-05 17:28:26,400 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed man working in a factory with a lot of electronics." They want me to classify the response as either "work" or "idle," and not add any extra text.

First, I need to analyze the caption. The key elements here are "working in a factory" and "electronics." These are all positive actions and activities. The word "working" directly indicates that the man is actively engaged in work. There's no mention of being idle, like laziness or absence. So the main action is work. I should just output "work" without any additional text.
...done thinking.

work
2025-12-05 17:28:37,471 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "workers in a factory working on a computer screen." They want me to act as a strict classifier and respond with either "work" or "idle". 

First, I need to understand the context. The caption mentions workers, a factory, and a computer screen. All of these elements point to a scene of labor or productivity. The key here is the action of working. Even though it's a factory, the main action is working, so it's more about work than idle. 

I should make sure there's no other context. No mention of being idle, just the activity. So the correct classification is "work". I need to stick to one word and not add any text. Double-check for any other possibilities, but I don't think there's anything else. So the answer is work.
...done thinking.

work
2025-12-05 17:28:48,550 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "Arafed worker in a factory cutting a piece of green apple." And I need to classify it as either "work" or "idle". The instructions say to respond with exactly one lowercase word, no punctuation or explanation.

First, I need to figure out if the caption is about a worker or someone idle. The caption mentions a worker in a factory, doing something, and there's a green apple. That's clearly a worker doing a job. So the main element is work. The apple is part of the action, so it's a work scenario. There's no mention of being idle, so the correct classification should be "work". I should make sure not to add anything else, just the word.
...done thinking.

work
2025-12-05 17:29:00,813 INFO Ollama decoded output: Thinking...
Okay, let's see. The user provided a caption: "arafed worker in a factory checking a box with a face mask." And they want me to classify it as either "work" or "idle". 

First, I need to understand the context. The caption mentions a worker in a factory, doing something related to a box and a face mask. That sounds like a job scenario, right? A worker in a factory would be engaged in work. The face mask and the box suggest they're in a professional setting. So it's about a worker's activity. 

Now, the user is asking for a strict classifier. They don't want any explanations or punctuation. So the answer has to be exactly one word: either "work" or "idle". Since the caption clearly describes a worker's actions, I'm pretty confident it's "work". No need to add anything else. Just confirm that the caption indicates a work-related scenario.
...done thinking.

work
2025-12-05 17:29:13,906 INFO Ollama decoded output: Thinking...
Okay, the user provided a caption: "arafed worker in a factory working on a laptop computer." They want me to classify it as either "work" or "idle". Let me think.

First, the caption mentions "worker," "factory," and "laptop." These all seem to be related to work. The key here is the presence of a worker in a work environment. The word "working" directly indicates that the person is actively engaged in a job. There's no mention of being idle, like sleeping or resting. So, the main action here is work. Therefore, the correct classification should be "work". I don't see any other words or context that would suggest being idle here. So, the answer is definitely "work".
...done thinking.

work
2025-12-08 11:31:00,880 INFO Loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text) from local cache on device=-1
2025-12-08 11:40:25,346 INFO Loaded captioner for model Salesforce/blip-image-captioning-large (task=image-to-text) from local cache on device=-1
2025-12-08 11:41:23,083 INFO Saved VLM upload to vlm_uploads\a427cd16-1ace-4721-a1ec-f4c5c329be82_assembly_idle.mp4
2025-12-08 11:58:01,659 INFO Saved VLM upload to vlm_uploads\cfa9d11a-9d59-4f76-b511-a2a0f943bf89_assembly_idle.mp4
