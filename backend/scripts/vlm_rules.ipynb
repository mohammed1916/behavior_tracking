{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fade6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Enhanced Motion Detection for Productive Work\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "def analyze_motion_pattern(frame_gray, prev_gray):\n",
    "    \"\"\"Enhanced motion analysis with region-based detection\"\"\"\n",
    "    h, w = frame_gray.shape\n",
    "    \n",
    "    # Calculate overall motion\n",
    "    diff = cv2.absdiff(frame_gray, prev_gray)\n",
    "    overall_motion = np.sum(diff) / (h * w)\n",
    "    \n",
    "    # Region-based motion analysis (work area detection)\n",
    "    # Bottom half = work area (hands, tools, desk)\n",
    "    # Top half = upper body (less relevant for work detection)\n",
    "    work_area = diff[h//2:, :]\n",
    "    upper_area = diff[:h//2, :]\n",
    "    \n",
    "    work_motion = np.sum(work_area) / work_area.size\n",
    "    upper_motion = np.sum(upper_area) / upper_area.size\n",
    "    \n",
    "    # Detect localized motion (small, precise movements = productive work)\n",
    "    # Apply threshold to get significant motion pixels\n",
    "    _, motion_mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of motion regions\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Analyze motion characteristics\n",
    "    num_motion_regions = len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "    \n",
    "    # Hand-like motion detection (small to medium regions)\n",
    "    hand_motion_regions = [c for c in contours if 100 < cv2.contourArea(c) < 5000]\n",
    "    hand_motion_score = len(hand_motion_regions)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_motion,\n",
    "        'work_area': work_motion,\n",
    "        'upper_area': upper_motion,\n",
    "        'num_regions': num_motion_regions,\n",
    "        'hand_score': hand_motion_score,\n",
    "        'work_ratio': work_motion / (upper_motion + 0.1)  # Avoid division by zero\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, motion_history):\n",
    "    \"\"\"Determine if motion pattern indicates productive work\"\"\"\n",
    "    \n",
    "    # Thresholds (tune these based on your environment)\n",
    "    WORK_MOTION_MIN = 2.0      # Minimum motion in work area\n",
    "    WORK_MOTION_MAX = 15.0     # Maximum (too much = not focused)\n",
    "    HAND_SCORE_MIN = 2         # Minimum hand-like motion regions\n",
    "    WORK_RATIO_MIN = 1.2       # Work area should have more motion than upper body\n",
    "    \n",
    "    # Add current stats to history\n",
    "    motion_history.append(motion_stats)\n",
    "    if len(motion_history) > MOTION_HISTORY_SIZE:\n",
    "        motion_history.pop(0)\n",
    "    \n",
    "    # Analyze recent motion pattern (smoothing)\n",
    "    if len(motion_history) >= 3:\n",
    "        avg_work_motion = np.mean([m['work_area'] for m in motion_history[-5:]])\n",
    "        avg_hand_score = np.mean([m['hand_score'] for m in motion_history[-5:]])\n",
    "        avg_work_ratio = np.mean([m['work_ratio'] for m in motion_history[-5:]])\n",
    "        consistency = np.std([m['work_area'] for m in motion_history[-5:]])\n",
    "        \n",
    "        # Productive work indicators:\n",
    "        # 1. Moderate, consistent motion in work area\n",
    "        # 2. Multiple small motion regions (hands working)\n",
    "        # 3. More motion in lower half than upper half\n",
    "        # 4. Consistent pattern (not erratic)\n",
    "        \n",
    "        is_productive = (\n",
    "            WORK_MOTION_MIN < avg_work_motion < WORK_MOTION_MAX and\n",
    "            avg_hand_score >= HAND_SCORE_MIN and\n",
    "            avg_work_ratio >= WORK_RATIO_MIN and\n",
    "            consistency < 5.0  # Consistent motion pattern\n",
    "        )\n",
    "        \n",
    "        return is_productive, avg_work_motion, avg_hand_score\n",
    "    \n",
    "    return False, 0, 0\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = \"\"\"<|vision_start|><|image_pad|><|vision_end|>\n",
    "\n",
    "        You are an expert activity recognition model.\n",
    "\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text, explanations, or repeats.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # output_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "        output_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=5,  # Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\n",
    "            do_sample=False,   # Greedy decoding for consistency\n",
    "        )\n",
    "\n",
    "    raw_result = processor.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # ENHANCED MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "    \n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        \n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "    \n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC (Enhanced)\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    \n",
    "    # Enhanced decision making with motion analysis\n",
    "    if motion_stats:\n",
    "        # Strong productive motion pattern detected\n",
    "        if is_productive_motion:\n",
    "            if \"idle\" in result or \"unknown\" in result:\n",
    "                return \"assembling_drone\"  # Override VLM with motion evidence\n",
    "            return \"assembling_drone\"\n",
    "        \n",
    "        # Minimal motion detected\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        \n",
    "        # Medium motion but not productive pattern\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            if \"idle\" in result:\n",
    "                return \"simply_sitting\"  # Some movement but not working\n",
    "            elif \"unknown\" in result:\n",
    "                return \"simply_sitting\"\n",
    "        \n",
    "        # High overall motion but not in work area (body movement)\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"  # Moving but not working\n",
    "    \n",
    "    # Fallback to simple logic\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "# cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"pls assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1983c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained(\"qwen_vlm_2b_activity_model\")\n",
    "# processor.save_pretrained(\"qwen_vlm_2b_activity_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78e4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e871728fcea4422a9361b68264bdf30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 43 files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964dfaa4e1544efe81c4bda98d4ca78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BBBS-AI-01\\d\\models\\hub\\models--HuggingFaceTB--SmolVLM-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457620cafa1e476d848900b5363315f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ece0e4714f4ddab164dff883ad805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/429 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad078fd9d68f4fe789b55c581ef54c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a8189dbec40d4ad47b2c051bc2b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71a9bc98a5c49a4b3db04426c21c378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SmolVLM.png:   0%|          | 0.00/146k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8844933e3e04574a5a5384274918359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab60fc00c9249b59efc44ebd428a9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51787b2f51b40898316ffacc513ca35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e657ed4c2684314a2516893d1931f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged.onnx_data:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92083d17b4864ce48a547662a89ebc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mixture_the_cauldron.png:   0%|          | 0.00/937k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400a0f26683340608a3d057f3ff01318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_fp16.onnx_data:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bdef3cc31e46cb93fef3eecb498e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_bnb4.onnx:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d570d89b7e87409fb659eb934629ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_int8.onnx:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047d000f89c46f0b4cd71a5ca2cda2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged.onnx:   0%|          | 0.00/168k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3641137e68f4e139db7edfb6f8699d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_fp16.onnx:   0%|          | 0.00/169k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca7225a120b4a68929d803198969f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_q4.onnx:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19be5a520b64bc6837035dc0708a10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_q4f16.onnx:   0%|          | 0.00/965M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d1704e77e947f8a78773d4d347c41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_quantized.onnx:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6e54166d214fa387993401126d7271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens.onnx:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f3be4ed189414d9768daf662d1392f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_uint8.onnx:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3974bd57e9f14dc2b20bd1864133771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_bnb4.onnx:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3388280b8246539fd077f711051b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_fp16.onnx:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1887d5a68d5f4de392ade1a0302fd8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_int8.onnx:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c5c24d3bed495aa20b30e213bb7e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_q4.onnx:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e92e3442a240ee96df990838535402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_q4f16.onnx:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33205b6d0e2e4aada90c2e1a9c5add52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder.onnx:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb025028063f48e5bbaf7bd4c96bfa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_uint8.onnx:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970fa79208b74d4fb294f6374719a426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_bnb4.onnx:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb307d8bbf049f9aeec992c5576df56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_fp16.onnx:   0%|          | 0.00/869M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52814d795cf9483796fb3ad2019a6325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_int8.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f888c40d6ab94f80b2cd3f9b23c6d46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_q4.onnx:   0%|          | 0.00/279M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d763743b604cbf8c70c83db67ea761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_q4f16.onnx:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ead79e046494205ac7ee5974cbe2e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_quantized.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08be6dad596a4b8cad6ccc54d0a766c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_quantized.onnx:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0aef5bbbec4945b48a41c4855a646a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a53405b055f4932b83f6cb75f4b2b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68400a26a3f841148f075fbc3b0ebdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "smolvlm-data.pdf: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f145e2ead12942488c80e7c2b029e64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3beebb094b8a4880865569d1d0548d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d253df6f7fe748f381e9771274b2785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b65991be7f4094804c600b8376a717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038268e04eba46c8b586a406b219fcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_uint8.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\BBBS-AI-01\\\\d\\\\models\\\\hub\\\\models--HuggingFaceTB--SmolVLM-Instruct\\\\snapshots\\\\81cd9a775a4d644f2faf4e7becff4559b46b14c7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    resume_download=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of images in the text [0] and images [1] should be the same.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 274\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Run VLM once per second (important)\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_time \u001b[38;5;241m-\u001b[39m prev_classify_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m--> 274\u001b[0m     new_activity \u001b[38;5;241m=\u001b[39m classify_activity(frame)\n\u001b[0;32m    275\u001b[0m     prev_classify_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# PHONE TIMER RESET LOGIC\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 141\u001b[0m, in \u001b[0;36mclassify_activity\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m    117\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n\u001b[0;32m    119\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|vision_start|><|image_pad|><|vision_end|>\u001b[39m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[38;5;124m    You are an expert activity recognition model.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124m    Answer:\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 141\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mprompt, images\u001b[38;5;241m=\u001b[39mimg, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# output_ids = model.generate(**inputs, max_new_tokens=20)\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \n\u001b[0;32m    147\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,   \u001b[38;5;66;03m# Greedy decoding for consistency\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\idefics3\\processing_idefics3.py:294\u001b[0m, in \u001b[0;36mIdefics3Processor.__call__\u001b[1;34m(self, images, text, audio, videos, image_seq_len, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_images_in_images \u001b[38;5;241m!=\u001b[39m n_images_in_text:\n\u001b[1;32m--> 294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    295\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of images in the text \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_images_in_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and images \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_images_in_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be the same.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m         )\n\u001b[0;32m    298\u001b[0m     image_rows \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)])\n\u001b[0;32m    299\u001b[0m     image_cols \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcols\u001b[39m\u001b[38;5;124m\"\u001b[39m, [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)])\n",
      "\u001b[1;31mValueError\u001b[0m: The number of images in the text [0] and images [1] should be the same."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "# model_name = \"openbmb/minicpm-v\"\n",
    "# model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Enhanced Motion Detection for Productive Work\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "def analyze_motion_pattern(frame_gray, prev_gray):\n",
    "    \"\"\"Enhanced motion analysis with region-based detection\"\"\"\n",
    "    h, w = frame_gray.shape\n",
    "    \n",
    "    # Calculate overall motion\n",
    "    diff = cv2.absdiff(frame_gray, prev_gray)\n",
    "    overall_motion = np.sum(diff) / (h * w)\n",
    "    \n",
    "    # Region-based motion analysis (work area detection)\n",
    "    # Bottom half = work area (hands, tools, desk)\n",
    "    # Top half = upper body (less relevant for work detection)\n",
    "    work_area = diff[h//2:, :]\n",
    "    upper_area = diff[:h//2, :]\n",
    "    \n",
    "    work_motion = np.sum(work_area) / work_area.size\n",
    "    upper_motion = np.sum(upper_area) / upper_area.size\n",
    "    \n",
    "    # Detect localized motion (small, precise movements = productive work)\n",
    "    # Apply threshold to get significant motion pixels\n",
    "    _, motion_mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of motion regions\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Analyze motion characteristics\n",
    "    num_motion_regions = len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "    \n",
    "    # Hand-like motion detection (small to medium regions)\n",
    "    hand_motion_regions = [c for c in contours if 100 < cv2.contourArea(c) < 5000]\n",
    "    hand_motion_score = len(hand_motion_regions)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_motion,\n",
    "        'work_area': work_motion,\n",
    "        'upper_area': upper_motion,\n",
    "        'num_regions': num_motion_regions,\n",
    "        'hand_score': hand_motion_score,\n",
    "        'work_ratio': work_motion / (upper_motion + 0.1)  # Avoid division by zero\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, motion_history):\n",
    "    \"\"\"Determine if motion pattern indicates productive work\"\"\"\n",
    "    \n",
    "    # Thresholds (tune these based on your environment)\n",
    "    WORK_MOTION_MIN = 2.0      # Minimum motion in work area\n",
    "    WORK_MOTION_MAX = 15.0     # Maximum (too much = not focused)\n",
    "    HAND_SCORE_MIN = 2         # Minimum hand-like motion regions\n",
    "    WORK_RATIO_MIN = 1.2       # Work area should have more motion than upper body\n",
    "    \n",
    "    # Add current stats to history\n",
    "    motion_history.append(motion_stats)\n",
    "    if len(motion_history) > MOTION_HISTORY_SIZE:\n",
    "        motion_history.pop(0)\n",
    "    \n",
    "    # Analyze recent motion pattern (smoothing)\n",
    "    if len(motion_history) >= 3:\n",
    "        avg_work_motion = np.mean([m['work_area'] for m in motion_history[-5:]])\n",
    "        avg_hand_score = np.mean([m['hand_score'] for m in motion_history[-5:]])\n",
    "        avg_work_ratio = np.mean([m['work_ratio'] for m in motion_history[-5:]])\n",
    "        consistency = np.std([m['work_area'] for m in motion_history[-5:]])\n",
    "        \n",
    "        # Productive work indicators:\n",
    "        # 1. Moderate, consistent motion in work area\n",
    "        # 2. Multiple small motion regions (hands working)\n",
    "        # 3. More motion in lower half than upper half\n",
    "        # 4. Consistent pattern (not erratic)\n",
    "        \n",
    "        is_productive = (\n",
    "            WORK_MOTION_MIN < avg_work_motion < WORK_MOTION_MAX and\n",
    "            avg_hand_score >= HAND_SCORE_MIN and\n",
    "            avg_work_ratio >= WORK_RATIO_MIN and\n",
    "            consistency < 5.0  # Consistent motion pattern\n",
    "        )\n",
    "        \n",
    "        return is_productive, avg_work_motion, avg_hand_score\n",
    "    \n",
    "    return False, 0, 0\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = \"\"\"<|vision_start|><|image_pad|><|vision_end|>\n",
    "\n",
    "        You are an expert activity recognition model.\n",
    "\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text, explanations, or repeats.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # output_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "        output_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=5,  # Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\n",
    "            do_sample=False,   # Greedy decoding for consistency\n",
    "        )\n",
    "\n",
    "    raw_result = processor.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # ENHANCED MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "    \n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        \n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "    \n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC (Enhanced)\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    \n",
    "    # Enhanced decision making with motion analysis\n",
    "    if motion_stats:\n",
    "        # Strong productive motion pattern detected\n",
    "        if is_productive_motion:\n",
    "            if \"idle\" in result or \"unknown\" in result:\n",
    "                return \"assembling_drone\"  # Override VLM with motion evidence\n",
    "            return \"assembling_drone\"\n",
    "        \n",
    "        # Minimal motion detected\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        \n",
    "        # Medium motion but not productive pattern\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            if \"idle\" in result:\n",
    "                return \"simply_sitting\"  # Some movement but not working\n",
    "            elif \"unknown\" in result:\n",
    "                return \"simply_sitting\"\n",
    "        \n",
    "        # High overall motion but not in work area (body movement)\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"  # Moving but not working\n",
    "    \n",
    "    # Fallback to simple logic\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "# cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print final metrics\n",
    "metrics.print_summary()\n",
    "accuracy_metrics.print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe095fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bac771",
   "metadata": {},
   "source": [
    "# BENCHMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d482dd",
   "metadata": {},
   "source": [
    "## Calculation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "# ===================================================\n",
    "# METRICS CALCULATION MODULE\n",
    "# ===================================================\n",
    "\n",
    "class InferenceMetrics:\n",
    "    def __init__(self):\n",
    "        self.frame_times = []\n",
    "        self.model_times = []\n",
    "        self.motion_times = []\n",
    "        self.total_frames = 0\n",
    "        self.start_time = None\n",
    "        self.gpu_memory_peak = 0\n",
    "        self.cpu_memory_peak = 0\n",
    "        \n",
    "    def get_gpu_memory(self):\n",
    "        \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024**2\n",
    "        return 0\n",
    "    \n",
    "    def get_cpu_memory(self):\n",
    "        \"\"\"Get current CPU memory usage in MB\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024**2\n",
    "    \n",
    "    def record_model_inference(self, elapsed_ms):\n",
    "        \"\"\"Record model inference time\"\"\"\n",
    "        self.model_times.append(elapsed_ms)\n",
    "        \n",
    "    def record_motion_detection(self, elapsed_ms):\n",
    "        \"\"\"Record motion detection time\"\"\"\n",
    "        self.motion_times.append(elapsed_ms)\n",
    "    \n",
    "    def record_frame(self, elapsed_ms):\n",
    "        \"\"\"Record total frame processing time\"\"\"\n",
    "        self.frame_times.append(elapsed_ms)\n",
    "        self.total_frames += 1\n",
    "        \n",
    "        # Update peak memory\n",
    "        gpu_mem = self.get_gpu_memory()\n",
    "        cpu_mem = self.get_cpu_memory()\n",
    "        self.gpu_memory_peak = max(self.gpu_memory_peak, gpu_mem)\n",
    "        self.cpu_memory_peak = max(self.cpu_memory_peak, cpu_mem)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Generate metrics summary\"\"\"\n",
    "        if not self.frame_times:\n",
    "            return \"No data collected\"\n",
    "        \n",
    "        import statistics\n",
    "        \n",
    "        summary = {\n",
    "            \"total_frames\": self.total_frames,\n",
    "            \"avg_frame_time_ms\": statistics.mean(self.frame_times),\n",
    "            \"median_frame_time_ms\": statistics.median(self.frame_times),\n",
    "            \"min_frame_time_ms\": min(self.frame_times),\n",
    "            \"max_frame_time_ms\": max(self.frame_times),\n",
    "            \"fps\": 1000 / statistics.mean(self.frame_times),\n",
    "        }\n",
    "        \n",
    "        if self.model_times:\n",
    "            summary[\"avg_model_time_ms\"] = statistics.mean(self.model_times)\n",
    "            summary[\"model_time_percent\"] = (statistics.mean(self.model_times) / statistics.mean(self.frame_times)) * 100\n",
    "        \n",
    "        if self.motion_times:\n",
    "            summary[\"avg_motion_time_ms\"] = statistics.mean(self.motion_times)\n",
    "            summary[\"motion_time_percent\"] = (statistics.mean(self.motion_times) / statistics.mean(self.frame_times)) * 100\n",
    "        \n",
    "        summary[\"gpu_memory_peak_mb\"] = self.gpu_memory_peak\n",
    "        summary[\"cpu_memory_peak_mb\"] = self.cpu_memory_peak\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted summary\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        if isinstance(summary, str):\n",
    "            print(summary)\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INFERENCE METRICS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Frames: {summary['total_frames']}\")\n",
    "        print(f\"Average FPS: {summary['fps']:.2f}\")\n",
    "        print(f\"Average Frame Time: {summary['avg_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Median Frame Time: {summary['median_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Min Frame Time: {summary['min_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Max Frame Time: {summary['max_frame_time_ms']:.2f} ms\")\n",
    "        \n",
    "        if \"avg_model_time_ms\" in summary:\n",
    "            print(f\"\\nModel Inference Time: {summary['avg_model_time_ms']:.2f} ms ({summary['model_time_percent']:.1f}%)\")\n",
    "        \n",
    "        if \"avg_motion_time_ms\" in summary:\n",
    "            print(f\"Motion Detection Time: {summary['avg_motion_time_ms']:.2f} ms ({summary['motion_time_percent']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nGPU Memory Peak: {summary['gpu_memory_peak_mb']:.2f} MB\")\n",
    "        print(f\"CPU Memory Peak: {summary['cpu_memory_peak_mb']:.2f} MB\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = InferenceMetrics()\n",
    "\n",
    "# Function to measure model loading time\n",
    "def measure_model_loading():\n",
    "    \"\"\"Measure model and processor loading time\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    processor_load_start = time.time()\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    processor_load_time = (time.time() - processor_load_start) * 1000\n",
    "    \n",
    "    model_load_start = time.time()\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "    model_load_time = (time.time() - model_load_start) * 1000\n",
    "    \n",
    "    total_load_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL LOADING METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processor Loading: {processor_load_time:.2f} ms\")\n",
    "    print(f\"Model Loading: {model_load_time:.2f} ms\")\n",
    "    print(f\"Total Loading: {total_load_time:.2f} ms\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return processor, model\n",
    "\n",
    "# Accuracy metrics (if you have ground truth labels)\n",
    "class ActivityAccuracyMetrics:\n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.ground_truth = []\n",
    "        \n",
    "    def add_prediction(self, pred, gt):\n",
    "        \"\"\"Add prediction and ground truth\"\"\"\n",
    "        self.predictions.append(pred)\n",
    "        self.ground_truth.append(gt)\n",
    "    \n",
    "    def get_accuracy(self):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        if not self.predictions:\n",
    "            return 0\n",
    "        correct = sum(1 for p, g in zip(self.predictions, self.ground_truth) if p == g)\n",
    "        return (correct / len(self.predictions)) * 100\n",
    "    \n",
    "    def get_confusion_matrix(self):\n",
    "        \"\"\"Get confusion matrix\"\"\"\n",
    "        from collections import defaultdict\n",
    "        confusion = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for pred, gt in zip(self.predictions, self.ground_truth):\n",
    "            confusion[gt][pred] += 1\n",
    "        \n",
    "        return dict(confusion)\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print accuracy report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ACCURACY METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Accuracy: {self.get_accuracy():.2f}%\")\n",
    "        print(f\"Total Predictions: {len(self.predictions)}\")\n",
    "        \n",
    "        confusion = self.get_confusion_matrix()\n",
    "        if confusion:\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            for gt, preds in confusion.items():\n",
    "                print(f\"  Ground Truth '{gt}':\")\n",
    "                for pred, count in preds.items():\n",
    "                    print(f\"    -> Predicted '{pred}': {count}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "accuracy_metrics = ActivityAccuracyMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf201f",
   "metadata": {},
   "source": [
    "## Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# BENCHMARK TEST \n",
    "# ===================================================\n",
    "\n",
    "def benchmark_model(num_frames=10):\n",
    "    \"\"\"Benchmark model on random frames\"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\nStarting benchmark test...\")\n",
    "    print(f\"Testing {num_frames} frames\\n\")\n",
    "    \n",
    "    benchmark_metrics = InferenceMetrics()\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # Create random frame\n",
    "        random_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        frame_start = time.time()\n",
    "        \n",
    "        # Dummy image processing\n",
    "        img = Image.fromarray(cv2.cvtColor(random_frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        prompt = \"Classify: assembling_drone, idle, using_phone, or unknown. Answer:\"\n",
    "        inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Measure inference\n",
    "        model_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n",
    "        model_elapsed = (time.time() - model_start) * 1000\n",
    "        benchmark_metrics.record_model_inference(model_elapsed)\n",
    "        \n",
    "        # Process result\n",
    "        processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        frame_elapsed = (time.time() - frame_start) * 1000\n",
    "        benchmark_metrics.record_frame(frame_elapsed)\n",
    "        \n",
    "        print(f\"Frame {i+1}/{num_frames}: {frame_elapsed:.2f}ms (Model: {model_elapsed:.2f}ms)\")\n",
    "    \n",
    "    benchmark_metrics.print_summary()\n",
    "    return benchmark_metrics\n",
    "\n",
    "# Uncomment to run benchmark:\n",
    "# benchmark_metrics = benchmark_model(num_frames=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
