{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fade6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deba0e9b7b544aa18d1f1b795cab590f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BBBS-AI-01\\d\\models\\hub\\models--Qwen--Qwen2-VL-2B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4990cffa7d6f4763b97a9b20e5cf8020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cab44fead24559b12953eb2cb4a3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f983b210d20944f982925bbac32fe64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe9224fcb91406f82cd05d6ac04f3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9964425d28c54e71af4b239f4dae9811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fd26ae738446c78051b9abc9a0875b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b81869c57b47329ad79c7d35ae6001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6ff97ea10f42f88b4f117bd8c3995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e89bb4abc404628842149f30e1e0121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21fe894d00145c5babbe4f1400b57eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Classification using VLM model\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = \"\"\"<|vision_start|><|image_pad|><|vision_end|>\n",
    "\n",
    "        You are an expert activity recognition model.\n",
    "\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text, explanations, or repeats.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "    raw_result = processor.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion = 0\n",
    "\n",
    "    if prev_frame_gray is not None:\n",
    "        diff = cv2.absdiff(frame_gray, prev_frame_gray)\n",
    "        motion = np.sum(diff) / (frame_gray.shape[0] * frame_gray.shape[1])\n",
    "\n",
    "    prev_frame_gray = frame_gray\n",
    "    MOTION_THRESHOLD = 3   # tune this\n",
    "\n",
    "    print(\"Motion:\", motion)\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "\n",
    "    # If VLM is unsure:\n",
    "    if \"idle\" in result and motion > MOTION_THRESHOLD:\n",
    "        return \"simply_sitting\"\n",
    "    elif \"unknown\" in result and motion > MOTION_THRESHOLD:\n",
    "        return \"working\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_20251105_141950.997_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"pls assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\" simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
