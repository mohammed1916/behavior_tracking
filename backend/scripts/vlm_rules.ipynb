{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67fade6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf528f465c3b42e5afda9dd6b9d20fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 7.35, Work Area: 9.74, Hand Score: 20, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 2.90, Work Area: 3.07, Hand Score: 3, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 4.18, Work Area: 5.43, Hand Score: 14, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 4.89, Work Area: 5.91, Hand Score: 16, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 3.11, Work Area: 3.34, Hand Score: 4, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 2.40, Work Area: 2.32, Hand Score: 1, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 2.30, Work Area: 2.14, Hand Score: 0, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 2.42, Work Area: 2.43, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 2.47, Work Area: 2.48, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 7.70, Work Area: 9.63, Hand Score: 13, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 3.05, Work Area: 3.26, Hand Score: 1, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 4.96, Work Area: 5.64, Hand Score: 20, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 3.31, Work Area: 3.70, Hand Score: 4, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 10.19, Work Area: 11.29, Hand Score: 16, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 8.91, Work Area: 10.54, Hand Score: 19, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 10.91, Work Area: 15.72, Hand Score: 17, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 14.28, Work Area: 20.68, Hand Score: 20, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 9.24, Work Area: 13.24, Hand Score: 25, Productive: True\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 3.35, Work Area: 3.97, Hand Score: 5, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         unknown\n",
      "Motion - Overall: 11.37, Work Area: 18.10, Hand Score: 16, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         using_phone\n",
      "Motion - Overall: 30.33, Work Area: 39.33, Hand Score: 19, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         using_phone\n",
      "Motion - Overall: 6.46, Work Area: 6.54, Hand Score: 10, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         using_phone\n",
      "Motion - Overall: 3.29, Work Area: 3.05, Hand Score: 6, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         using_phone\n",
      "Motion - Overall: 17.88, Work Area: 25.14, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         using_phone\n",
      "Motion - Overall: 32.49, Work Area: 36.31, Hand Score: 40, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         using_phone\n",
      "Motion - Overall: 5.94, Work Area: 6.01, Hand Score: 14, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 33.08, Work Area: 55.06, Hand Score: 8, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 36.31, Work Area: 36.89, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 3.96, Work Area: 4.49, Hand Score: 10, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 4.18, Work Area: 4.77, Hand Score: 16, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 4.31, Work Area: 5.01, Hand Score: 13, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "         idle\n",
      "Motion - Overall: 3.43, Work Area: 3.67, Hand Score: 6, Productive: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 264\u001b[0m\n\u001b[0;32m    261\u001b[0m phone_missing_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Enhanced Motion Detection for Productive Work\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "def analyze_motion_pattern(frame_gray, prev_gray):\n",
    "    \"\"\"Enhanced motion analysis with region-based detection\"\"\"\n",
    "    h, w = frame_gray.shape\n",
    "    \n",
    "    # Calculate overall motion\n",
    "    diff = cv2.absdiff(frame_gray, prev_gray)\n",
    "    overall_motion = np.sum(diff) / (h * w)\n",
    "    \n",
    "    # Region-based motion analysis (work area detection)\n",
    "    # Bottom half = work area (hands, tools, desk)\n",
    "    # Top half = upper body (less relevant for work detection)\n",
    "    work_area = diff[h//2:, :]\n",
    "    upper_area = diff[:h//2, :]\n",
    "    \n",
    "    work_motion = np.sum(work_area) / work_area.size\n",
    "    upper_motion = np.sum(upper_area) / upper_area.size\n",
    "    \n",
    "    # Detect localized motion (small, precise movements = productive work)\n",
    "    # Apply threshold to get significant motion pixels\n",
    "    _, motion_mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of motion regions\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Analyze motion characteristics\n",
    "    num_motion_regions = len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "    \n",
    "    # Hand-like motion detection (small to medium regions)\n",
    "    hand_motion_regions = [c for c in contours if 100 < cv2.contourArea(c) < 5000]\n",
    "    hand_motion_score = len(hand_motion_regions)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_motion,\n",
    "        'work_area': work_motion,\n",
    "        'upper_area': upper_motion,\n",
    "        'num_regions': num_motion_regions,\n",
    "        'hand_score': hand_motion_score,\n",
    "        'work_ratio': work_motion / (upper_motion + 0.1)  # Avoid division by zero\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, motion_history):\n",
    "    \"\"\"Determine if motion pattern indicates productive work\"\"\"\n",
    "    \n",
    "    # Thresholds (tune these based on your environment)\n",
    "    WORK_MOTION_MIN = 2.0      # Minimum motion in work area\n",
    "    WORK_MOTION_MAX = 15.0     # Maximum (too much = not focused)\n",
    "    HAND_SCORE_MIN = 2         # Minimum hand-like motion regions\n",
    "    WORK_RATIO_MIN = 1.2       # Work area should have more motion than upper body\n",
    "    \n",
    "    # Add current stats to history\n",
    "    motion_history.append(motion_stats)\n",
    "    if len(motion_history) > MOTION_HISTORY_SIZE:\n",
    "        motion_history.pop(0)\n",
    "    \n",
    "    # Analyze recent motion pattern (smoothing)\n",
    "    if len(motion_history) >= 3:\n",
    "        avg_work_motion = np.mean([m['work_area'] for m in motion_history[-5:]])\n",
    "        avg_hand_score = np.mean([m['hand_score'] for m in motion_history[-5:]])\n",
    "        avg_work_ratio = np.mean([m['work_ratio'] for m in motion_history[-5:]])\n",
    "        consistency = np.std([m['work_area'] for m in motion_history[-5:]])\n",
    "        \n",
    "        # Productive work indicators:\n",
    "        # 1. Moderate, consistent motion in work area\n",
    "        # 2. Multiple small motion regions (hands working)\n",
    "        # 3. More motion in lower half than upper half\n",
    "        # 4. Consistent pattern (not erratic)\n",
    "        \n",
    "        is_productive = (\n",
    "            WORK_MOTION_MIN < avg_work_motion < WORK_MOTION_MAX and\n",
    "            avg_hand_score >= HAND_SCORE_MIN and\n",
    "            avg_work_ratio >= WORK_RATIO_MIN and\n",
    "            consistency < 5.0  # Consistent motion pattern\n",
    "        )\n",
    "        \n",
    "        return is_productive, avg_work_motion, avg_hand_score\n",
    "    \n",
    "    return False, 0, 0\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = \"\"\"<|vision_start|><|image_pad|><|vision_end|>\n",
    "\n",
    "        You are an expert activity recognition model.\n",
    "\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text, explanations, or repeats.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # output_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "        output_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=5,  # Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\n",
    "            do_sample=False,   # Greedy decoding for consistency\n",
    "        )\n",
    "\n",
    "    raw_result = processor.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # ENHANCED MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "    \n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        \n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "    \n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC (Enhanced)\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    \n",
    "    # Enhanced decision making with motion analysis\n",
    "    if motion_stats:\n",
    "        # Strong productive motion pattern detected\n",
    "        if is_productive_motion:\n",
    "            if \"idle\" in result or \"unknown\" in result:\n",
    "                return \"assembling_drone\"  # Override VLM with motion evidence\n",
    "            return \"assembling_drone\"\n",
    "        \n",
    "        # Minimal motion detected\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        \n",
    "        # Medium motion but not productive pattern\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            if \"idle\" in result:\n",
    "                return \"simply_sitting\"  # Some movement but not working\n",
    "            elif \"unknown\" in result:\n",
    "                return \"simply_sitting\"\n",
    "        \n",
    "        # High overall motion but not in work area (body movement)\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"  # Moving but not working\n",
    "    \n",
    "    # Fallback to simple logic\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "# cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"pls assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1983c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained(\"qwen_vlm_2b_activity_model\")\n",
    "# processor.save_pretrained(\"qwen_vlm_2b_activity_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6538647e9f4c4f8a9cbef449e1a391a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BBBS-AI-01\\d\\models\\hub\\models--llava-hf--llava-1.5-7b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d1f996784e4f1dbe4fefece4b26668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a075e5550450ca1e783029d9bb895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519a9f8ee588484b8bbb78fb8a0d14b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183489ed6a3242bfa922ef8a5ba5a29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bb5d6471294345aa40c447a49b2ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e009699a757e475497d67bf0b00c3e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a124c3ecd7a9403497ce92e50ec71729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873eb0f171be4771b4aab6b6091fae60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515b3e51460440e5832e5bbeadd0f03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0bdb061e2e4fee83076303215c76cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00387684cf854668ad6b6c0e6cf63be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa98cddc94504d2f935f74ffc6fe437c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8973059fd934158a721e84822e357ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca6b26dbedc443292312ce1259cc643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "# model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Enhanced Motion Detection for Productive Work\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "def analyze_motion_pattern(frame_gray, prev_gray):\n",
    "    \"\"\"Enhanced motion analysis with region-based detection\"\"\"\n",
    "    h, w = frame_gray.shape\n",
    "    \n",
    "    # Calculate overall motion\n",
    "    diff = cv2.absdiff(frame_gray, prev_gray)\n",
    "    overall_motion = np.sum(diff) / (h * w)\n",
    "    \n",
    "    # Region-based motion analysis (work area detection)\n",
    "    # Bottom half = work area (hands, tools, desk)\n",
    "    # Top half = upper body (less relevant for work detection)\n",
    "    work_area = diff[h//2:, :]\n",
    "    upper_area = diff[:h//2, :]\n",
    "    \n",
    "    work_motion = np.sum(work_area) / work_area.size\n",
    "    upper_motion = np.sum(upper_area) / upper_area.size\n",
    "    \n",
    "    # Detect localized motion (small, precise movements = productive work)\n",
    "    # Apply threshold to get significant motion pixels\n",
    "    _, motion_mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of motion regions\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Analyze motion characteristics\n",
    "    num_motion_regions = len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "    \n",
    "    # Hand-like motion detection (small to medium regions)\n",
    "    hand_motion_regions = [c for c in contours if 100 < cv2.contourArea(c) < 5000]\n",
    "    hand_motion_score = len(hand_motion_regions)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_motion,\n",
    "        'work_area': work_motion,\n",
    "        'upper_area': upper_motion,\n",
    "        'num_regions': num_motion_regions,\n",
    "        'hand_score': hand_motion_score,\n",
    "        'work_ratio': work_motion / (upper_motion + 0.1)  # Avoid division by zero\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, motion_history):\n",
    "    \"\"\"Determine if motion pattern indicates productive work\"\"\"\n",
    "    \n",
    "    # Thresholds (tune these based on your environment)\n",
    "    WORK_MOTION_MIN = 2.0      # Minimum motion in work area\n",
    "    WORK_MOTION_MAX = 15.0     # Maximum (too much = not focused)\n",
    "    HAND_SCORE_MIN = 2         # Minimum hand-like motion regions\n",
    "    WORK_RATIO_MIN = 1.2       # Work area should have more motion than upper body\n",
    "    \n",
    "    # Add current stats to history\n",
    "    motion_history.append(motion_stats)\n",
    "    if len(motion_history) > MOTION_HISTORY_SIZE:\n",
    "        motion_history.pop(0)\n",
    "    \n",
    "    # Analyze recent motion pattern (smoothing)\n",
    "    if len(motion_history) >= 3:\n",
    "        avg_work_motion = np.mean([m['work_area'] for m in motion_history[-5:]])\n",
    "        avg_hand_score = np.mean([m['hand_score'] for m in motion_history[-5:]])\n",
    "        avg_work_ratio = np.mean([m['work_ratio'] for m in motion_history[-5:]])\n",
    "        consistency = np.std([m['work_area'] for m in motion_history[-5:]])\n",
    "        \n",
    "        # Productive work indicators:\n",
    "        # 1. Moderate, consistent motion in work area\n",
    "        # 2. Multiple small motion regions (hands working)\n",
    "        # 3. More motion in lower half than upper half\n",
    "        # 4. Consistent pattern (not erratic)\n",
    "        \n",
    "        is_productive = (\n",
    "            WORK_MOTION_MIN < avg_work_motion < WORK_MOTION_MAX and\n",
    "            avg_hand_score >= HAND_SCORE_MIN and\n",
    "            avg_work_ratio >= WORK_RATIO_MIN and\n",
    "            consistency < 5.0  # Consistent motion pattern\n",
    "        )\n",
    "        \n",
    "        return is_productive, avg_work_motion, avg_hand_score\n",
    "    \n",
    "    return False, 0, 0\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = \"\"\"<|vision_start|><|image_pad|><|vision_end|>\n",
    "\n",
    "        You are an expert activity recognition model.\n",
    "\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text, explanations, or repeats.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # output_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "        output_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=5,  # Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\n",
    "            do_sample=False,   # Greedy decoding for consistency\n",
    "        )\n",
    "\n",
    "    raw_result = processor.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # ENHANCED MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "    \n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        \n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "    \n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC (Enhanced)\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    \n",
    "    # Enhanced decision making with motion analysis\n",
    "    if motion_stats:\n",
    "        # Strong productive motion pattern detected\n",
    "        if is_productive_motion:\n",
    "            if \"idle\" in result or \"unknown\" in result:\n",
    "                return \"assembling_drone\"  # Override VLM with motion evidence\n",
    "            return \"assembling_drone\"\n",
    "        \n",
    "        # Minimal motion detected\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        \n",
    "        # Medium motion but not productive pattern\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            if \"idle\" in result:\n",
    "                return \"simply_sitting\"  # Some movement but not working\n",
    "            elif \"unknown\" in result:\n",
    "                return \"simply_sitting\"\n",
    "        \n",
    "        # High overall motion but not in work area (body movement)\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"  # Moving but not working\n",
    "    \n",
    "    # Fallback to simple logic\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "# cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print final metrics\n",
    "metrics.print_summary()\n",
    "accuracy_metrics.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bac771",
   "metadata": {},
   "source": [
    "# BENCHMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d482dd",
   "metadata": {},
   "source": [
    "## Calculation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "# ===================================================\n",
    "# METRICS CALCULATION MODULE\n",
    "# ===================================================\n",
    "\n",
    "class InferenceMetrics:\n",
    "    def __init__(self):\n",
    "        self.frame_times = []\n",
    "        self.model_times = []\n",
    "        self.motion_times = []\n",
    "        self.total_frames = 0\n",
    "        self.start_time = None\n",
    "        self.gpu_memory_peak = 0\n",
    "        self.cpu_memory_peak = 0\n",
    "        \n",
    "    def get_gpu_memory(self):\n",
    "        \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024**2\n",
    "        return 0\n",
    "    \n",
    "    def get_cpu_memory(self):\n",
    "        \"\"\"Get current CPU memory usage in MB\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024**2\n",
    "    \n",
    "    def record_model_inference(self, elapsed_ms):\n",
    "        \"\"\"Record model inference time\"\"\"\n",
    "        self.model_times.append(elapsed_ms)\n",
    "        \n",
    "    def record_motion_detection(self, elapsed_ms):\n",
    "        \"\"\"Record motion detection time\"\"\"\n",
    "        self.motion_times.append(elapsed_ms)\n",
    "    \n",
    "    def record_frame(self, elapsed_ms):\n",
    "        \"\"\"Record total frame processing time\"\"\"\n",
    "        self.frame_times.append(elapsed_ms)\n",
    "        self.total_frames += 1\n",
    "        \n",
    "        # Update peak memory\n",
    "        gpu_mem = self.get_gpu_memory()\n",
    "        cpu_mem = self.get_cpu_memory()\n",
    "        self.gpu_memory_peak = max(self.gpu_memory_peak, gpu_mem)\n",
    "        self.cpu_memory_peak = max(self.cpu_memory_peak, cpu_mem)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Generate metrics summary\"\"\"\n",
    "        if not self.frame_times:\n",
    "            return \"No data collected\"\n",
    "        \n",
    "        import statistics\n",
    "        \n",
    "        summary = {\n",
    "            \"total_frames\": self.total_frames,\n",
    "            \"avg_frame_time_ms\": statistics.mean(self.frame_times),\n",
    "            \"median_frame_time_ms\": statistics.median(self.frame_times),\n",
    "            \"min_frame_time_ms\": min(self.frame_times),\n",
    "            \"max_frame_time_ms\": max(self.frame_times),\n",
    "            \"fps\": 1000 / statistics.mean(self.frame_times),\n",
    "        }\n",
    "        \n",
    "        if self.model_times:\n",
    "            summary[\"avg_model_time_ms\"] = statistics.mean(self.model_times)\n",
    "            summary[\"model_time_percent\"] = (statistics.mean(self.model_times) / statistics.mean(self.frame_times)) * 100\n",
    "        \n",
    "        if self.motion_times:\n",
    "            summary[\"avg_motion_time_ms\"] = statistics.mean(self.motion_times)\n",
    "            summary[\"motion_time_percent\"] = (statistics.mean(self.motion_times) / statistics.mean(self.frame_times)) * 100\n",
    "        \n",
    "        summary[\"gpu_memory_peak_mb\"] = self.gpu_memory_peak\n",
    "        summary[\"cpu_memory_peak_mb\"] = self.cpu_memory_peak\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted summary\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        if isinstance(summary, str):\n",
    "            print(summary)\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INFERENCE METRICS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Frames: {summary['total_frames']}\")\n",
    "        print(f\"Average FPS: {summary['fps']:.2f}\")\n",
    "        print(f\"Average Frame Time: {summary['avg_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Median Frame Time: {summary['median_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Min Frame Time: {summary['min_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Max Frame Time: {summary['max_frame_time_ms']:.2f} ms\")\n",
    "        \n",
    "        if \"avg_model_time_ms\" in summary:\n",
    "            print(f\"\\nModel Inference Time: {summary['avg_model_time_ms']:.2f} ms ({summary['model_time_percent']:.1f}%)\")\n",
    "        \n",
    "        if \"avg_motion_time_ms\" in summary:\n",
    "            print(f\"Motion Detection Time: {summary['avg_motion_time_ms']:.2f} ms ({summary['motion_time_percent']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nGPU Memory Peak: {summary['gpu_memory_peak_mb']:.2f} MB\")\n",
    "        print(f\"CPU Memory Peak: {summary['cpu_memory_peak_mb']:.2f} MB\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = InferenceMetrics()\n",
    "\n",
    "# Function to measure model loading time\n",
    "def measure_model_loading():\n",
    "    \"\"\"Measure model and processor loading time\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    processor_load_start = time.time()\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    processor_load_time = (time.time() - processor_load_start) * 1000\n",
    "    \n",
    "    model_load_start = time.time()\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "    model_load_time = (time.time() - model_load_start) * 1000\n",
    "    \n",
    "    total_load_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL LOADING METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processor Loading: {processor_load_time:.2f} ms\")\n",
    "    print(f\"Model Loading: {model_load_time:.2f} ms\")\n",
    "    print(f\"Total Loading: {total_load_time:.2f} ms\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return processor, model\n",
    "\n",
    "# Accuracy metrics (if you have ground truth labels)\n",
    "class ActivityAccuracyMetrics:\n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.ground_truth = []\n",
    "        \n",
    "    def add_prediction(self, pred, gt):\n",
    "        \"\"\"Add prediction and ground truth\"\"\"\n",
    "        self.predictions.append(pred)\n",
    "        self.ground_truth.append(gt)\n",
    "    \n",
    "    def get_accuracy(self):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        if not self.predictions:\n",
    "            return 0\n",
    "        correct = sum(1 for p, g in zip(self.predictions, self.ground_truth) if p == g)\n",
    "        return (correct / len(self.predictions)) * 100\n",
    "    \n",
    "    def get_confusion_matrix(self):\n",
    "        \"\"\"Get confusion matrix\"\"\"\n",
    "        from collections import defaultdict\n",
    "        confusion = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for pred, gt in zip(self.predictions, self.ground_truth):\n",
    "            confusion[gt][pred] += 1\n",
    "        \n",
    "        return dict(confusion)\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print accuracy report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ACCURACY METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Accuracy: {self.get_accuracy():.2f}%\")\n",
    "        print(f\"Total Predictions: {len(self.predictions)}\")\n",
    "        \n",
    "        confusion = self.get_confusion_matrix()\n",
    "        if confusion:\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            for gt, preds in confusion.items():\n",
    "                print(f\"  Ground Truth '{gt}':\")\n",
    "                for pred, count in preds.items():\n",
    "                    print(f\"    -> Predicted '{pred}': {count}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "accuracy_metrics = ActivityAccuracyMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf201f",
   "metadata": {},
   "source": [
    "## Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# BENCHMARK TEST \n",
    "# ===================================================\n",
    "\n",
    "def benchmark_model(num_frames=10):\n",
    "    \"\"\"Benchmark model on random frames\"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\nStarting benchmark test...\")\n",
    "    print(f\"Testing {num_frames} frames\\n\")\n",
    "    \n",
    "    benchmark_metrics = InferenceMetrics()\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # Create random frame\n",
    "        random_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        frame_start = time.time()\n",
    "        \n",
    "        # Dummy image processing\n",
    "        img = Image.fromarray(cv2.cvtColor(random_frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        prompt = \"Classify: assembling_drone, idle, using_phone, or unknown. Answer:\"\n",
    "        inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Measure inference\n",
    "        model_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n",
    "        model_elapsed = (time.time() - model_start) * 1000\n",
    "        benchmark_metrics.record_model_inference(model_elapsed)\n",
    "        \n",
    "        # Process result\n",
    "        processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        frame_elapsed = (time.time() - frame_start) * 1000\n",
    "        benchmark_metrics.record_frame(frame_elapsed)\n",
    "        \n",
    "        print(f\"Frame {i+1}/{num_frames}: {frame_elapsed:.2f}ms (Model: {model_elapsed:.2f}ms)\")\n",
    "    \n",
    "    benchmark_metrics.print_summary()\n",
    "    return benchmark_metrics\n",
    "\n",
    "# Uncomment to run benchmark:\n",
    "# benchmark_metrics = benchmark_model(num_frames=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
