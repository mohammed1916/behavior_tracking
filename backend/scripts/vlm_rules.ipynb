{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238b74b8",
   "metadata": {},
   "source": [
    "# Tests whether Activities succh as using phones are predicted and can be run with cuda and within lightweight 4060 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cecac",
   "metadata": {},
   "source": [
    "## Qwen/Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fade6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c917a6a54ff4ad5a86d49740df0f080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### you are an expert activity recognition model.\n",
      "\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text, explanations, or repeats.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        \n",
      "\n",
      "\n",
      "        the person is\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 272\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Run VLM once per second (important)\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_time \u001b[38;5;241m-\u001b[39m prev_classify_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m--> 272\u001b[0m     new_activity \u001b[38;5;241m=\u001b[39m classify_activity(frame)\n\u001b[0;32m    273\u001b[0m     prev_classify_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# PHONE TIMER RESET LOGIC\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 143\u001b[0m, in \u001b[0;36mclassify_activity\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m    139\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39m[prompt], images\u001b[38;5;241m=\u001b[39m[img], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# output_ids = model.generate(**inputs, max_new_tokens=20)\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \n\u001b[0;32m    145\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,   \u001b[38;5;66;03m# Greedy decoding for consistency\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    149\u001b[0m raw_result \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#########################\u001b[39m\u001b[38;5;124m\"\u001b[39m, raw_result)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   2791\u001b[0m     outputs,\n\u001b[0;32m   2792\u001b[0m     model_kwargs,\n\u001b[0;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2794\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1356\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m   1351\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[0;32m   1352\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1353\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1354\u001b[0m )\n\u001b[1;32m-> 1356\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1357\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1358\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m   1359\u001b[0m     pixel_values_videos\u001b[38;5;241m=\u001b[39mpixel_values_videos,\n\u001b[0;32m   1360\u001b[0m     image_grid_thw\u001b[38;5;241m=\u001b[39mimage_grid_thw,\n\u001b[0;32m   1361\u001b[0m     video_grid_thw\u001b[38;5;241m=\u001b[39mvideo_grid_thw,\n\u001b[0;32m   1362\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1363\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1364\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1365\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1366\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1367\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1368\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1369\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1370\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1372\u001b[0m )\n\u001b[0;32m   1374\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1375\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1220\u001b[0m, in \u001b[0;36mQwen2VLModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m   1217\u001b[0m         delta \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39mrepeat_interleave(batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m delta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1218\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids \u001b[38;5;241m+\u001b[39m delta\u001b[38;5;241m.\u001b[39mto(position_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1220\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[0;32m   1221\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1222\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1223\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1224\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1225\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1226\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1227\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1228\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1229\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1230\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1232\u001b[0m )\n\u001b[0;32m   1234\u001b[0m output \u001b[38;5;241m=\u001b[39m Qwen2VLModelOutputWithPast(\n\u001b[0;32m   1235\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mlast_hidden_state,\n\u001b[0;32m   1236\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mpast_key_values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m     rope_deltas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_deltas,\n\u001b[0;32m   1240\u001b[0m )\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:862\u001b[0m, in \u001b[0;36mQwen2VLTextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    860\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 862\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    863\u001b[0m     hidden_states,\n\u001b[0;32m    864\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type],\n\u001b[0;32m    865\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mtext_position_ids,\n\u001b[0;32m    866\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    867\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    868\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    869\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    870\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    872\u001b[0m )\n\u001b[0;32m    874\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:601\u001b[0m, in \u001b[0;36mQwen2VLDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    602\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    603\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    604\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    605\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    606\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    607\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    608\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    609\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    611\u001b[0m )\n\u001b[0;32m    612\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:526\u001b[0m, in \u001b[0;36mQwen2VLAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    524\u001b[0m     attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 526\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    528\u001b[0m     query_states,\n\u001b[0;32m    529\u001b[0m     key_states,\n\u001b[0;32m    530\u001b[0m     value_states,\n\u001b[0;32m    531\u001b[0m     attention_mask,\n\u001b[0;32m    532\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    533\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    534\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,\n\u001b[0;32m    535\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,  \u001b[38;5;66;03m# pass positions for FA2\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    537\u001b[0m )\n\u001b[0;32m    539\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    540\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\integrations\\sdpa_attention.py:96\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_not(attention_mask\u001b[38;5;241m.\u001b[39mbool())\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 96\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m     97\u001b[0m     query,\n\u001b[0;32m     98\u001b[0m     key,\n\u001b[0;32m     99\u001b[0m     value,\n\u001b[0;32m    100\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    101\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m    102\u001b[0m     scale\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[0;32m    103\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msdpa_kwargs,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Note: device_map=\"auto\" already places the model on the correct device(s),\n",
    "# so we don't need to call .to(device) afterward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1983c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained(\"qwen_vlm_2b_activity_model\")\n",
    "# processor.save_pretrained(\"qwen_vlm_2b_activity_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78e4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e871728fcea4422a9361b68264bdf30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 43 files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964dfaa4e1544efe81c4bda98d4ca78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BBBS-AI-01\\d\\models\\hub\\models--HuggingFaceTB--SmolVLM-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457620cafa1e476d848900b5363315f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ece0e4714f4ddab164dff883ad805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/429 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad078fd9d68f4fe789b55c581ef54c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a8189dbec40d4ad47b2c051bc2b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71a9bc98a5c49a4b3db04426c21c378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SmolVLM.png:   0%|          | 0.00/146k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8844933e3e04574a5a5384274918359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab60fc00c9249b59efc44ebd428a9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51787b2f51b40898316ffacc513ca35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e657ed4c2684314a2516893d1931f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged.onnx_data:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92083d17b4864ce48a547662a89ebc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mixture_the_cauldron.png:   0%|          | 0.00/937k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400a0f26683340608a3d057f3ff01318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_fp16.onnx_data:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bdef3cc31e46cb93fef3eecb498e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_bnb4.onnx:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d570d89b7e87409fb659eb934629ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_int8.onnx:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047d000f89c46f0b4cd71a5ca2cda2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged.onnx:   0%|          | 0.00/168k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3641137e68f4e139db7edfb6f8699d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_fp16.onnx:   0%|          | 0.00/169k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca7225a120b4a68929d803198969f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_q4.onnx:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19be5a520b64bc6837035dc0708a10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_q4f16.onnx:   0%|          | 0.00/965M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d1704e77e947f8a78773d4d347c41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_quantized.onnx:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6e54166d214fa387993401126d7271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens.onnx:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f3be4ed189414d9768daf662d1392f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_uint8.onnx:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3974bd57e9f14dc2b20bd1864133771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_bnb4.onnx:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3388280b8246539fd077f711051b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_fp16.onnx:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1887d5a68d5f4de392ade1a0302fd8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_int8.onnx:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c5c24d3bed495aa20b30e213bb7e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_q4.onnx:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e92e3442a240ee96df990838535402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_q4f16.onnx:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33205b6d0e2e4aada90c2e1a9c5add52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder.onnx:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb025028063f48e5bbaf7bd4c96bfa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_uint8.onnx:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970fa79208b74d4fb294f6374719a426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_bnb4.onnx:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb307d8bbf049f9aeec992c5576df56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_fp16.onnx:   0%|          | 0.00/869M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52814d795cf9483796fb3ad2019a6325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_int8.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f888c40d6ab94f80b2cd3f9b23c6d46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_q4.onnx:   0%|          | 0.00/279M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d763743b604cbf8c70c83db67ea761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_q4f16.onnx:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ead79e046494205ac7ee5974cbe2e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_quantized.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08be6dad596a4b8cad6ccc54d0a766c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/embed_tokens_quantized.onnx:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0aef5bbbec4945b48a41c4855a646a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a53405b055f4932b83f6cb75f4b2b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68400a26a3f841148f075fbc3b0ebdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "smolvlm-data.pdf: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f145e2ead12942488c80e7c2b029e64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3beebb094b8a4880865569d1d0548d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d253df6f7fe748f381e9771274b2785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b65991be7f4094804c600b8376a717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038268e04eba46c8b586a406b219fcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vision_encoder_uint8.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\BBBS-AI-01\\\\d\\\\models\\\\hub\\\\models--HuggingFaceTB--SmolVLM-Instruct\\\\snapshots\\\\81cd9a775a4d644f2faf4e7becff4559b46b14c7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    resume_download=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09f986",
   "metadata": {},
   "source": [
    "## HuggingFaceTB/SmolVLM-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### <row_1_col_1><row_1_col_2><row_1_col_3><row_1_col_4>\n",
      "<row_2_col_1><row_2_col_2><row_2_col_3><row_2_col_4>\n",
      "<row_3_col_1><row_3_col_2><row_3_col_3><row_3_col_4>\n",
      "\n",
      "<global-img>\n",
      "        you are an expert activity recognition model.\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        1. assembling_dr\n",
      "######################### <row_1_col_1><row_1_col_2><row_1_col_3><row_1_col_4>\n",
      "<row_2_col_1><row_2_col_2><row_2_col_3><row_2_col_4>\n",
      "<row_3_col_1><row_3_col_2><row_3_col_3><row_3_col_4>\n",
      "\n",
      "<global-img>\n",
      "        you are an expert activity recognition model.\n",
      "        look only at the main person in the image. ignore all other people or objects.\n",
      "        classify their current action into exactly one label from the following:\n",
      "\n",
      "        1. assembling_drone → the person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
      "        2. idle → the person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
      "        3. using_phone → the person is clearly holding or interacting with a phone.\n",
      "        4. unknown → if the activity cannot be confidently identified.\n",
      "\n",
      "        rules:\n",
      "        - do not guess.\n",
      "        - only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
      "        - do not add any extra text.\n",
      "        - end your answer with \"\"\n",
      "\n",
      "        answer:\n",
      "        1. assembling_dr\n",
      "Motion - Overall: 47.80, Work Area: 68.92, Hand Score: 14, Productive: False\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "# model_name = \"openbmb/minicpm-v\"\n",
    "# model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    # device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Enhanced Motion Detection for Productive Work\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "def analyze_motion_pattern(frame_gray, prev_gray):\n",
    "    \"\"\"Enhanced motion analysis with region-based detection\"\"\"\n",
    "    h, w = frame_gray.shape\n",
    "    \n",
    "    # Calculate overall motion\n",
    "    diff = cv2.absdiff(frame_gray, prev_gray)\n",
    "    overall_motion = np.sum(diff) / (h * w)\n",
    "    \n",
    "    # Region-based motion analysis (work area detection)\n",
    "    # Bottom half = work area (hands, tools, desk)\n",
    "    # Top half = upper body (less relevant for work detection)\n",
    "    work_area = diff[h//2:, :]\n",
    "    upper_area = diff[:h//2, :]\n",
    "    \n",
    "    work_motion = np.sum(work_area) / work_area.size\n",
    "    upper_motion = np.sum(upper_area) / upper_area.size\n",
    "    \n",
    "    # Detect localized motion (small, precise movements = productive work)\n",
    "    # Apply threshold to get significant motion pixels\n",
    "    _, motion_mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of motion regions\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Analyze motion characteristics\n",
    "    num_motion_regions = len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "    \n",
    "    # Hand-like motion detection (small to medium regions)\n",
    "    hand_motion_regions = [c for c in contours if 100 < cv2.contourArea(c) < 5000]\n",
    "    hand_motion_score = len(hand_motion_regions)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_motion,\n",
    "        'work_area': work_motion,\n",
    "        'upper_area': upper_motion,\n",
    "        'num_regions': num_motion_regions,\n",
    "        'hand_score': hand_motion_score,\n",
    "        'work_ratio': work_motion / (upper_motion + 0.1)  # Avoid division by zero\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, motion_history):\n",
    "    \"\"\"Determine if motion pattern indicates productive work\"\"\"\n",
    "    \n",
    "    # Thresholds (tune these based on your environment)\n",
    "    WORK_MOTION_MIN = 2.0      # Minimum motion in work area\n",
    "    WORK_MOTION_MAX = 15.0     # Maximum (too much = not focused)\n",
    "    HAND_SCORE_MIN = 2         # Minimum hand-like motion regions\n",
    "    WORK_RATIO_MIN = 1.2       # Work area should have more motion than upper body\n",
    "    \n",
    "    # Add current stats to history\n",
    "    motion_history.append(motion_stats)\n",
    "    if len(motion_history) > MOTION_HISTORY_SIZE:\n",
    "        motion_history.pop(0)\n",
    "    \n",
    "    # Analyze recent motion pattern (smoothing)\n",
    "    if len(motion_history) >= 3:\n",
    "        avg_work_motion = np.mean([m['work_area'] for m in motion_history[-5:]])\n",
    "        avg_hand_score = np.mean([m['hand_score'] for m in motion_history[-5:]])\n",
    "        avg_work_ratio = np.mean([m['work_ratio'] for m in motion_history[-5:]])\n",
    "        consistency = np.std([m['work_area'] for m in motion_history[-5:]])\n",
    "        \n",
    "        # Productive work indicators:\n",
    "        # 1. Moderate, consistent motion in work area\n",
    "        # 2. Multiple small motion regions (hands working)\n",
    "        # 3. More motion in lower half than upper half\n",
    "        # 4. Consistent pattern (not erratic)\n",
    "        \n",
    "        is_productive = (\n",
    "            WORK_MOTION_MIN < avg_work_motion < WORK_MOTION_MAX and\n",
    "            avg_hand_score >= HAND_SCORE_MIN and\n",
    "            avg_work_ratio >= WORK_RATIO_MIN and\n",
    "            consistency < 5.0  # Consistent motion pattern\n",
    "        )\n",
    "        \n",
    "        return is_productive, avg_work_motion, avg_hand_score\n",
    "    \n",
    "    return False, 0, 0\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = prompt = \"\"\"<image>\n",
    "        You are an expert activity recognition model.\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # output_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "        output_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=5,  # Only need \"assembling_drone\", \"idle\", \"using_phone\", or \"unknown\"\n",
    "            do_sample=False,   # Greedy decoding for consistency\n",
    "        )\n",
    "\n",
    "    raw_result = processor.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # ENHANCED MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "    \n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        \n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "    \n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC (Enhanced)\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    \n",
    "    # Enhanced decision making with motion analysis\n",
    "    if motion_stats:\n",
    "        # Strong productive motion pattern detected\n",
    "        if is_productive_motion:\n",
    "            if \"idle\" in result or \"unknown\" in result:\n",
    "                return \"assembling_drone\"  # Override VLM with motion evidence\n",
    "            return \"assembling_drone\"\n",
    "        \n",
    "        # Minimal motion detected\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        \n",
    "        # Medium motion but not productive pattern\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            if \"idle\" in result:\n",
    "                return \"simply_sitting\"  # Some movement but not working\n",
    "            elif \"unknown\" in result:\n",
    "                return \"simply_sitting\"\n",
    "        \n",
    "        # High overall motion but not in work area (body movement)\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"  # Moving but not working\n",
    "    \n",
    "    # Fallback to simple logic\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "# cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# # Print final metrics\n",
    "# metrics.print_summary()\n",
    "# accuracy_metrics.print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe095fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a3ff7",
   "metadata": {},
   "source": [
    "## jinaai/jina-vlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ffda88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cbf36f069847f8bdda177e6bccb14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### look b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b b b b b b b b b b b bler b b bam bamler b bler blerlerler bler b b b b bler bler b b b blerler b b b b b b b b b b b blerler b b b b b b b b bler b b b blerler b b b b b b b b b b b b b b b b b b b b b b b b b btil bler b b b blerler b b b b b btil b b b b b b b b bler b b b b b b b b b b b b b b b b b b b bler bler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b blerler bler b b blerler b b blerlerler b b b b b b b b b b b b b b bler bler b b b bler b b b b b b b b b bler b b b b b b bler b b b b b b b b b b b b bler bler b b b b b b b b b b b b b bler bler b bler b b b b b b b b b b bler b b b b b b bler b b blerler bler b b b b b b b b b bler b b b b b b b b b b bler b b b b bler b b b bler b bler b b bler bler bler b b b b b b b b b b b b b b b bler b b btil b bler b b b b b b b b b b b b b b b b bler b b btil bler b b b b b b b b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b blerlerlerler b b b b b b b b b b b b b b b b bler b b blerler b b b b b b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b blerler b b b b b b b b b b b b bler b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b b b b bler b b b b b b b bler b bler b b b b bler b b b b bler b b b b b btil b b b b b b b b b b b b b b b b b b b b b b b btil b b b b b b b b b b b b b b b b b b b b b b b b b blerler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b b b b b b br b b br bamessage blerlerler br b b b bessageization b brrr b brlerr br b b b._izationization b._ization b bization b._ br._lerler clz bizationamlerlerlerlerlerlerlerlerlerlerizationlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr b._ cl brr bizationler brrization b b bization._ler b cller b b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "######################### look b b b b b b b b b b b b bler b b b b b b b look b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b bler blerlerlerler b b b b b bler bler b b b b bler b b b b b b b bler b blerlerler b b b b b b b b bler b b b blerler b b b b b b b b bler b b b b b b b b b b b b b b b btil bler b bam b b b b b b b b btil b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b bler b b b b b b b b b blerlerler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b bler b b blerler b b b blerler b b b b b b b b b b b b b b b b b b bler b b bler b b b b b b b b b b b b b b b b b b b bler b b bler b b b b b bler b b b b b b b bler bler b b b b b b blerler b b b b b b b b b b b b bler b bler b b bler b b b b bler b b b b b b b b b b b b b b b b b btil b b b b b b b b b bler b b b b b b b bler b b b btil b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b b b blerlerlerler b b b b b b b b b b b b b b b b b b b b blerler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b bler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b btil b b b b b b b b b b b b b b b b b b b b b b b btil b b b b b b b b b b b blerler b b b b b b b b b b b blerler b b b b b b b bler b bler b b b b b b b b b bler b b b b b b b b b b b b b b b b b b b b b b b b b bler b b b b bler b bler b b b b b b b b b b b br b b br bamessage blerlerler br b b b bessageization b brrr b brlerr br b b b._izationization b._ization b bization bz br._lerler clessage bizationamlerlerlerlerlerlerlerlerlerlerizationlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr b._ cl brr bizationler brrization b b bization._ler b cller b b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "Motion - Overall: 4.60, Work Area: 6.46, Hand Score: 16, Productive: False\n",
      "######################### look b b b b b b b b b b b b b b b cl b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b br b b br bam�lerlerlerler br b b b bizationization b bizationrizationization blerlerr br b bizationessageizationizationization._ization b bization b._ b._._lerler cl._ bizationamleressagelerlerlerlerlerlerlerlerlerlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr br cl brr bizationler brrizationizationization bizationessageler b cller cl b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "Motion - Overall: 109.54, Work Area: 147.39, Hand Score: 0, Productive: False\n",
      "######################### look b b b b b b b b b b b b b b b cl b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b br b b br bam�lerlerlerler br b b b bizationization b bizationrizationization blerlerr br b bizationessageizationizationization._ization b bization b._ b._._lerler cl._ bizationamleressagelerlerlerlerlerlerlerlerlerlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr br cl brr bizationler brrizationizationization bizationessageler b cller cl b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### look b b b b b b b b b b b b b b b cl b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b br b b br bam�lerlerlerler br b b b bizationization b bizationrizationization blerlerr br b bizationessageizationizationization._ization b bization b._ b._._lerler cl._ bizationamleressagelerlerlerlerlerlerlerlerlerlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr br cl brr bizationler brrizationizationization bizationessageler b cller cl b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### look b b b b b b b b b b b b b b b cl b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b br b b br bam�lerlerlerler br b b b bizationization b bizationrizationization blerlerr br b bizationessageizationizationization._ization b bization b._ b._._lerler cl._ bizationamleressagelerlerlerlerlerlerlerlerlerlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr br cl brr bizationler brrizationizationization bizationessageler b cller cl b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n",
      "######################### look b b b b b b b b b b b b b b b cl b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b br b b br bam�lerlerlerler br b b b bizationization b bizationrizationization blerlerr br b bizationessageizationizationization._ization b bization b._ b._._lerler cl._ bizationamleressagelerlerlerlerlerlerlerlerlerlerler cllerler cllerlerlerlerler b b b bler clr b blerlerlerlerler blerlerlerlerlerlerlerlerlerler b b b bler cl clr b blerler bler b b bler b b b bler clr b bler blerlerr br cl brr bizationler brrizationizationization bizationessageler b cller cl b b cl b b cl brr bizationlerlerlerr brr b b bessage b b b._ cl b b\n",
      "Motion - Overall: 0.00, Work Area: 0.00, Hand Score: 0, Productive: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 277\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Run VLM once per second (important)\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_time \u001b[38;5;241m-\u001b[39m prev_classify_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m--> 277\u001b[0m     new_activity \u001b[38;5;241m=\u001b[39m classify_activity(frame)\n\u001b[0;32m    278\u001b[0m     prev_classify_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# PHONE TIMER RESET LOGIC\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 143\u001b[0m, in \u001b[0;36mclassify_activity\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m    140\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39m[prompt], images\u001b[38;5;241m=\u001b[39m[img], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m--> 143\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# output[0] = logits from decoder\u001b[39;00m\n\u001b[0;32m    146\u001b[0m logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]                      \u001b[38;5;66;03m# [B, seq, vocab]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\d\\models\\modules\\transformers_modules\\jinaai\\jina_hyphen_vlm\\e224ac6aa893135e06eee77d06d7bb1faec2a627\\modeling_jvlm.py:500\u001b[0m, in \u001b[0;36mJinaVLM.forward\u001b[1;34m(self, input_ids, inputs_embeds, images, image_masks, image_input_idx, attention_mask, causal_mask, response_mask, position_ids, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     image_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(images, image_masks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    499\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m image_out\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[0;32m    501\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    502\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    503\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    504\u001b[0m     causal_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    505\u001b[0m     response_mask\u001b[38;5;241m=\u001b[39mresponse_mask,\n\u001b[0;32m    506\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    507\u001b[0m     image_features\u001b[38;5;241m=\u001b[39mimage_features,\n\u001b[0;32m    508\u001b[0m     image_input_idx\u001b[38;5;241m=\u001b[39mimage_input_idx,\n\u001b[0;32m    509\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    510\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    511\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    513\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\d\\models\\modules\\transformers_modules\\jinaai\\jina_hyphen_vlm\\e224ac6aa893135e06eee77d06d7bb1faec2a627\\modeling_jvlm.py:421\u001b[0m, in \u001b[0;36mJinaVLMTextModel.forward\u001b[1;34m(self, input_ids, inputs_embeds, image_features, image_input_idx, attention_mask, causal_mask, response_mask, position_ids, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m all_attention_weights \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 421\u001b[0m     x, att_weights \u001b[38;5;241m=\u001b[39m layer(\n\u001b[0;32m    422\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    423\u001b[0m         rope_embeddings\u001b[38;5;241m=\u001b[39mrope_embeddings,\n\u001b[0;32m    424\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    425\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    426\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    427\u001b[0m         drop_mask\u001b[38;5;241m=\u001b[39mresponse_mask,\n\u001b[0;32m    428\u001b[0m         attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation,\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    431\u001b[0m     all_hidden_states\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m att_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\d\\models\\modules\\transformers_modules\\jinaai\\jina_hyphen_vlm\\e224ac6aa893135e06eee77d06d7bb1faec2a627\\blocks_jvlm.py:1152\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, rope_embeddings, attention_mask, past_key_values, cache_position, drop_mask, attn_implementation, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1142\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[0;32m   1150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostnorm:\n\u001b[1;32m-> 1152\u001b[0m         x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_lnorm(x)\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1154\u001b[0m         x_norm \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\d\\models\\modules\\transformers_modules\\jinaai\\jina_hyphen_vlm\\e224ac6aa893135e06eee77d06d7bb1faec2a627\\blocks_jvlm.py:578\u001b[0m, in \u001b[0;36mRMSLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    576\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    577\u001b[0m     variance \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 578\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[0;32m    579\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(og_dtype)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModel\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# ------------------------------------------------------\n",
    "# Load Qwen2-VL 2B (CORRECT)\n",
    "# ------------------------------------------------------\n",
    "model_name = \"jinaai/jina-vlm\"\n",
    "# model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "# model_name = \"openbmb/minicpm-v\"\n",
    "# model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Enhanced Motion Detection for Productive Work\n",
    "# ------------------------------------------------------\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "def analyze_motion_pattern(frame_gray, prev_gray):\n",
    "    \"\"\"Enhanced motion analysis with region-based detection\"\"\"\n",
    "    h, w = frame_gray.shape\n",
    "    \n",
    "    # Calculate overall motion\n",
    "    diff = cv2.absdiff(frame_gray, prev_gray)\n",
    "    overall_motion = np.sum(diff) / (h * w)\n",
    "    \n",
    "    # Region-based motion analysis (work area detection)\n",
    "    # Bottom half = work area (hands, tools, desk)\n",
    "    # Top half = upper body (less relevant for work detection)\n",
    "    work_area = diff[h//2:, :]\n",
    "    upper_area = diff[:h//2, :]\n",
    "    \n",
    "    work_motion = np.sum(work_area) / work_area.size\n",
    "    upper_motion = np.sum(upper_area) / upper_area.size\n",
    "    \n",
    "    # Detect localized motion (small, precise movements = productive work)\n",
    "    # Apply threshold to get significant motion pixels\n",
    "    _, motion_mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of motion regions\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Analyze motion characteristics\n",
    "    num_motion_regions = len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "    \n",
    "    # Hand-like motion detection (small to medium regions)\n",
    "    hand_motion_regions = [c for c in contours if 100 < cv2.contourArea(c) < 5000]\n",
    "    hand_motion_score = len(hand_motion_regions)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_motion,\n",
    "        'work_area': work_motion,\n",
    "        'upper_area': upper_motion,\n",
    "        'num_regions': num_motion_regions,\n",
    "        'hand_score': hand_motion_score,\n",
    "        'work_ratio': work_motion / (upper_motion + 0.1)  # Avoid division by zero\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, motion_history):\n",
    "    \"\"\"Determine if motion pattern indicates productive work\"\"\"\n",
    "    \n",
    "    # Thresholds (tune these based on your environment)\n",
    "    WORK_MOTION_MIN = 2.0      # Minimum motion in work area\n",
    "    WORK_MOTION_MAX = 15.0     # Maximum (too much = not focused)\n",
    "    HAND_SCORE_MIN = 2         # Minimum hand-like motion regions\n",
    "    WORK_RATIO_MIN = 1.2       # Work area should have more motion than upper body\n",
    "    \n",
    "    # Add current stats to history\n",
    "    motion_history.append(motion_stats)\n",
    "    if len(motion_history) > MOTION_HISTORY_SIZE:\n",
    "        motion_history.pop(0)\n",
    "    \n",
    "    # Analyze recent motion pattern (smoothing)\n",
    "    if len(motion_history) >= 3:\n",
    "        avg_work_motion = np.mean([m['work_area'] for m in motion_history[-5:]])\n",
    "        avg_hand_score = np.mean([m['hand_score'] for m in motion_history[-5:]])\n",
    "        avg_work_ratio = np.mean([m['work_ratio'] for m in motion_history[-5:]])\n",
    "        consistency = np.std([m['work_area'] for m in motion_history[-5:]])\n",
    "        \n",
    "        # Productive work indicators:\n",
    "        # 1. Moderate, consistent motion in work area\n",
    "        # 2. Multiple small motion regions (hands working)\n",
    "        # 3. More motion in lower half than upper half\n",
    "        # 4. Consistent pattern (not erratic)\n",
    "        \n",
    "        is_productive = (\n",
    "            WORK_MOTION_MIN < avg_work_motion < WORK_MOTION_MAX and\n",
    "            avg_hand_score >= HAND_SCORE_MIN and\n",
    "            avg_work_ratio >= WORK_RATIO_MIN and\n",
    "            consistency < 5.0  # Consistent motion pattern\n",
    "        )\n",
    "        \n",
    "        return is_productive, avg_work_motion, avg_hand_score\n",
    "    \n",
    "    return False, 0, 0\n",
    "\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = prompt = \"\"\"<image>\n",
    "        You are an expert activity recognition model.\n",
    "        Look ONLY at the MAIN PERSON in the image. Ignore all other people or objects.\n",
    "        Classify their CURRENT ACTION into exactly ONE label from the following:\n",
    "\n",
    "        1. assembling_drone → The person is working with tools, touching a drone, handling drone parts, connecting wires, tightening screws, or performing assembly actions.\n",
    "        2. idle → The person is standing or sitting without doing any task, arms resting, not interacting with objects.\n",
    "        3. using_phone → The person is clearly holding or interacting with a phone.\n",
    "        4. unknown → If the activity cannot be confidently identified.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT guess.\n",
    "        - Only output exactly one label: assembling_drone, idle, using_phone, or unknown.\n",
    "        - Do not add any extra text.\n",
    "        - End your answer with \"<|endoftext|>\"\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    # output[0] = logits from decoder\n",
    "    logits = output[0]                      # [B, seq, vocab]\n",
    "    token_ids = logits.argmax(dim=-1)       # pick best token each step\n",
    "\n",
    "    raw_result = processor.decode(\n",
    "        token_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    ).strip().lower()\n",
    "\n",
    "\n",
    "    print(\"#########################\", raw_result)\n",
    "\n",
    "    # last non-empty line\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # ENHANCED MOTION DETECTION\n",
    "    # ----------------------------\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "    \n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        \n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "    \n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # ----------------------------\n",
    "    # FINAL DECISION LOGIC (Enhanced)\n",
    "    # ----------------------------\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    \n",
    "    # Enhanced decision making with motion analysis\n",
    "    if motion_stats:\n",
    "        # Strong productive motion pattern detected\n",
    "        if is_productive_motion:\n",
    "            if \"idle\" in result or \"unknown\" in result:\n",
    "                return \"assembling_drone\"  # Override VLM with motion evidence\n",
    "            return \"assembling_drone\"\n",
    "        \n",
    "        # Minimal motion detected\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        \n",
    "        # Medium motion but not productive pattern\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            if \"idle\" in result:\n",
    "                return \"simply_sitting\"  # Some movement but not working\n",
    "            elif \"unknown\" in result:\n",
    "                return \"simply_sitting\"\n",
    "        \n",
    "        # High overall motion but not in work area (body movement)\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"  # Moving but not working\n",
    "    \n",
    "    # Fallback to simple logic\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    else:\n",
    "        return \"idle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "drone_limit=20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "# cap = cv2.VideoCapture(\"drone.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'XVID' or 'mp4v'\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Run VLM once per second (important)\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # PHONE TIMER RESET LOGIC\n",
    "        # ----------------------------------------------------\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            # Reset only if phone is missing for enough frames\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            # Normal activity change\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "\n",
    "    if current_activity == \"working\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Assemble drone\"\n",
    "\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"simply_sitting for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "\n",
    "    if current_activity == \"assembling_drone\" and elapsed > drone_limit:\n",
    "        alert_message = f\"drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451d7da",
   "metadata": {},
   "source": [
    "## OpenGVLab/InternVL2-2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n",
      "Starting activity monitoring...press 'q' to quit.\n",
      "\n",
      "######################### unknown\n",
      "######################### unknown\n",
      "Motion - Overall: 3.03, Work Area: 3.64, Hand Score: 2.595952690972222, Productive: False\n",
      "######################### unknown\n",
      "Motion - Overall: 3.10, Work Area: 3.76, Hand Score: 2.697775607638889, Productive: False\n",
      "######################### unknown\n",
      "Motion - Overall: 23.16, Work Area: 45.28, Hand Score: 22.99506293402778, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 5.06, Work Area: 6.56, Hand Score: 5.727951388888889, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 23.32, Work Area: 44.40, Hand Score: 23.191102430555556, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 13.38, Work Area: 25.07, Hand Score: 12.03232421875, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 12.60, Work Area: 27.81, Hand Score: 5.116167534722222, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 18.12, Work Area: 19.79, Hand Score: 16.70355902777778, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 22.01, Work Area: 38.39, Hand Score: 11.345236545138889, Productive: True\n",
      "######################### unknown\n",
      "Motion - Overall: 30.81, Work Area: 49.20, Hand Score: 19.17800564236111, Productive: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Model / processor setup \n",
    "# ------------------------------------------------------\n",
    "model_name = \"OpenGVLab/InternVL2-2B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Load as AutoModel which will auto-select the right class\n",
    "try:\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load with AutoModel: {e}\")\n",
    "    # Try alternate loading\n",
    "    from transformers import InternVLChatModel\n",
    "    model = InternVLChatModel.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Motion analysis functions\n",
    "# ------------------------------------------------------\n",
    "def analyze_motion_pattern(curr_frame, prev_frame):\n",
    "    diff = cv2.absdiff(curr_frame, prev_frame)\n",
    "    motion_score = np.mean(diff)\n",
    "\n",
    "    h, w = diff.shape\n",
    "    work_area = diff[int(h * 0.5):h, int(w * 0.2):int(w * 0.8)]\n",
    "    work_score = np.mean(work_area)\n",
    "\n",
    "    hand_area = diff[:int(h * 0.5), int(w * 0.2):int(w * 0.8)]\n",
    "    hand_score = np.mean(hand_area)\n",
    "\n",
    "    return {\n",
    "        \"overall\": motion_score,\n",
    "        \"work_area\": work_score,\n",
    "        \"hand_score\": hand_score,\n",
    "        \"work_ratio\": (work_score + 1e-5) / (motion_score + 1e-5)\n",
    "    }\n",
    "\n",
    "def classify_motion_as_productive(motion_stats, history):\n",
    "    history.append(motion_stats[\"work_area\"])\n",
    "    if len(history) > 10:\n",
    "        history.pop(0)\n",
    "\n",
    "    avg_work = np.mean(history)\n",
    "    productive = avg_work > 10 and motion_stats[\"hand_score\"] > 5\n",
    "    return productive, avg_work, motion_stats[\"hand_score\"]\n",
    "\n",
    "prev_frame_gray = None\n",
    "motion_history = []\n",
    "MOTION_HISTORY_SIZE = 10\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# InternVL2-based activity classification\n",
    "# ------------------------------------------------------\n",
    "def classify_activity(frame):\n",
    "    global prev_frame_gray, motion_history\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    prompt = \"\"\"\n",
    "You are an expert activity recognition model.\n",
    "Look ONLY at the MAIN PERSON in the image.\n",
    "\n",
    "Classify their CURRENT ACTION into exactly ONE label:\n",
    "- assembling_drone\n",
    "- idle\n",
    "- using_phone\n",
    "- unknown\n",
    "\n",
    "Rules:\n",
    "- No guessing.\n",
    "- Output only ONE label.\n",
    "- No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        try:\n",
    "            # Convert PIL image to tensor format\n",
    "            img_array = np.array(img) / 255.0\n",
    "            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "            if device == \"cuda\":\n",
    "                img_tensor = img_tensor.half()\n",
    "            \n",
    "            # Tokenize prompt only\n",
    "            inputs = processor(prompt, return_tensors='pt').to(device)\n",
    "            \n",
    "            # Manually call model with image and text\n",
    "            outputs = model(input_ids=inputs.input_ids, pixel_values=img_tensor, max_new_tokens=50)\n",
    "            raw_result = processor.decode(outputs, skip_special_tokens=True).strip().lower()\n",
    "        except:\n",
    "            # Fallback: try simpler approach silently\n",
    "            try:\n",
    "                img_array = np.array(img).astype(np.float32) / 255.0\n",
    "                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                if device == \"cuda\":\n",
    "                    img_tensor = img_tensor.half()\n",
    "                inputs = processor(text=prompt, return_tensors='pt').to(device)\n",
    "                outputs = model.generate(input_ids=inputs.input_ids, pixel_values=img_tensor, max_new_tokens=50)\n",
    "                raw_result = processor.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "            except:\n",
    "                raw_result = \"unknown\"\n",
    "\n",
    "        print(\"#########################\", raw_result)\n",
    "\n",
    "    # Clean output\n",
    "    lines = [line.strip() for line in raw_result.splitlines() if line.strip()]\n",
    "    result = lines[-1] if lines else \"unknown\"\n",
    "\n",
    "    # Motion analysis\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_stats = None\n",
    "    is_productive_motion = False\n",
    "\n",
    "    if prev_frame_gray is not None:\n",
    "        motion_stats = analyze_motion_pattern(frame_gray, prev_frame_gray)\n",
    "        is_productive_motion, avg_work_motion, hand_score = classify_motion_as_productive(\n",
    "            motion_stats, motion_history\n",
    "        )\n",
    "        print(f\"Motion - Overall: {motion_stats['overall']:.2f}, Work Area: {motion_stats['work_area']:.2f}, \"\n",
    "              f\"Hand Score: {motion_stats['hand_score']}, Productive: {is_productive_motion}\")\n",
    "\n",
    "    prev_frame_gray = frame_gray\n",
    "\n",
    "    # High-level fusion logic\n",
    "    if \"phone\" in result:\n",
    "        return \"using_phone\"\n",
    "    if \"assemble\" in result or \"drone\" in result:\n",
    "        return \"assembling_drone\"\n",
    "    if motion_stats:\n",
    "        if is_productive_motion:\n",
    "            return \"assembling_drone\"\n",
    "        if motion_stats['overall'] < 2.0:\n",
    "            return \"idle\"\n",
    "        if 2.0 <= motion_stats['overall'] < 8.0:\n",
    "            return \"simply_sitting\"\n",
    "        if motion_stats['overall'] > 8.0 and motion_stats['work_ratio'] < 1.0:\n",
    "            return \"simply_sitting\"\n",
    "    if \"idle\" in result:\n",
    "        return \"idle\"\n",
    "    elif \"unknown\" in result:\n",
    "        return \"simply_sitting\"\n",
    "    return \"idle\"\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Activity tracking + CSV logging setup\n",
    "# ------------------------------------------------------\n",
    "current_activity = \"unknown\"\n",
    "activity_start_time = time.time()\n",
    "log_data = []\n",
    "CSV_FILE = \"activity_log.csv\"\n",
    "\n",
    "IDLE_LIMIT = 10\n",
    "PHONE_LIMIT = 10\n",
    "DRONE_LIMIT = 20\n",
    "\n",
    "def log_activity(activity, start, end):\n",
    "    duration = round(end - start, 2)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_data.append([timestamp, activity, duration])\n",
    "    df = pd.DataFrame(log_data, columns=[\"timestamp\", \"activity\", \"duration_sec\"])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Real-time video stream\n",
    "# ------------------------------------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Try a few frames to see if camera is actually working\n",
    "test_frames = 0\n",
    "for i in range(5):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        test_frames += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "if test_frames == 0:\n",
    "    print(\"Error: Could not access camera (no frames captured). Camera may not be available.\")\n",
    "    print(\"Please ensure a camera is connected and functioning properly.\")\n",
    "    cap.release()\n",
    "    sys.exit(1)\n",
    "\n",
    "# Reset camera\n",
    "cap.release()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"data_collection_.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "prev_classify_time = time.time()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "alert_message = \"\"\n",
    "\n",
    "PHONE_RESET_FRAMES = 5\n",
    "phone_missing_frames = 0\n",
    "\n",
    "print(\"Starting activity monitoring...press 'q' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - prev_classify_time >= 1.0:\n",
    "        new_activity = classify_activity(frame)\n",
    "        prev_classify_time = current_time\n",
    "\n",
    "        # PHONE RESET LOGIC\n",
    "        if current_activity == \"using_phone\":\n",
    "            if new_activity != \"using_phone\":\n",
    "                phone_missing_frames += 1\n",
    "            else:\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "            if phone_missing_frames >= PHONE_RESET_FRAMES:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "        else:\n",
    "            if new_activity != current_activity:\n",
    "                log_activity(current_activity, activity_start_time, current_time)\n",
    "                current_activity = new_activity\n",
    "                activity_start_time = current_time\n",
    "                phone_missing_frames = 0\n",
    "\n",
    "    elapsed = current_time - activity_start_time\n",
    "    alert_message = \"\"\n",
    "    if current_activity == \"idle\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Idle for {int(elapsed)} sec\"\n",
    "    if current_activity == \"simply_sitting\" and elapsed > IDLE_LIMIT:\n",
    "        alert_message = f\"Simply sitting for {int(elapsed)} sec\"\n",
    "    if current_activity == \"using_phone\" and elapsed > PHONE_LIMIT:\n",
    "        alert_message = f\"Phone usage for {int(elapsed)} sec\"\n",
    "    if current_activity == \"assembling_drone\" and elapsed > DRONE_LIMIT:\n",
    "        alert_message = f\"Drone usage limit exceeded\"\n",
    "\n",
    "    cv2.putText(frame, f\"Activity: {current_activity}\", (30, 40), font, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Time: {int(elapsed)} sec\", (30, 80), font, 1, (255, 255, 0), 2)\n",
    "    if alert_message:\n",
    "        cv2.putText(frame, alert_message, (30, 120), font, 1, (0, 0, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Drone Assembly Monitoring\", frame)\n",
    "    out.write(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "log_activity(current_activity, activity_start_time, time.time())\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bac771",
   "metadata": {},
   "source": [
    "# BENCHMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d482dd",
   "metadata": {},
   "source": [
    "## Calculation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "# ===================================================\n",
    "# METRICS CALCULATION MODULE\n",
    "# ===================================================\n",
    "\n",
    "class InferenceMetrics:\n",
    "    def __init__(self):\n",
    "        self.frame_times = []\n",
    "        self.model_times = []\n",
    "        self.motion_times = []\n",
    "        self.total_frames = 0\n",
    "        self.start_time = None\n",
    "        self.gpu_memory_peak = 0\n",
    "        self.cpu_memory_peak = 0\n",
    "        \n",
    "    def get_gpu_memory(self):\n",
    "        \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024**2\n",
    "        return 0\n",
    "    \n",
    "    def get_cpu_memory(self):\n",
    "        \"\"\"Get current CPU memory usage in MB\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024**2\n",
    "    \n",
    "    def record_model_inference(self, elapsed_ms):\n",
    "        \"\"\"Record model inference time\"\"\"\n",
    "        self.model_times.append(elapsed_ms)\n",
    "        \n",
    "    def record_motion_detection(self, elapsed_ms):\n",
    "        \"\"\"Record motion detection time\"\"\"\n",
    "        self.motion_times.append(elapsed_ms)\n",
    "    \n",
    "    def record_frame(self, elapsed_ms):\n",
    "        \"\"\"Record total frame processing time\"\"\"\n",
    "        self.frame_times.append(elapsed_ms)\n",
    "        self.total_frames += 1\n",
    "        \n",
    "        # Update peak memory\n",
    "        gpu_mem = self.get_gpu_memory()\n",
    "        cpu_mem = self.get_cpu_memory()\n",
    "        self.gpu_memory_peak = max(self.gpu_memory_peak, gpu_mem)\n",
    "        self.cpu_memory_peak = max(self.cpu_memory_peak, cpu_mem)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Generate metrics summary\"\"\"\n",
    "        if not self.frame_times:\n",
    "            return \"No data collected\"\n",
    "        \n",
    "        import statistics\n",
    "        \n",
    "        summary = {\n",
    "            \"total_frames\": self.total_frames,\n",
    "            \"avg_frame_time_ms\": statistics.mean(self.frame_times),\n",
    "            \"median_frame_time_ms\": statistics.median(self.frame_times),\n",
    "            \"min_frame_time_ms\": min(self.frame_times),\n",
    "            \"max_frame_time_ms\": max(self.frame_times),\n",
    "            \"fps\": 1000 / statistics.mean(self.frame_times),\n",
    "        }\n",
    "        \n",
    "        if self.model_times:\n",
    "            summary[\"avg_model_time_ms\"] = statistics.mean(self.model_times)\n",
    "            summary[\"model_time_percent\"] = (statistics.mean(self.model_times) / statistics.mean(self.frame_times)) * 100\n",
    "        \n",
    "        if self.motion_times:\n",
    "            summary[\"avg_motion_time_ms\"] = statistics.mean(self.motion_times)\n",
    "            summary[\"motion_time_percent\"] = (statistics.mean(self.motion_times) / statistics.mean(self.frame_times)) * 100\n",
    "        \n",
    "        summary[\"gpu_memory_peak_mb\"] = self.gpu_memory_peak\n",
    "        summary[\"cpu_memory_peak_mb\"] = self.cpu_memory_peak\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted summary\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        if isinstance(summary, str):\n",
    "            print(summary)\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INFERENCE METRICS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Frames: {summary['total_frames']}\")\n",
    "        print(f\"Average FPS: {summary['fps']:.2f}\")\n",
    "        print(f\"Average Frame Time: {summary['avg_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Median Frame Time: {summary['median_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Min Frame Time: {summary['min_frame_time_ms']:.2f} ms\")\n",
    "        print(f\"Max Frame Time: {summary['max_frame_time_ms']:.2f} ms\")\n",
    "        \n",
    "        if \"avg_model_time_ms\" in summary:\n",
    "            print(f\"\\nModel Inference Time: {summary['avg_model_time_ms']:.2f} ms ({summary['model_time_percent']:.1f}%)\")\n",
    "        \n",
    "        if \"avg_motion_time_ms\" in summary:\n",
    "            print(f\"Motion Detection Time: {summary['avg_motion_time_ms']:.2f} ms ({summary['motion_time_percent']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nGPU Memory Peak: {summary['gpu_memory_peak_mb']:.2f} MB\")\n",
    "        print(f\"CPU Memory Peak: {summary['cpu_memory_peak_mb']:.2f} MB\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = InferenceMetrics()\n",
    "\n",
    "# Function to measure model loading time\n",
    "def measure_model_loading():\n",
    "    \"\"\"Measure model and processor loading time\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    processor_load_start = time.time()\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    processor_load_time = (time.time() - processor_load_start) * 1000\n",
    "    \n",
    "    model_load_start = time.time()\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "    model_load_time = (time.time() - model_load_start) * 1000\n",
    "    \n",
    "    total_load_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL LOADING METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processor Loading: {processor_load_time:.2f} ms\")\n",
    "    print(f\"Model Loading: {model_load_time:.2f} ms\")\n",
    "    print(f\"Total Loading: {total_load_time:.2f} ms\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return processor, model\n",
    "\n",
    "# Accuracy metrics (if you have ground truth labels)\n",
    "class ActivityAccuracyMetrics:\n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.ground_truth = []\n",
    "        \n",
    "    def add_prediction(self, pred, gt):\n",
    "        \"\"\"Add prediction and ground truth\"\"\"\n",
    "        self.predictions.append(pred)\n",
    "        self.ground_truth.append(gt)\n",
    "    \n",
    "    def get_accuracy(self):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        if not self.predictions:\n",
    "            return 0\n",
    "        correct = sum(1 for p, g in zip(self.predictions, self.ground_truth) if p == g)\n",
    "        return (correct / len(self.predictions)) * 100\n",
    "    \n",
    "    def get_confusion_matrix(self):\n",
    "        \"\"\"Get confusion matrix\"\"\"\n",
    "        from collections import defaultdict\n",
    "        confusion = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for pred, gt in zip(self.predictions, self.ground_truth):\n",
    "            confusion[gt][pred] += 1\n",
    "        \n",
    "        return dict(confusion)\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print accuracy report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ACCURACY METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Accuracy: {self.get_accuracy():.2f}%\")\n",
    "        print(f\"Total Predictions: {len(self.predictions)}\")\n",
    "        \n",
    "        confusion = self.get_confusion_matrix()\n",
    "        if confusion:\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            for gt, preds in confusion.items():\n",
    "                print(f\"  Ground Truth '{gt}':\")\n",
    "                for pred, count in preds.items():\n",
    "                    print(f\"    -> Predicted '{pred}': {count}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "accuracy_metrics = ActivityAccuracyMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf201f",
   "metadata": {},
   "source": [
    "## Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88c74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# BENCHMARK TEST \n",
    "# ===================================================\n",
    "\n",
    "def benchmark_model(num_frames=10):\n",
    "    \"\"\"Benchmark model on random frames\"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\nStarting benchmark test...\")\n",
    "    print(f\"Testing {num_frames} frames\\n\")\n",
    "    \n",
    "    benchmark_metrics = InferenceMetrics()\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # Create random frame\n",
    "        random_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        frame_start = time.time()\n",
    "        \n",
    "        # Dummy image processing\n",
    "        img = Image.fromarray(cv2.cvtColor(random_frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        prompt = \"Classify: assembling_drone, idle, using_phone, or unknown. Answer:\"\n",
    "        inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Measure inference\n",
    "        model_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n",
    "        model_elapsed = (time.time() - model_start) * 1000\n",
    "        benchmark_metrics.record_model_inference(model_elapsed)\n",
    "        \n",
    "        # Process result\n",
    "        processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        frame_elapsed = (time.time() - frame_start) * 1000\n",
    "        benchmark_metrics.record_frame(frame_elapsed)\n",
    "        \n",
    "        print(f\"Frame {i+1}/{num_frames}: {frame_elapsed:.2f}ms (Model: {model_elapsed:.2f}ms)\")\n",
    "    \n",
    "    benchmark_metrics.print_summary()\n",
    "    return benchmark_metrics\n",
    "\n",
    "# Uncomment to run benchmark:\n",
    "# benchmark_metrics = benchmark_model(num_frames=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
