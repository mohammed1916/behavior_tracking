{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c595a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt': 100% ━━━━━━━━━━━━ 83.7MB 9.5MB/s 8.8s 8.8s<0.0s4s\n",
      "\n",
      "0: 480x640 (no detections), 55.0ms\n",
      "Speed: 4.0ms preprocess, 55.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.0ms\n",
      "Speed: 1.5ms preprocess, 17.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.8ms\n",
      "Speed: 9.5ms preprocess, 16.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.8ms\n",
      "Speed: 1.5ms preprocess, 16.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.6ms\n",
      "Speed: 1.8ms preprocess, 17.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 80.2ms\n",
      "Speed: 1.9ms preprocess, 80.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.0ms\n",
      "Speed: 2.5ms preprocess, 79.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 157.5ms\n",
      "Speed: 2.9ms preprocess, 157.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 95.7ms\n",
      "Speed: 2.7ms preprocess, 95.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 83.4ms\n",
      "Speed: 1.9ms preprocess, 83.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 80.2ms\n",
      "Speed: 2.0ms preprocess, 80.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 144.7ms\n",
      "Speed: 2.7ms preprocess, 144.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 73.9ms\n",
      "Speed: 2.1ms preprocess, 73.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 130.3ms\n",
      "Speed: 2.7ms preprocess, 130.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 154.0ms\n",
      "Speed: 2.6ms preprocess, 154.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 128.3ms\n",
      "Speed: 3.0ms preprocess, 128.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 85.3ms\n",
      "Speed: 1.9ms preprocess, 85.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.1ms\n",
      "Speed: 20.0ms preprocess, 37.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.8ms\n",
      "Speed: 1.5ms preprocess, 15.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.0ms\n",
      "Speed: 2.4ms preprocess, 16.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 27.2ms\n",
      "Speed: 2.3ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 25.4ms\n",
      "Speed: 3.0ms preprocess, 25.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 26.1ms\n",
      "Speed: 2.3ms preprocess, 26.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 25.6ms\n",
      "Speed: 1.7ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.9ms\n",
      "Speed: 3.0ms preprocess, 39.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 28.3ms\n",
      "Speed: 2.5ms preprocess, 28.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 67.9ms\n",
      "Speed: 2.8ms preprocess, 67.9ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 146.9ms\n",
      "Speed: 2.7ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 33.2ms\n",
      "Speed: 2.2ms preprocess, 33.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 89.1ms\n",
      "Speed: 3.0ms preprocess, 89.1ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 82.3ms\n",
      "Speed: 3.2ms preprocess, 82.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 81.0ms\n",
      "Speed: 3.3ms preprocess, 81.0ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 77.8ms\n",
      "Speed: 3.3ms preprocess, 77.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 148.9ms\n",
      "Speed: 3.2ms preprocess, 148.9ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 130.5ms\n",
      "Speed: 3.5ms preprocess, 130.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 88.9ms\n",
      "Speed: 3.9ms preprocess, 88.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 81.0ms\n",
      "Speed: 3.1ms preprocess, 81.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.8ms\n",
      "Speed: 3.6ms preprocess, 79.8ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.4ms\n",
      "Speed: 2.6ms preprocess, 79.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 132.3ms\n",
      "Speed: 3.4ms preprocess, 132.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 74.0ms\n",
      "Speed: 2.4ms preprocess, 74.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 130.4ms\n",
      "Speed: 3.1ms preprocess, 130.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 91.6ms\n",
      "Speed: 3.2ms preprocess, 91.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 180.5ms\n",
      "Speed: 3.2ms preprocess, 180.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 130.6ms\n",
      "Speed: 2.9ms preprocess, 130.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 73.1ms\n",
      "Speed: 3.0ms preprocess, 73.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 129.2ms\n",
      "Speed: 3.2ms preprocess, 129.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 84.7ms\n",
      "Speed: 2.8ms preprocess, 84.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 81.0ms\n",
      "Speed: 3.0ms preprocess, 81.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 165.8ms\n",
      "Speed: 3.0ms preprocess, 165.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 144.7ms\n",
      "Speed: 3.1ms preprocess, 144.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 81.0ms\n",
      "Speed: 2.6ms preprocess, 81.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.7ms\n",
      "Speed: 2.3ms preprocess, 79.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 148.2ms\n",
      "Speed: 3.2ms preprocess, 148.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 76.4ms\n",
      "Speed: 3.9ms preprocess, 76.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 140.6ms\n",
      "Speed: 2.2ms preprocess, 140.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 164.8ms\n",
      "Speed: 2.5ms preprocess, 164.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 0.9ms preprocess, 15.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 68.7ms\n",
      "Speed: 1.4ms preprocess, 68.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 92.7ms\n",
      "Speed: 1.9ms preprocess, 92.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 82.7ms\n",
      "Speed: 2.0ms preprocess, 82.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.3ms\n",
      "Speed: 2.0ms preprocess, 79.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 147.0ms\n",
      "Speed: 2.3ms preprocess, 147.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 127.9ms\n",
      "Speed: 3.9ms preprocess, 127.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 127.9ms\n",
      "Speed: 3.0ms preprocess, 127.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 74.0ms\n",
      "Speed: 2.8ms preprocess, 74.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 131.5ms\n",
      "Speed: 3.0ms preprocess, 131.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 77.8ms\n",
      "Speed: 2.0ms preprocess, 77.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 146.0ms\n",
      "Speed: 2.3ms preprocess, 146.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 125.7ms\n",
      "Speed: 2.6ms preprocess, 125.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 81.0ms\n",
      "Speed: 2.0ms preprocess, 81.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.3ms\n",
      "Speed: 2.1ms preprocess, 79.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 120.5ms\n",
      "Speed: 2.5ms preprocess, 120.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 93.2ms\n",
      "Speed: 2.0ms preprocess, 93.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 82.6ms\n",
      "Speed: 2.0ms preprocess, 82.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.4ms\n",
      "Speed: 1.9ms preprocess, 79.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 144.8ms\n",
      "Speed: 2.4ms preprocess, 144.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 74.2ms\n",
      "Speed: 1.9ms preprocess, 74.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 129.9ms\n",
      "Speed: 2.2ms preprocess, 129.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 162.5ms\n",
      "Speed: 2.3ms preprocess, 162.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 159.8ms\n",
      "Speed: 2.8ms preprocess, 159.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 140.8ms\n",
      "Speed: 3.4ms preprocess, 140.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 72.7ms\n",
      "Speed: 2.7ms preprocess, 72.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 393\u001b[0m\n\u001b[0;32m    390\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Process pose and hands\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m results_pose \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(frame_rgb)\n\u001b[0;32m    394\u001b[0m results_hands \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(frame_rgb)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# Extract keypoints\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Time Limits (in seconds) ---\n",
    "TIME_LIMIT = 20   # set as desired\n",
    "\n",
    "# Cooldown frames before resetting activity (anti-cheat)\n",
    "RESET_DELAY_FRAMES = 10\n",
    "\n",
    "activity_reset_counters = {\n",
    "    \"phone\": 0,\n",
    "    \"cup\": 0,\n",
    "    \"book\": 0,\n",
    "    \"laptop\": 0\n",
    "}\n",
    "\n",
    "# detection miss tolerance (frames) before zeroing CONFIRM counter\n",
    "DETECT_RESET_FRAMES = 50\n",
    "detection_miss_counters = {\n",
    "    \"phone\": 0,\n",
    "    \"cup\": 0,\n",
    "    \"book\": 0,\n",
    "    \"laptop\": 0\n",
    "}\n",
    "\n",
    "# Start timers when activity begins\n",
    "activity_timers = {\n",
    "    \"phone\": None,\n",
    "    \"cup\": None,\n",
    "    \"book\": None,\n",
    "    \"laptop\": None\n",
    "}\n",
    "\n",
    "# Whether the limit already exceeded (stop repeated saving)\n",
    "activity_flags = {\n",
    "    \"phone\": False,\n",
    "    \"cup\": False,\n",
    "    \"book\": False,\n",
    "    \"laptop\": False\n",
    "}\n",
    "\n",
    "# Save folder\n",
    "SAVE_FOLDER = \"exceeded_frames\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize MediaPipe\n",
    "# -----------------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,\n",
    "                       min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# -----------------------------\n",
    "# Load YOLOv8 object detection model\n",
    "# -----------------------------\n",
    "object_model = YOLO(\"yolov8l.pt\")  # COCO trained (adjust to your model path)\n",
    "\n",
    "# -----------------------------\n",
    "# Webcam and motion tracking\n",
    "# -----------------------------\n",
    "cap = cv2.VideoCapture(1)\n",
    "prev_keypoints = None\n",
    "movement_history = []\n",
    "\n",
    "# Thresholds\n",
    "MOTION_THRESHOLD = 5\n",
    "STATIC_FRAMES = 30  # used for movement_history window\n",
    "\n",
    "# Acting detection params\n",
    "MIN_BODY_MOTION = 3.0      # body motion threshold for \"real\" activity\n",
    "MIN_HAND_MOTION = 5.0      # hand motion threshold for \"real\" activity\n",
    "ACTING_STATIC_FRAMES = 20  # consecutive low-motion frames to mark acting\n",
    "\n",
    "acting_counters = {\n",
    "    \"phone\": 0,\n",
    "    \"cup\": 0,\n",
    "    \"book\": 0,\n",
    "    \"laptop\": 0\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def extract_keypoints(results_pose, results_hands, frame_shape):\n",
    "    keypoints = []\n",
    "\n",
    "    # -------- POSE (33 keypoints) --------\n",
    "    if results_pose.pose_landmarks:\n",
    "        for lm in results_pose.pose_landmarks.landmark:\n",
    "            x = int(lm.x * frame_shape[1])\n",
    "            y = int(lm.y * frame_shape[0])\n",
    "            keypoints.append([x, y])\n",
    "    else:\n",
    "        keypoints += [[0, 0]] * 33\n",
    "\n",
    "    # -------- HANDS (42 keypoints = 21 per hand) --------\n",
    "    hand_kps = []\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for hand in results_hands.multi_hand_landmarks:\n",
    "            for lm in hand.landmark:\n",
    "                x = int(lm.x * frame_shape[1])\n",
    "                y = int(lm.y * frame_shape[0])\n",
    "                hand_kps.append([x, y])\n",
    "\n",
    "    # pad hand keypoints to EXACTLY 42\n",
    "    while len(hand_kps) < 42:\n",
    "        hand_kps.append([0, 0])\n",
    "\n",
    "    # combine pose+hands\n",
    "    keypoints += hand_kps\n",
    "\n",
    "    # -------- ENFORCE EXACT SIZE (75 keypoints) --------\n",
    "    keypoints = keypoints[:75]     # truncate if more\n",
    "    return np.array(keypoints)\n",
    "\n",
    "\n",
    "def draw_pose_mask(mask, keypoints):\n",
    "    \"\"\"Draw body + hands skeleton as mask\"\"\"\n",
    "    body_connections = mp_pose.POSE_CONNECTIONS\n",
    "    for connection in body_connections:\n",
    "        a, b = connection\n",
    "        if keypoints[a][0] > 0 and keypoints[b][0] > 0:\n",
    "            cv2.line(mask, tuple(keypoints[a]), tuple(keypoints[b]), (255,255,255), 2)\n",
    "\n",
    "    hand_connections = mp_hands.HAND_CONNECTIONS\n",
    "    # Left hand: indices 33:54 (21 points)\n",
    "    left_hand = keypoints[33:54]\n",
    "    for connection in hand_connections:\n",
    "        a, b = connection\n",
    "        if left_hand[a][0] > 0 and left_hand[b][0] > 0:\n",
    "            cv2.line(mask, tuple(left_hand[a]), tuple(left_hand[b]), (255,255,255), 2)\n",
    "    # Right hand: indices 54:75\n",
    "    right_hand = keypoints[54:75]\n",
    "    for connection in hand_connections:\n",
    "        a, b = connection\n",
    "        if right_hand[a][0] > 0 and right_hand[b][0] > 0:\n",
    "            cv2.line(mask, tuple(right_hand[a]), tuple(right_hand[b]), (255,255,255), 2)\n",
    "    return mask\n",
    "\n",
    "def point_to_box(point, box):\n",
    "    if point is None:\n",
    "        return 9999\n",
    "    x1, y1, x2, y2 = box\n",
    "    cx, cy = (x1 + x2)/2, (y1 + y2)/2\n",
    "    return np.hypot(point[0]-cx, point[1]-cy)\n",
    "\n",
    "# Persistent detection counters\n",
    "task_memory = {\"phone\": 0, \"cup\": 0, \"book\": 0, \"laptop\": 0}\n",
    "CONFIRM_FRAMES = 3  # required consecutive detections\n",
    "\n",
    "PHONE_DIST = 150\n",
    "BOTTLE_DIST = 150\n",
    "BOOK_DIST = 180\n",
    "LAPTOP_DIST = 200\n",
    "\n",
    "def get_hand_centroid(keypoints):\n",
    "    \"\"\"Return average HAND position (more stable than wrist).\"\"\"\n",
    "    left_hand = keypoints[33:54]\n",
    "    right_hand = keypoints[54:75]\n",
    "\n",
    "    def centroid(hand):\n",
    "        valid = [pt for pt in hand if pt[0] > 0]\n",
    "        if not valid:\n",
    "            return None\n",
    "        arr = np.array(valid)\n",
    "        return np.mean(arr, axis=0)\n",
    "\n",
    "    return centroid(left_hand), centroid(right_hand)\n",
    "\n",
    "def update_activity_timer(activity, is_active, frame):\n",
    "    global activity_timers, activity_flags, activity_reset_counters\n",
    "\n",
    "    if is_active:\n",
    "        # Detected this frame → cancel reset countdown\n",
    "        activity_reset_counters[activity] = 0\n",
    "\n",
    "        # Start timer if not already running\n",
    "        if activity_timers[activity] is None:\n",
    "            activity_timers[activity] = time.time()\n",
    "\n",
    "        elapsed = time.time() - activity_timers[activity]\n",
    "\n",
    "        # Check if time exceeded\n",
    "        if elapsed >= TIME_LIMIT:\n",
    "            if not activity_flags[activity]:\n",
    "                ts = int(time.time())\n",
    "                # Save a copy of frame to avoid race\n",
    "                if frame is not None:\n",
    "                    cv2.imwrite(f\"{SAVE_FOLDER}/{activity}_{ts}.jpg\", frame)\n",
    "                activity_flags[activity] = True\n",
    "\n",
    "            return True  # limit exceeded\n",
    "\n",
    "        return False  # still ongoing\n",
    "\n",
    "    else:\n",
    "        # Not detected → increase cooldown counter\n",
    "        activity_reset_counters[activity] += 1\n",
    "\n",
    "        # Only reset if disappeared for enough frames\n",
    "        if activity_reset_counters[activity] >= RESET_DELAY_FRAMES:\n",
    "            activity_timers[activity] = None\n",
    "            activity_flags[activity] = False\n",
    "            activity_reset_counters[activity] = 0\n",
    "\n",
    "        return False\n",
    "\n",
    "def is_acting(activity, avg_motion, avg_hand_motion):\n",
    "    \"\"\"Return True if user is likely pretending (holding object but minimal motion).\"\"\"\n",
    "    global acting_counters\n",
    "    low_body = avg_motion < MIN_BODY_MOTION\n",
    "    low_hand = avg_hand_motion < MIN_HAND_MOTION\n",
    "\n",
    "    if low_body and low_hand:\n",
    "        acting_counters[activity] += 1\n",
    "    else:\n",
    "        acting_counters[activity] = 0\n",
    "\n",
    "    return acting_counters[activity] >= ACTING_STATIC_FRAMES\n",
    "\n",
    "def detect_task(objects, keypoints, avg_motion, avg_hand_motion, frame):\n",
    "    \"\"\"\n",
    "    Detect tasks and incorporate:\n",
    "    - object proximity checks\n",
    "    - persistent detection (task_memory)\n",
    "    - acting detection (using avg_motion & avg_hand_motion)\n",
    "    - activity timers (update_activity_timer)\n",
    "    \"\"\"\n",
    "    global task_memory, detection_miss_counters\n",
    "\n",
    "    left_hand, right_hand = get_hand_centroid(keypoints)\n",
    "    nose = keypoints[0] if keypoints[0][0] > 0 else None\n",
    "\n",
    "    # Detected flags for this frame\n",
    "    detected_phone = False\n",
    "    detected_cup = False\n",
    "    detected_book = False\n",
    "    detected_laptop = False\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1️⃣ Using Phone (PRIORITY)\n",
    "    # -----------------------------\n",
    "    if \"cell phone\" in objects:\n",
    "        phone = objects[\"cell phone\"]\n",
    "        if (left_hand is not None and point_to_box(left_hand, phone) < PHONE_DIST) or \\\n",
    "           (right_hand is not None and point_to_box(right_hand, phone) < PHONE_DIST):\n",
    "            detected_phone = True\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2️⃣ Bottle / Cup\n",
    "    # -----------------------------\n",
    "    cup = objects.get(\"cup\") if objects.get(\"cup\") is not None else objects.get(\"bottle\")\n",
    "\n",
    "    drinking = False\n",
    "    if cup is not None:\n",
    "        if (left_hand is not None and point_to_box(left_hand, cup) < BOTTLE_DIST) or \\\n",
    "           (right_hand is not None and point_to_box(right_hand, cup) < BOTTLE_DIST):\n",
    "            detected_cup = True\n",
    "\n",
    "        if nose is not None and point_to_box(nose, cup) < 150:\n",
    "            drinking = True  # don't return early — use as label\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3️⃣ Book / Notebook\n",
    "    # -----------------------------\n",
    "    nb = objects.get(\"book\")\n",
    "    reading_posture = False\n",
    "    if nb is not None:\n",
    "        if (left_hand is not None and point_to_box(left_hand, nb) < BOOK_DIST) or \\\n",
    "           (right_hand is not None and point_to_box(right_hand, nb) < BOOK_DIST):\n",
    "            detected_book = True\n",
    "\n",
    "        if nose is not None and point_to_box(nose, nb) < 200:\n",
    "            reading_posture = True\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4️⃣ Laptop\n",
    "    # -----------------------------\n",
    "    if \"laptop\" in objects:\n",
    "        lp = objects[\"laptop\"]\n",
    "        if (left_hand is not None and point_to_box(left_hand, lp) < LAPTOP_DIST) or \\\n",
    "           (right_hand is not None and point_to_box(right_hand, lp) < LAPTOP_DIST):\n",
    "            detected_laptop = True\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Update task_memory with small miss-tolerance\n",
    "    # -----------------------------------------------\n",
    "    # PHONE\n",
    "    if detected_phone:\n",
    "        detection_miss_counters[\"phone\"] = 0\n",
    "        task_memory[\"phone\"] += 1\n",
    "    else:\n",
    "        detection_miss_counters[\"phone\"] += 1\n",
    "        if detection_miss_counters[\"phone\"] >= DETECT_RESET_FRAMES:\n",
    "            task_memory[\"phone\"] = 0\n",
    "            detection_miss_counters[\"phone\"] = 0\n",
    "\n",
    "    # CUP / BOTTLE\n",
    "    if detected_cup:\n",
    "        detection_miss_counters[\"cup\"] = 0\n",
    "        task_memory[\"cup\"] += 1\n",
    "    else:\n",
    "        detection_miss_counters[\"cup\"] += 1\n",
    "        if detection_miss_counters[\"cup\"] >= DETECT_RESET_FRAMES:\n",
    "            task_memory[\"cup\"] = 0\n",
    "            detection_miss_counters[\"cup\"] = 0\n",
    "\n",
    "    # BOOK\n",
    "    if detected_book:\n",
    "        detection_miss_counters[\"book\"] = 0\n",
    "        task_memory[\"book\"] += 1\n",
    "    else:\n",
    "        detection_miss_counters[\"book\"] += 1\n",
    "        if detection_miss_counters[\"book\"] >= DETECT_RESET_FRAMES:\n",
    "            task_memory[\"book\"] = 0\n",
    "            detection_miss_counters[\"book\"] = 0\n",
    "\n",
    "    # LAPTOP\n",
    "    if detected_laptop:\n",
    "        detection_miss_counters[\"laptop\"] = 0\n",
    "        task_memory[\"laptop\"] += 1\n",
    "    else:\n",
    "        detection_miss_counters[\"laptop\"] += 1\n",
    "        if detection_miss_counters[\"laptop\"] >= DETECT_RESET_FRAMES:\n",
    "            task_memory[\"laptop\"] = 0\n",
    "            detection_miss_counters[\"laptop\"] = 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # PRIORITY LOGIC with acting detection\n",
    "    # -----------------------------\n",
    "    # PHONE\n",
    "    if task_memory[\"phone\"] >= CONFIRM_FRAMES:\n",
    "        if is_acting(\"phone\", avg_motion, avg_hand_motion):\n",
    "            # Do not start/continue timer when acting; show explicit label\n",
    "            return \"Acting\"\n",
    "        if update_activity_timer(\"phone\", True, frame):\n",
    "            return \"⚠ Time limit exceeded\"\n",
    "        return \"Using mobile phone\"\n",
    "    else:\n",
    "        update_activity_timer(\"phone\", False, None)\n",
    "\n",
    "    # CUP\n",
    "    if task_memory[\"cup\"] >= CONFIRM_FRAMES:\n",
    "        if is_acting(\"cup\", avg_motion, avg_hand_motion):\n",
    "            return \"Acting\"\n",
    "        if update_activity_timer(\"cup\", True, frame):\n",
    "            return \"⚠ Time limit exceeded\"\n",
    "        return \"Drinking\" if drinking else \"Holding cup\"\n",
    "    else:\n",
    "        update_activity_timer(\"cup\", False, None)\n",
    "\n",
    "    # BOOK\n",
    "    if task_memory[\"book\"] >= CONFIRM_FRAMES:\n",
    "        if is_acting(\"book\", avg_motion, avg_hand_motion):\n",
    "            return \"Acting\"\n",
    "        if update_activity_timer(\"book\", True, frame):\n",
    "            return \"⚠ Time limit exceeded\"\n",
    "        return \"Reading\" if reading_posture else \"Holding notebook\"\n",
    "    else:\n",
    "        update_activity_timer(\"book\", False, None)\n",
    "\n",
    "    # LAPTOP\n",
    "    if task_memory[\"laptop\"] >= CONFIRM_FRAMES:\n",
    "        if is_acting(\"laptop\", avg_motion, avg_hand_motion):\n",
    "            return \"Acting\"\n",
    "        if update_activity_timer(\"laptop\", True, frame):\n",
    "            return \"⚠ Time limit exceeded\"\n",
    "        return \"using Laptop\"\n",
    "    else:\n",
    "        update_activity_timer(\"laptop\", False, None)\n",
    "\n",
    "    return \"Idle\"\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop\n",
    "# -----------------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process pose and hands\n",
    "    results_pose = pose.process(frame_rgb)\n",
    "    results_hands = hands.process(frame_rgb)\n",
    "\n",
    "    # Extract keypoints\n",
    "    kps = extract_keypoints(results_pose, results_hands, frame.shape)\n",
    "\n",
    "    # Draw skeleton mask\n",
    "    mask = np.zeros_like(frame)\n",
    "    mask = draw_pose_mask(mask, kps)\n",
    "\n",
    "    # Motion calculation (body-wide)\n",
    "    if prev_keypoints is not None:\n",
    "        # Only consider valid keypoints (non-zero)\n",
    "        valid = (kps[:,0] > 0) & (prev_keypoints[:,0] > 0)\n",
    "\n",
    "        if np.any(valid):\n",
    "            diff = np.linalg.norm(kps[valid] - prev_keypoints[valid], axis=1)\n",
    "            movement = np.mean(diff)\n",
    "        else:\n",
    "            movement = 0\n",
    "\n",
    "        movement_history.append(movement)\n",
    "\n",
    "    prev_keypoints = kps.copy()\n",
    "\n",
    "    if len(movement_history) > STATIC_FRAMES:\n",
    "        movement_history.pop(0)\n",
    "\n",
    "    avg_motion = np.mean(movement_history) if movement_history else 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Hand-specific motion\n",
    "    # -----------------------------\n",
    "    # left_hand_indices = 33..53, right_hand_indices = 54..74\n",
    "    if prev_keypoints is not None:\n",
    "        # compute per-keypoint movement for hands; filter zeros\n",
    "        left_idx = list(range(33, 54))\n",
    "        right_idx = list(range(54, 75))\n",
    "\n",
    "        # ensure prev_keypoints exists earlier than current (we copy above)\n",
    "        # here prev_keypoints is current frame's previous stored state, but we've already set prev_keypoints = kps.copy()\n",
    "        # so we need a separate previous variable; to avoid complicating, we compute hand motion using movement_history approach:\n",
    "        # We'll approximate hand motion from the most recent two stored frames in movement_history by recomputing using last two kps snapshots.\n",
    "        # Simpler: keep last_kps variable. Let's implement last_kps variable outside loop. (We'll add it dynamically.)\n",
    "\n",
    "        pass\n",
    "\n",
    "    # To compute hand motion robustly we need the previous keypoints from the prior frame.\n",
    "    # We'll implement last_kps to hold the previous frame's keypoints.\n",
    "    # (Define last_kps if not present.)\n",
    "    try:\n",
    "        last_kps\n",
    "    except NameError:\n",
    "        last_kps = None\n",
    "\n",
    "    # compute avg_hand_motion using last_kps (previous frame) and current kps\n",
    "    if last_kps is not None:\n",
    "        left_idx = list(range(33, 54))\n",
    "        right_idx = list(range(54, 75))\n",
    "\n",
    "        left_valid = (kps[left_idx,0] > 0) & (last_kps[left_idx,0] > 0)\n",
    "        right_valid = (kps[right_idx,0] > 0) & (last_kps[right_idx,0] > 0)\n",
    "\n",
    "        if np.any(left_valid):\n",
    "            left_motion = np.linalg.norm(kps[left_idx][left_valid] - last_kps[left_idx][left_valid], axis=1)\n",
    "            avg_left_hand_motion = np.mean(left_motion)\n",
    "        else:\n",
    "            avg_left_hand_motion = 0.0\n",
    "\n",
    "        if np.any(right_valid):\n",
    "            right_motion = np.linalg.norm(kps[right_idx][right_valid] - last_kps[right_idx][right_valid], axis=1)\n",
    "            avg_right_hand_motion = np.mean(right_motion)\n",
    "        else:\n",
    "            avg_right_hand_motion = 0.0\n",
    "\n",
    "        avg_hand_motion = (avg_left_hand_motion + avg_right_hand_motion) / 2.0\n",
    "    else:\n",
    "        avg_hand_motion = 0.0\n",
    "\n",
    "    # update last_kps for next iteration\n",
    "    last_kps = kps.copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # YOLO Object Detection\n",
    "    # -----------------------------\n",
    "    CONF_THRESH = 0.7\n",
    "    # include both cup and bottle class names\n",
    "    SELECTED_CLASSES = [\"cell phone\", \"laptop\"]\n",
    "    obj_results = object_model(frame)[0]\n",
    "    objects = {}\n",
    "    for box, cls, conf in zip(obj_results.boxes.xyxy, obj_results.boxes.cls, obj_results.boxes.conf):\n",
    "        clss = object_model.names[int(cls)]\n",
    "        conf = float(conf)\n",
    "\n",
    "        if conf < CONF_THRESH or clss not in SELECTED_CLASSES:\n",
    "            continue\n",
    "\n",
    "        label = clss\n",
    "        objects[label] = box.cpu().numpy()\n",
    "\n",
    "        # Draw detected objects\n",
    "        x1, y1, x2, y2 = box.cpu().numpy()\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255,0,0), 2)\n",
    "        cv2.putText(frame, label, (int(x1), int(y1)-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
    "\n",
    "    frame_global = frame.copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Detect task (pass motion metrics)\n",
    "    # -----------------------------\n",
    "    task = detect_task(objects, kps, avg_motion, avg_hand_motion, frame_global)\n",
    "\n",
    "    # If motion too low, mark as Not Working / Working only when Idle\n",
    "    if task == \"Idle\":\n",
    "        if avg_motion < MOTION_THRESHOLD:\n",
    "            task = \"Not Working\"\n",
    "        else:\n",
    "            task = \"Working\"\n",
    "\n",
    "    # Draw detailed status (you can also show avg motions for debugging)\n",
    "    status_text = f\"Task: {task}\"\n",
    "    debug_text = f\"BodyM: {avg_motion:.2f} HandM: {avg_hand_motion:.2f}\"\n",
    "    cv2.putText(frame, status_text, (30,50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,0),4)\n",
    "    # cv2.putText(frame, debug_text, (30,80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255),2)\n",
    "\n",
    "    overlay = cv2.addWeighted(frame, 0.7, mask, 0.3, 0)\n",
    "    cv2.imshow(\"Task + Pose\", overlay)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
